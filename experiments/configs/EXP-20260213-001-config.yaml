# VDC: Visual Drift Compensation for Cross-Lingual MLLM Adaptation
# Experiment ID: EXP-20260213-001 (v2: top-tier venue revision)
# Created: 2026-02-13
# Target: ACL / EMNLP / NeurIPS 2026

# ============================================================================
# GLOBAL SETTINGS
# ============================================================================
experiment:
  id: "EXP-20260213-001"
  name: "VDC-Visual-Drift-Compensation"
  version: 2
  description: "Novel training framework to preserve visual grounding during cross-lingual MLLM adaptation via drift-aware visual-linguistic co-adaptation"
  author: "Juyeon"
  created_at: "2026-02-13"
  hypothesis_doc: "docs/01-hypothesis/features/korean-mllm-visual-blindness-hypothesis.md"
  method_doc: "docs/01-hypothesis/features/visual-drift-compensation-hypothesis.md"
  design_doc: "docs/02-experiment-design/features/EXP-20260213-001-design.md"

  phases:
    phase1:
      name: "Diagnostic Study"
      purpose: "Prove drift exists and correlates with visual blindness"
      timeline: "4 weeks"
      estimated_gpu_days: 28

    phase2:
      name: "VDC Ablation"
      purpose: "Component ablation + hyperparameter optimization"
      timeline: "6 weeks"
      estimated_gpu_days: 52

    phase3:
      name: "Main Results"
      purpose: "Full comparison at 7B-8B scale across 5 languages"
      timeline: "8-12 weeks"
      estimated_gpu_days: 282

  seeds: [42, 123, 456]
  output_dir: "/NetDisk/juyeon/research/results/EXP-20260213-001"
  cache_dir: "/NetDisk/juyeon/research/.cache"
  log_dir: "/NetDisk/juyeon/research/results/EXP-20260213-001/logs"

# ============================================================================
# MODELS
# ============================================================================
models:
  # --- Ablation models (2B class) ---
  internvl_2b:
    name: "InternVL2.5-2B"
    source: "OpenGVLab/InternVL2_5-2B"
    vision_encoder: "InternViT-300M"
    projector_type: "mlp"
    llm: "InternLM2-1.8B"
    total_params: "2.2B"
    usage: "ablation"
    image_size: 448
    patch_size: 14

  llava_05b:
    name: "LLaVA-OneVision-0.5B"
    source: "lmms-lab/llava-onevision-qwen2-0.5b-ov"
    vision_encoder: "SigLIP-400M"
    projector_type: "mlp"
    llm: "Qwen2-0.5B"
    total_params: "0.9B"
    usage: "ablation"
    image_size: 384
    patch_size: 14

  qwen2vl_2b:
    name: "Qwen2-VL-2B"
    source: "Qwen/Qwen2-VL-2B-Instruct"
    vision_encoder: "ViT-600M"
    projector_type: "cross_attention_merger"
    llm: "Qwen2-1.5B"
    total_params: "2.1B"
    usage: "ablation"
    image_size: "dynamic"
    patch_size: 14

  # --- Main result models (7B-8B class) ---
  internvl_8b:
    name: "InternVL2.5-8B"
    source: "OpenGVLab/InternVL2_5-8B"
    vision_encoder: "InternViT-6B"
    projector_type: "mlp"
    llm: "InternLM2-7B"
    total_params: "8.1B"
    usage: "main_results"
    image_size: 448
    patch_size: 14

  qwen2vl_7b:
    name: "Qwen2-VL-7B"
    source: "Qwen/Qwen2-VL-7B-Instruct"
    vision_encoder: "ViT-600M"
    projector_type: "cross_attention_merger"
    llm: "Qwen2-7B"
    total_params: "7.6B"
    usage: "main_results"
    image_size: "dynamic"
    patch_size: 14

# ============================================================================
# LANGUAGES
# ============================================================================
languages:
  korean:
    code: "ko"
    script: "hangul"
    family: "koreanic"
    training_data: "data/korean_vqa/"
    training_size: 100000

  japanese:
    code: "ja"
    script: "kanji_kana"
    family: "japonic"
    training_data: "data/japanese_vqa/"
    training_size: 100000

  chinese:
    code: "zh"
    script: "hanzi"
    family: "sinitic"
    training_data: "data/chinese_vqa/"
    training_size: 150000

  arabic:
    code: "ar"
    script: "arabic"
    family: "semitic"
    training_data: "data/arabic_vqa/"
    training_size: 80000

  thai:
    code: "th"
    script: "thai"
    family: "kra_dai"
    training_data: "data/thai_vqa/"
    training_size: 60000

# ============================================================================
# VDC METHOD CONFIGURATION
# ============================================================================
vdc:
  # Component 1: Drift-Aware Anchor Compensation
  drift:
    lambda: 0.5
    anchor_set_size: 50
    anchor_update_freq: 100  # steps
    metric: "mmd"
    kernel: "rbf"
    kernel_bandwidth: "median"  # median heuristic

  # Component 2: Contrastive Visual Grounding
  contrast:
    lambda: 0.1
    compute_freq: 2  # every 2nd step
    stop_grad_text_only: true
    temperature: 1.0

  # Component 3: Drift-Adaptive Visual Alignment
  align:
    lambda_base: 0.3
    adaptive_alpha: 1.0
    critical_layers: "auto"
    num_layers: 4
    loss_type: "mse"
    normalize: true

  # Component 4: Modality-Aware LoRA
  lora:
    base_rank: 16
    projector_beta: 1.5
    alpha_ratio: 2.0  # alpha = rank * alpha_ratio
    dropout: 0.05
    target_modules:
      - "q_proj"
      - "k_proj"
      - "v_proj"
      - "o_proj"
      - "gate_proj"
      - "up_proj"
      - "down_proj"
    rank_allocation: "mir_proportional"

  # Training
  optimizer: "AdamW"
  base_lr: 2.0e-5
  weight_decay: 0.01
  lr_scheduler: "cosine"
  warmup_ratio: 0.03
  epochs: 3
  max_length: 2048
  bf16: true
  gradient_checkpointing: true

  # 2B models
  batch_2b:
    per_device: 4
    grad_accum: 2
    effective: 16

  # 7B-8B models (2 GPUs, ZeRO-2)
  batch_7b:
    per_device: 1
    grad_accum: 8
    effective: 16

# ============================================================================
# BASELINES
# ============================================================================
baselines:
  B0_pretrained:
    description: "No fine-tuning"
    cost: "0x"

  B1_standard_lora:
    lr: 2.0e-5
    epochs: 3
    lora_r: 64
    lora_alpha: 128
    lora_dropout: 0.05
    lora_target: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    trainable: ["lora"]
    frozen: ["vision_encoder", "projector", "llm_base"]
    cost: "1.0x"

  B2_full_ft:
    lr: 2.0e-5
    epochs: 3
    trainable: ["llm", "projector"]
    frozen: ["vision_encoder"]
    cost: "1.2x"

  B3_viral:
    lr: 2.0e-5
    epochs: 3
    trainable: ["llm", "projector"]
    frozen: ["vision_encoder"]
    viral_lambda: 0.5
    viral_loss: "mse"
    viral_stop_grad: true
    cost: "1.3x"

  B4_unfreezing:
    lr: 2.0e-6
    epochs: 3
    trainable: ["vision_encoder", "llm", "projector"]
    cost: "3-5x"

  B5_staged:
    stage1:
      trainable: ["projector"]
      lr: 1.0e-3
      epochs: 1
      data: "captioning"
    stage2:
      trainable: ["llm_lora"]
      lr: 2.0e-5
      epochs: 2
      data: "vqa"
      lora_r: 64
    stage3:
      trainable: ["llm_lora", "projector"]
      lr: 5.0e-6
      epochs: 1
      data: "vqa_curated"
    cost: "2.0x"

# ============================================================================
# PHASE 2: ABLATION VARIANTS
# ============================================================================
ablation:
  primary_model: "internvl_2b"
  primary_language: "korean"

  variants:
    V0: {drift: false, contrast: false, align: false, ma_lora: false, desc: "Standard LoRA (B1)"}
    V1: {drift: true,  contrast: false, align: false, ma_lora: false, desc: "Drift only"}
    V2: {drift: false, contrast: true,  align: false, ma_lora: false, desc: "Contrastive only"}
    V3: {drift: false, contrast: false, align: true,  ma_lora: false, desc: "Alignment only (≈VIRAL)"}
    V4: {drift: false, contrast: false, align: false, ma_lora: true,  desc: "MA-LoRA only"}
    V5: {drift: true,  contrast: true,  align: false, ma_lora: false, desc: "Drift + Contrastive"}
    V6: {drift: true,  contrast: false, align: true,  ma_lora: false, desc: "Drift + Alignment"}
    V7: {drift: true,  contrast: true,  align: true,  ma_lora: false, desc: "All losses, uniform LoRA"}
    V8: {drift: true,  contrast: true,  align: true,  ma_lora: true,  desc: "Full VDC"}

  hyperparameter_search:
    lambda_drift: [0.1, 0.3, 0.5, 1.0]
    lambda_contrast: [0.05, 0.1, 0.3]
    lambda_align: [0.1, 0.3, 0.5]
    adaptive_alpha: [0.5, 1.0, 2.0]
    anchor_freq: [50, 100, 500]
    contrast_freq: [1, 2, 5]
    base_rank: [8, 16, 32]
    projector_beta: [1.0, 1.5, 2.0]
    strategy: "sequential"

# ============================================================================
# EVALUATION
# ============================================================================
evaluation:
  visual_grounding:
    - {name: "xGQA", type: "vqa", languages: "all", scale: "all"}
    - {name: "POPE", type: "hallucination", languages: "all", scale: "all"}
    - {name: "CountBenchQA", type: "counting", languages: "all", scale: "all"}
    - {name: "Spatial-Bench", type: "spatial", languages: "all", scale: "all"}

  general_capability:
    - {name: "MMBench", type: "general", languages: "all", scale: "7b_only"}
    - {name: "SEED-Bench-2", type: "general", languages: "all", scale: "7b_only"}
    - {name: "OCRBench", type: "ocr", languages: "all", scale: "all"}
    - {name: "MMStar", type: "reasoning", languages: "all", scale: "7b_only"}

  retention:
    - {name: "VQAv2", type: "vqa", language: "en", purpose: "english_forgetting"}
    - {name: "language_nlu", type: "nlu", language: "target", purpose: "language_retention"}

  diagnostic:
    vds:
      method: "noise_image_comparison"
      noise_type: "gaussian"
      num_samples: 1000

  statistical_tests:
    significance: 0.05
    method: "paired_ttest"
    confidence_interval: "bootstrap_95"
    effect_size: "cohens_d"
    num_seeds: 3

# ============================================================================
# HARDWARE & SOFTWARE
# ============================================================================
hardware:
  gpus: "2x NVIDIA A100-PCIE-80GB"
  vram_per_gpu: "80 GB"
  strategy:
    distributed: "DeepSpeed ZeRO-2"
    mixed_precision: "bf16"
    gradient_checkpointing: true
    flash_attention: true

software:
  python: "3.10"
  framework: "PyTorch 2.1+"
  key_libraries:
    - "transformers >= 4.40"
    - "deepspeed >= 0.14"
    - "peft >= 0.10"
    - "flash-attn >= 2.5"
    - "lmms-eval"
    - "scikit-learn"
    - "matplotlib"
    - "seaborn"

# ============================================================================
# BUDGET
# ============================================================================
budget:
  phase1_gpu_days: 28
  phase2_gpu_days: 52
  phase3_gpu_days: 282
  total_gpu_days: 362
  with_2_gpus_calendar_days: 181
  compressed_timeline_weeks: 22
  storage_estimate_gb: 1000

  # Minimum viable (early submission)
  minimum_viable:
    phase1: 28
    phase2: 52
    phase3_minimum: 24  # 1 model × 3 languages × 4 methods × 1 seed
    total: 104
    calendar_weeks: 12
