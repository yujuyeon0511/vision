# InternViT-Next Experiment Configuration
# Experiment ID: EXP-20260212-001
# Created: 2026-02-12
# Description: Vision encoder modernization with 2D-RoPE, window attention, spatial features, and adaptive compression

# ============================================================================
# GLOBAL SETTINGS
# ============================================================================
experiment:
  id: "EXP-20260212-001"
  name: "InternViT-Next-Vision-Encoder-Modernization"
  description: "Systematic modernization of InternViT-6B with 2D-RoPE, window attention, spatial features, and adaptive compression"
  author: "Juyeon"
  created_at: "2026-02-12"
  hypothesis_doc: "docs/01-hypothesis/features/vision-encoder-improvement-hypothesis.md"

  # Two-phase experimental strategy
  phases:
    phase1:
      name: "Mini-InternVL Validation"
      base_model: "OpenGVLab/Mini-InternVL-2B-V1-5"  # or InternVL2.5-2B
      vision_encoder_size: "300M"
      purpose: "Rapid validation of all architectural modifications"
      timeline: "2-3 weeks"

    phase2:
      name: "Full InternVL Production"
      base_model: "OpenGVLab/InternVL2_5-8B"
      vision_encoder_size: "6B"
      purpose: "Apply validated modifications to production-scale model"
      timeline: "4-6 weeks"

  seeds: [42, 123, 456]
  output_dir: "/NetDisk/juyeon/research/results/EXP-20260212-001"
  cache_dir: "/NetDisk/juyeon/research/.cache"

# ============================================================================
# MODEL ARCHITECTURE
# ============================================================================
model:
  # Base model configuration (Phase 2 - 6B)
  base:
    vision_encoder: "OpenGVLab/InternViT-6B-448px-V1-5"
    projector: "mlp"
    llm_decoder: "internlm/internlm2-chat-8b"
    image_size: 448
    patch_size: 14
    num_patches: 1024  # (448/14)^2 = 1024
    max_tiles: 12
    vision_hidden_size: 4096
    llm_hidden_size: 4096
    projector_hidden_size: 4096
    vision_num_layers: 24  # InternViT-6B
    llm_num_layers: 32     # InternLM2-8B

  # Base model configuration (Phase 1 - 300M)
  base_mini:
    vision_encoder: "OpenGVLab/InternViT-300M-448px"
    projector: "mlp"
    llm_decoder: "internlm/internlm2-chat-1_8b"
    image_size: 448
    patch_size: 14
    num_patches: 1024
    max_tiles: 6  # reduced for Mini
    vision_hidden_size: 1024
    llm_hidden_size: 2048
    projector_hidden_size: 2048
    vision_num_layers: 24
    llm_num_layers: 24

  # ============================================================================
  # MODIFICATION 1: 2D-RoPE (Rotary Position Embedding)
  # ============================================================================
  rope_2d:
    enabled: false  # set to true for experiments B1-B5

    # Core parameters
    dim_h: 64  # dimension for height component
    dim_w: 64  # dimension for width component
    base_freq: 10000.0  # base frequency for RoPE
    interpolation: "linear"  # extrapolation method for unseen resolutions

    # Training parameters
    training_resolution: [448, 448]  # base training resolution
    max_extrapolation_ratio: 4.0  # allow up to 4x resolution (e.g., 448 -> 1792)

    # Position encoding computation
    compute_method: "2d_factorized"  # "2d_factorized" or "2d_interleaved"
    scaling_factor: 1.0
    ntk_scaling: false  # NTK-aware scaling for long context (set true if needed)

    # Integration strategy
    replace_abs_pos_embed: true  # completely remove learnable pos_embed
    apply_per_layer: true  # apply RoPE in each attention layer (not global)
    rope_theta: 10000.0  # theta for frequency calculation

    # Training stability
    progressive_unfreezing:
      enabled: true
      stage_2a:
        freeze_layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]  # freeze bottom 12 layers
        train_layers: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]  # train top 12
        steps: 5000
      stage_2b:
        freeze_layers: [0, 1, 2, 3, 4, 5, 6, 7]  # freeze bottom 8 layers
        train_layers: [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]  # train top 16
        steps: 5000
      stage_2c:
        freeze_layers: []  # train all layers
        train_layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
        steps: 10000

  # ============================================================================
  # MODIFICATION 2: Window Attention
  # ============================================================================
  window_attention:
    enabled: false  # set to true for experiments B2-B5

    # Window configuration
    window_size: 8  # 8×8 patches per window (112×112 pixels at patch_size=14)

    # Layer assignment (total 24 layers)
    # Window layers: 1, 2, 3, 4, 6, 7, 8, 9, 11, 12, 13, 14, 16, 17, 18, 19 (16 layers)
    # Global layers: 0, 5, 10, 15, 20, 21, 22, 23 (8 layers)
    global_layer_indices: [0, 5, 10, 15, 20, 21, 22, 23]
    window_layer_indices: [1, 2, 3, 4, 6, 7, 8, 9, 11, 12, 13, 14, 16, 17, 18, 19]

    # Advanced options
    use_shifted_window: false  # Swin Transformer style shifted windows
    window_shift_size: 4  # shift size if use_shifted_window=true
    relative_position_bias: false  # use relative position bias within windows

    # Flash Attention 2 integration
    use_flash_attention: true  # use Flash Attention 2 for efficient computation
    flash_attn_version: 2

    # Training strategy
    init_from_global:
      enabled: true  # initialize window attention from pretrained global attention
      copy_qkv_weights: true
      copy_proj_weights: true

    # Layer-wise learning rate decay
    layer_wise_lr_decay:
      enabled: true
      decay_rate: 0.9  # LR for layer i = base_lr * (decay_rate ^ i)

  # ============================================================================
  # MODIFICATION 3: Spatial Feature Enhancement
  # ============================================================================
  spatial_features:
    type: "none"  # "none", "llava_sp", "piip", "combined"
    enabled: false  # set to true for experiments B3a/b/c, B4, B5

    # Option A: LLaVA-SP Spatial Tokens
    llava_sp:
      enabled: false  # set to true for B3a, B3c
      num_tokens: 6  # add 6 spatial tokens to encoder output

      # Convolutional kernel extraction
      kernel_sizes: [3, 3, 3, 3, 3, 3]  # one per token
      strides: [2, 2, 2, 2, 2, 2]
      paddings: [1, 1, 1, 1, 1, 1]

      # Feature extraction layers
      extract_from_layer: 23  # extract from final layer output
      hidden_channels: 4096  # match vision encoder hidden size

      # Projection
      use_projection: true
      projection_hidden_dim: 2048
      projection_activation: "gelu"

      # Initialization
      init_method: "xavier_uniform"

    # Option B: PIIP Multi-Scale Pyramid
    piip:
      enabled: false  # set to true for B3b, B3c

      # Multi-scale extraction
      extract_layers: [8, 16, 24]  # L1 (low-res), L2 (mid-res), L3 (high-res)
      scale_names: ["L1_low", "L2_mid", "L3_high"]

      # Inverted parameter pyramid (more params for high-res)
      scale_hidden_dims:
        L1_low: 1024   # 1/4 of base
        L2_mid: 2048   # 1/2 of base
        L3_high: 4096  # full base

      # Per-scale projection
      use_scale_projectors: true
      projector_layers: 2  # 2-layer MLP per scale
      projector_activation: "gelu"

      # Cross-scale fusion
      fusion_method: "attention"  # "concat", "weighted_sum", "attention"
      fusion_hidden_dim: 2048

      # Output
      num_tokens_per_scale: 1  # add 1 token per scale (total 3)
      adaptive_pool_size: [16, 16]  # pool to consistent size before fusion

      # Initialization
      init_method: "kaiming_normal"

    # Option C: Combined (LLaVA-SP + PIIP)
    combined:
      enabled: false  # set to true for B3c
      use_both: true
      total_added_tokens: 9  # 6 (SP) + 3 (PIIP)

      # Fusion strategy
      fusion_method: "concat"  # or "attention", "gated"
      use_fusion_layer: false  # add final fusion layer on top

  # ============================================================================
  # MODIFICATION 4: Adaptive Token Compression
  # ============================================================================
  adaptive_compression:
    enabled: false  # set to true for experiments B4, B5

    # Compression method
    method: "attention_based"  # "attention_based", "spatial_uniform", "learned"

    # Compression ratios per scale (if using multi-scale)
    compression_ratios:
      L1_low: 0.5   # drop 50% of low-res tokens
      L2_mid: 0.3   # drop 30% of mid-res tokens
      L3_high: 0.1  # drop 10% of high-res tokens

    # Single-scale compression (if not using multi-scale)
    default_compression_ratio: 0.3  # drop 30% overall

    # Importance scoring
    importance_scorer:
      type: "learned_mlp"  # "learned_mlp", "attention_sum", "gradient_based"
      hidden_dim: 1024
      num_layers: 2
      activation: "gelu"
      use_sigmoid: true  # output [0, 1] importance scores

    # Token selection
    selection_method: "top_k"  # "top_k", "threshold", "stochastic"
    score_threshold: 0.1  # if selection_method="threshold"
    min_tokens_per_image: 256  # safety lower bound
    max_tokens_per_image: 1024  # safety upper bound

    # Training
    warmup_steps: 2000  # gradually increase compression rate
    warmup_schedule: "linear"  # "linear", "cosine"
    start_compression_ratio: 0.0  # start with no compression

    # Inference
    apply_at_inference: true  # also apply compression during inference
    dynamic_compression: false  # adjust compression based on image complexity

    # Text-aware compression (for Korean OCR)
    text_aware:
      enabled: true
      text_detector:
        type: "learned"  # "learned" or "rule_based"
        hidden_dim: 256
        num_layers: 2
        use_pretrained_ocr: false  # use pretrained OCR model as detector
      boost_text_regions: 0.5  # add 0.5 to importance score for text regions
      min_text_tokens: 128  # preserve at least 128 tokens in text regions

  # ============================================================================
  # Vision Encoder Training Configuration
  # ============================================================================
  vision_encoder_training:
    # LoRA fine-tuning (for faster training)
    lora:
      enabled: false  # set to true for LoRA variant
      rank: 32
      alpha: 8  # effective LR = alpha/rank = 0.25
      dropout: 0.05
      target_modules: ["qkv", "proj"]  # attention projection modules
      layers_to_train: [18, 19, 20, 21, 22, 23]  # top 6 layers only

    # Full fine-tuning
    full_finetune:
      enabled: true  # default: full fine-tune
      layers_to_train: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]  # top 12 layers
      freeze_bottom_layers: true
      num_frozen_layers: 12

    # Layer-wise learning rate decay
    layer_wise_lr_decay:
      enabled: true
      decay_rate: 0.9
      base_lr: 2e-5  # for top layer, bottom layers get base_lr * decay_rate^layer_idx

  # ============================================================================
  # Projector Configuration
  # ============================================================================
  projector:
    type: "mlp"  # all variants use MLP projector (no C-Abstractor in this experiment)

    # MLP configuration
    mlp:
      num_layers: 2
      hidden_size: 4096
      output_size: 4096  # match LLM hidden size
      activation: "gelu"
      dropout: 0.0
      layer_norm: true

  # ============================================================================
  # Decoder Configuration
  # ============================================================================
  decoder:
    # LoRA fine-tuning (Stage 3)
    lora:
      enabled: true  # default for all variants
      rank: 64
      alpha: 16
      dropout: 0.05
      target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
      modules_to_save: ["embed_tokens", "lm_head"]

# ============================================================================
# TRAINING CONFIGURATION
# ============================================================================
training:
  # ============================================================================
  # Stage 1: Projector Pre-training
  # ============================================================================
  stage1_projector_pretraining:
    enabled: true
    name: "projector_pretraining"
    description: "Train projector on image-caption alignment"

    # What to train
    trainable:
      vision_encoder: false
      projector: true
      llm_decoder: false

    # Data
    data:
      sources: ["cc3m_en", "cc3m_ko", "korean_captions"]
      mixing_weights: [0.4, 0.3, 0.3]
      task_type: "caption"
      max_length: 256
      total_samples: 400000

    # Hyperparameters (6B)
    hyperparameters_6b:
      max_steps: 10000
      eval_steps: 1000
      save_steps: 2000
      logging_steps: 100

      batch_size:
        per_device: 2
        gradient_accumulation_steps: 64
        effective_batch_size: 256  # 2 * 2 GPUs * 64

      optimizer:
        type: "adamw"
        lr: 1e-3
        betas: [0.9, 0.95]
        eps: 1e-8
        weight_decay: 0.05

      scheduler:
        type: "cosine"
        warmup_steps: 500
        min_lr: 0.0

      precision:
        mixed_precision: "bf16"
        gradient_checkpointing: false

    # Hyperparameters (300M Mini)
    hyperparameters_mini:
      max_steps: 5000
      eval_steps: 500
      save_steps: 1000
      logging_steps: 50

      batch_size:
        per_device: 4
        gradient_accumulation_steps: 32
        effective_batch_size: 256

      optimizer:
        type: "adamw"
        lr: 1e-3
        betas: [0.9, 0.95]
        eps: 1e-8
        weight_decay: 0.05

      scheduler:
        type: "cosine"
        warmup_steps: 250
        min_lr: 0.0

      precision:
        mixed_precision: "bf16"
        gradient_checkpointing: false

  # ============================================================================
  # Stage 2: Vision Encoder Adaptation
  # ============================================================================
  stage2_vision_adaptation:
    enabled: true
    name: "vision_encoder_adaptation"
    description: "Adapt vision encoder to bilingual text-rich images with modifications"

    # What to train
    trainable:
      vision_encoder: true  # LoRA or full fine-tune top layers
      projector: true
      llm_decoder: false

    # Data
    data:
      sources: ["textvqa", "docvqa", "chartqa", "plotqa", "korean_ocr", "k_dtcbench_train"]
      mixing_weights: [0.2, 0.15, 0.15, 0.15, 0.25, 0.10]
      task_type: "vqa"
      max_length: 512
      total_samples: 300000
      korean_ratio: 0.35
      english_ratio: 0.65

    # Hyperparameters (6B)
    hyperparameters_6b:
      max_steps: 20000
      eval_steps: 2000
      save_steps: 5000
      logging_steps: 100

      batch_size:
        per_device: 1
        gradient_accumulation_steps: 64
        effective_batch_size: 128  # 1 * 2 GPUs * 64

      optimizer:
        type: "adamw"
        lr:
          vision_encoder: 1e-4  # for LoRA; 2e-5 for full fine-tune
          projector: 5e-5
        betas: [0.9, 0.95]
        eps: 1e-8
        weight_decay: 0.05

      scheduler:
        type: "cosine"
        warmup_steps: 1000
        min_lr: 0.0

      precision:
        mixed_precision: "bf16"
        gradient_checkpointing: true

      deepspeed:
        enabled: true
        stage: 2

    # Hyperparameters (300M Mini)
    hyperparameters_mini:
      max_steps: 10000
      eval_steps: 1000
      save_steps: 2500
      logging_steps: 50

      batch_size:
        per_device: 2
        gradient_accumulation_steps: 32
        effective_batch_size: 128

      optimizer:
        type: "adamw"
        lr:
          vision_encoder: 2e-4
          projector: 1e-4
        betas: [0.9, 0.95]
        eps: 1e-8
        weight_decay: 0.05

      scheduler:
        type: "cosine"
        warmup_steps: 500
        min_lr: 0.0

      precision:
        mixed_precision: "bf16"
        gradient_checkpointing: false

    # Special training for modifications
    modification_training:
      # 2D-RoPE progressive unfreezing
      rope_progressive:
        enabled: true  # for B1-B5
        use_progressive_unfreezing: true  # as defined in model.rope_2d.progressive_unfreezing

      # Window attention initialization
      window_attn_init:
        enabled: true  # for B2-B5
        copy_from_global: true
        warmup_window_layers: 1000  # steps to warmup window attention layers

      # Spatial features warmup
      spatial_features_warmup:
        enabled: true  # for B3a/b/c, B4, B5
        higher_lr_for_spatial: true
        spatial_module_lr_multiplier: 2.0  # 2x LR for spatial modules

      # Adaptive compression warmup
      compression_warmup:
        enabled: true  # for B4, B5
        use_warmup_schedule: true  # as defined in model.adaptive_compression
        warmup_steps: 2000

  # ============================================================================
  # Stage 3: End-to-End Fine-tuning
  # ============================================================================
  stage3_end_to_end:
    enabled: true
    name: "end_to_end_finetuning"
    description: "Full bilingual instruction tuning"

    # What to train
    trainable:
      vision_encoder: false  # frozen (preserve Stage 2 adaptations)
      projector: false       # frozen
      llm_decoder: true      # LoRA or full fine-tune

    # Data
    data:
      sources: ["llava_instruct", "sharegpt4v", "textvqa", "docvqa", "chartqa",
                "table_llava", "plotqa", "infovqa", "mathvista",
                "k_dtcbench_train", "varco_vision_ko", "korean_ocr", "korean_ui", "korean_math"]
      mixing_weights: [0.25, 0.15, 0.05, 0.05, 0.05, 0.08, 0.05, 0.02, 0.01,
                       0.10, 0.08, 0.05, 0.03, 0.03]
      task_type: "vqa"
      max_length: 2048
      total_samples: 700000
      korean_ratio: 0.40
      english_ratio: 0.60

    # Hyperparameters (6B)
    hyperparameters_6b:
      max_steps: 50000
      eval_steps: 2500
      save_steps: 5000
      logging_steps: 100

      batch_size:
        per_device: 2
        gradient_accumulation_steps: 16
        effective_batch_size: 64  # 2 * 2 GPUs * 16

      optimizer:
        type: "adamw"
        lr:
          lora: 2e-4
        betas: [0.9, 0.95]
        eps: 1e-8
        weight_decay: 0.05

      scheduler:
        type: "cosine"
        warmup_steps: 2000
        min_lr: 0.0

      precision:
        mixed_precision: "bf16"
        gradient_checkpointing: true

      deepspeed:
        enabled: true
        stage: 2

    # Hyperparameters (300M Mini)
    hyperparameters_mini:
      max_steps: 25000
      eval_steps: 1000
      save_steps: 2500
      logging_steps: 50

      batch_size:
        per_device: 4
        gradient_accumulation_steps: 8
        effective_batch_size: 64

      optimizer:
        type: "adamw"
        lr:
          lora: 2e-4
        betas: [0.9, 0.95]
        eps: 1e-8
        weight_decay: 0.05

      scheduler:
        type: "cosine"
        warmup_steps: 1000
        min_lr: 0.0

      precision:
        mixed_precision: "bf16"
        gradient_checkpointing: false

  # ============================================================================
  # Common Training Settings
  # ============================================================================
  common:
    # Regularization
    regularization:
      gradient_clip_norm: 1.0
      label_smoothing: 0.0
      dropout: 0.0

    # DeepSpeed configuration
    deepspeed:
      enabled: true
      config_file: null  # auto-generated
      stage: 2  # ZeRO-2
      offload_optimizer: true
      offload_param: false
      cpu_offload: true
      gradient_accumulation_steps: null  # inherit from batch_size
      gradient_clipping: 1.0
      fp16:
        enabled: false
      bf16:
        enabled: true
      zero_optimization:
        stage: 2
        allgather_partitions: true
        reduce_scatter: true
        overlap_comm: true
        contiguous_gradients: true

    # Flash Attention
    flash_attention:
      enabled: true
      version: 2
      use_triton: false

# ============================================================================
# DATA CONFIGURATION
# ============================================================================
data:
  # Dataset paths
  paths:
    # Korean datasets
    k_dtcbench_train: "/NetDisk/juyeon/research/data/korean/k_dtcbench/train"
    k_dtcbench_test: "/NetDisk/juyeon/research/data/korean/k_dtcbench/test"
    varco_vision_ko: "/NetDisk/juyeon/research/data/korean/varco_vision"
    korean_ocr: "/NetDisk/juyeon/research/data/korean/ocr_synthetic"
    korean_ui: "/NetDisk/juyeon/research/data/korean/ui_screenshots"
    korean_math: "/NetDisk/juyeon/research/data/korean/math_translated"
    korean_captions: "/NetDisk/juyeon/research/data/korean/captions"

    # English datasets
    llava_instruct: "/NetDisk/juyeon/research/data/english/llava_v1_5_mix665k"
    sharegpt4v: "/NetDisk/juyeon/research/data/english/sharegpt4v_100k"
    textvqa: "/NetDisk/juyeon/research/data/english/textvqa"
    docvqa: "/NetDisk/juyeon/research/data/english/docvqa"
    chartqa: "/NetDisk/juyeon/research/data/english/chartqa"
    table_llava: "/NetDisk/juyeon/research/data/english/table_llava"
    plotqa: "/NetDisk/juyeon/research/data/english/plotqa"
    infovqa: "/NetDisk/juyeon/research/data/english/infovqa"
    mathvista: "/NetDisk/juyeon/research/data/english/mathvista"
    cc3m_en: "/NetDisk/juyeon/research/data/english/cc3m"
    cc3m_ko: "/NetDisk/juyeon/research/data/korean/cc3m_translated"

  # Preprocessing
  preprocessing:
    image_size: 448
    patch_size: 14
    dynamic_resolution: true
    max_tiles: 12  # for 6B; 6 for Mini
    pad_to_square: true
    normalize:
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]

    # Augmentation (Stage 2 only)
    augmentation:
      enabled: false
      random_crop: false
      random_flip: false
      color_jitter: false

  # Tokenization
  tokenization:
    tokenizer: "internlm/internlm2-chat-8b"
    max_length: 2048
    padding: "max_length"
    truncation: true
    add_special_tokens: true

  # Data loading
  dataloader:
    num_workers: 8
    pin_memory: true
    prefetch_factor: 2
    persistent_workers: true
    drop_last: true
    shuffle: true

# ============================================================================
# EVALUATION CONFIGURATION
# ============================================================================
evaluation:
  # Benchmark suite
  benchmarks:
    # General VQA (High Priority)
    mmbench_en:
      path: "/NetDisk/juyeon/research/data/benchmarks/mmbench/en"
      metric: "accuracy"
      priority: "high"
      language: "en"

    mmbench_ko:
      path: "/NetDisk/juyeon/research/data/benchmarks/mmbench/ko"
      metric: "accuracy"
      priority: "high"
      language: "ko"

    mmstar:
      path: "/NetDisk/juyeon/research/data/benchmarks/mmstar"
      metric: "accuracy"
      priority: "high"
      language: "en"

    # OCR & Text (High Priority)
    ocr_bench:
      path: "/NetDisk/juyeon/research/data/benchmarks/ocr_bench"
      metric: "f1"
      priority: "high"
      language: "en"

    textvqa:
      path: "/NetDisk/juyeon/research/data/benchmarks/textvqa/val"
      metric: "vqa_score"
      priority: "high"
      language: "en"

    korean_ocr:
      path: "/NetDisk/juyeon/research/data/benchmarks/korean_ocr"
      metric: "cer"  # character error rate
      priority: "critical"
      language: "ko"

    # Chart Understanding (High Priority)
    chartqa:
      path: "/NetDisk/juyeon/research/data/benchmarks/chartqa/test"
      metric: "relaxed_accuracy"
      priority: "high"
      language: "en"

    charxiv:
      path: "/NetDisk/juyeon/research/data/benchmarks/charxiv"
      metric: "accuracy"
      priority: "high"
      language: "en"

    # Table Understanding (High Priority)
    wtq:
      path: "/NetDisk/juyeon/research/data/benchmarks/wtq"
      metric: "accuracy"
      priority: "high"
      language: "en"

    # Document Understanding (High Priority)
    docvqa:
      path: "/NetDisk/juyeon/research/data/benchmarks/docvqa/test"
      metric: "anls"
      priority: "high"
      language: "en"

    infovqa:
      path: "/NetDisk/juyeon/research/data/benchmarks/infovqa/test"
      metric: "anls"
      priority: "medium"
      language: "en"

    # Math Reasoning (High Priority)
    mathvista:
      path: "/NetDisk/juyeon/research/data/benchmarks/mathvista"
      metric: "accuracy"
      priority: "high"
      language: "en"

    # Korean-Specific (Critical)
    k_dtcbench:
      path: "/NetDisk/juyeon/research/data/benchmarks/k_dtcbench/test"
      metric: "accuracy"
      priority: "critical"
      language: "ko"

    # Secondary benchmarks
    seed_bench:
      path: "/NetDisk/juyeon/research/data/benchmarks/seed_bench"
      metric: "accuracy"
      priority: "medium"
      language: "en"

    mchartqa:
      path: "/NetDisk/juyeon/research/data/benchmarks/mchartqa"
      metric: "accuracy"
      priority: "medium"
      language: "multi"

    tabfact:
      path: "/NetDisk/juyeon/research/data/benchmarks/tabfact"
      metric: "accuracy"
      priority: "medium"
      language: "en"

    mathverse:
      path: "/NetDisk/juyeon/research/data/benchmarks/mathverse"
      metric: "accuracy"
      priority: "medium"
      language: "en"

  # Aggregated metrics
  aggregated_metrics:
    korean_average:
      benchmarks: ["k_dtcbench", "mmbench_ko", "korean_ocr"]
      weight: "equal"

    english_average:
      benchmarks: ["mmbench_en", "textvqa", "ocr_bench", "docvqa", "chartqa", "mathvista"]
      weight: "equal"

    structured_content_average:
      benchmarks: ["chartqa", "docvqa", "wtq", "k_dtcbench"]
      weight: "equal"
      purpose: "Test multi-scale and spatial features effectiveness"

    overall_average:
      benchmarks: "all_high_priority"
      weight: "equal"

  # Inference settings
  inference:
    temperature: 0.0  # greedy decoding
    top_p: null
    top_k: null
    max_new_tokens: 512
    repetition_penalty: 1.0
    do_sample: false
    num_beams: 1
    batch_size: 4

  # Statistical analysis
  statistical:
    num_seeds: 3
    seeds: [42, 123, 456]
    confidence_level: 0.95
    significance_level: 0.05
    test: "paired_t_test"
    multiple_comparison_correction: "bonferroni"
    report_std: true
    report_ci: true
    decimal_places: 4

  # Efficiency profiling
  efficiency:
    measure_flops: true
    measure_latency: true
    measure_memory: true
    measure_throughput: true

    # Test resolutions
    test_resolutions:
      - [448, 448]   # base
      - [672, 672]   # 1.5x
      - [896, 896]   # 2x
      - [1344, 1344] # 3x (if window attention enabled)

    # Metrics
    metrics:
      - "inference_flops"
      - "time_to_first_token"
      - "total_generation_time"
      - "peak_gpu_memory"
      - "visual_tokens_per_image"
      - "images_per_second"

# ============================================================================
# EXPERIMENT VARIANTS
# ============================================================================
experiments:
  # ============================================================================
  # Phase 1: Mini-InternVL (300M) Validation Experiments
  # ============================================================================

  A1_Mini:
    name: "baseline_mini"
    description: "Mini-InternVL baseline (300M, no modifications)"
    phase: 1
    base_model: "mini"

    model:
      rope_2d: {enabled: false}
      window_attention: {enabled: false}
      spatial_features: {type: "none", enabled: false}
      adaptive_compression: {enabled: false}

    training:
      stage1: {enabled: true, config: "hyperparameters_mini"}
      stage2: {enabled: true, config: "hyperparameters_mini"}
      stage3: {enabled: true, config: "hyperparameters_mini"}

  B1_Mini:
    name: "rope_mini"
    description: "Mini-InternVL + 2D-RoPE (300M validation)"
    phase: 1
    base_model: "mini"

    model:
      rope_2d: {enabled: true}
      window_attention: {enabled: false}
      spatial_features: {type: "none", enabled: false}
      adaptive_compression: {enabled: false}

    training:
      stage1: {enabled: true, config: "hyperparameters_mini"}
      stage2: {enabled: true, config: "hyperparameters_mini", use_progressive_unfreezing: true}
      stage3: {enabled: true, config: "hyperparameters_mini"}

  B2_Mini:
    name: "rope_window_mini"
    description: "Mini-InternVL + 2D-RoPE + Window Attention (300M validation)"
    phase: 1
    base_model: "mini"

    model:
      rope_2d: {enabled: true}
      window_attention: {enabled: true}
      spatial_features: {type: "none", enabled: false}
      adaptive_compression: {enabled: false}

    training:
      stage1: {enabled: true, config: "hyperparameters_mini"}
      stage2: {enabled: true, config: "hyperparameters_mini", use_progressive_unfreezing: true}
      stage3: {enabled: true, config: "hyperparameters_mini"}

  B3a_Mini:
    name: "rope_window_sp_mini"
    description: "B2_Mini + LLaVA-SP spatial tokens (300M validation)"
    phase: 1
    base_model: "mini"

    model:
      rope_2d: {enabled: true}
      window_attention: {enabled: true}
      spatial_features: {type: "llava_sp", enabled: true}
      adaptive_compression: {enabled: false}

    training:
      stage1: {enabled: true, config: "hyperparameters_mini"}
      stage2: {enabled: true, config: "hyperparameters_mini", use_progressive_unfreezing: true}
      stage3: {enabled: true, config: "hyperparameters_mini"}

  B3b_Mini:
    name: "rope_window_piip_mini"
    description: "B2_Mini + PIIP multi-scale (300M validation)"
    phase: 1
    base_model: "mini"

    model:
      rope_2d: {enabled: true}
      window_attention: {enabled: true}
      spatial_features: {type: "piip", enabled: true}
      adaptive_compression: {enabled: false}

    training:
      stage1: {enabled: true, config: "hyperparameters_mini"}
      stage2: {enabled: true, config: "hyperparameters_mini", use_progressive_unfreezing: true}
      stage3: {enabled: true, config: "hyperparameters_mini"}

  B3c_Mini:
    name: "rope_window_combined_mini"
    description: "B2_Mini + Combined spatial (SP + PIIP) (300M validation)"
    phase: 1
    base_model: "mini"

    model:
      rope_2d: {enabled: true}
      window_attention: {enabled: true}
      spatial_features: {type: "combined", enabled: true}
      adaptive_compression: {enabled: false}

    training:
      stage1: {enabled: true, config: "hyperparameters_mini"}
      stage2: {enabled: true, config: "hyperparameters_mini", use_progressive_unfreezing: true}
      stage3: {enabled: true, config: "hyperparameters_mini"}

  B4_Mini:
    name: "full_internvit_next_mini"
    description: "B3[best]_Mini + Adaptive Compression (300M validation)"
    phase: 1
    base_model: "mini"
    depends_on: "B3a_Mini OR B3b_Mini OR B3c_Mini"  # select best from B3 variants

    model:
      rope_2d: {enabled: true}
      window_attention: {enabled: true}
      spatial_features: {type: "TBD", enabled: true}  # set to best from B3a/b/c
      adaptive_compression: {enabled: true}

    training:
      stage1: {enabled: true, config: "hyperparameters_mini"}
      stage2: {enabled: true, config: "hyperparameters_mini", use_progressive_unfreezing: true, use_compression_warmup: true}
      stage3: {enabled: true, config: "hyperparameters_mini"}

  # ============================================================================
  # Phase 2: Full InternVL2.5-8B (6B) Production Experiments
  # ============================================================================

  A0:
    name: "baseline_zero_shot"
    description: "InternVL2.5-8B zero-shot (no training)"
    phase: 2
    base_model: "6b"

    model:
      rope_2d: {enabled: false}
      window_attention: {enabled: false}
      spatial_features: {type: "none", enabled: false}
      adaptive_compression: {enabled: false}

    training:
      stage1: {enabled: false}
      stage2: {enabled: false}
      stage3: {enabled: false}

  A1:
    name: "baseline_finetuned"
    description: "InternVL2.5-8B baseline (fine-tuned, no modifications)"
    phase: 2
    base_model: "6b"

    model:
      rope_2d: {enabled: false}
      window_attention: {enabled: false}
      spatial_features: {type: "none", enabled: false}
      adaptive_compression: {enabled: false}

    training:
      stage1: {enabled: true, config: "hyperparameters_6b"}
      stage2: {enabled: true, config: "hyperparameters_6b"}
      stage3: {enabled: true, config: "hyperparameters_6b"}

  B1:
    name: "rope_6b"
    description: "InternVL2.5-8B + 2D-RoPE (6B)"
    phase: 2
    base_model: "6b"

    model:
      rope_2d: {enabled: true}
      window_attention: {enabled: false}
      spatial_features: {type: "none", enabled: false}
      adaptive_compression: {enabled: false}

    training:
      stage1: {enabled: true, config: "hyperparameters_6b"}
      stage2: {enabled: true, config: "hyperparameters_6b", use_progressive_unfreezing: true}
      stage3: {enabled: true, config: "hyperparameters_6b"}

  B2:
    name: "rope_window_6b"
    description: "InternVL2.5-8B + 2D-RoPE + Window Attention (6B)"
    phase: 2
    base_model: "6b"

    model:
      rope_2d: {enabled: true}
      window_attention: {enabled: true}
      spatial_features: {type: "none", enabled: false}
      adaptive_compression: {enabled: false}

    training:
      stage1: {enabled: true, config: "hyperparameters_6b"}
      stage2: {enabled: true, config: "hyperparameters_6b", use_progressive_unfreezing: true}
      stage3: {enabled: true, config: "hyperparameters_6b"}

  B3_best:
    name: "rope_window_spatial_6b"
    description: "B2 + Best spatial features from Phase 1 (6B)"
    phase: 2
    base_model: "6b"
    depends_on: "Phase 1 B3 results"

    model:
      rope_2d: {enabled: true}
      window_attention: {enabled: true}
      spatial_features: {type: "TBD", enabled: true}  # set to best from Phase 1
      adaptive_compression: {enabled: false}

    training:
      stage1: {enabled: true, config: "hyperparameters_6b"}
      stage2: {enabled: true, config: "hyperparameters_6b", use_progressive_unfreezing: true}
      stage3: {enabled: true, config: "hyperparameters_6b"}

  B4:
    name: "rope_window_spatial_compression_6b"
    description: "B3_best + Adaptive Compression (6B)"
    phase: 2
    base_model: "6b"
    depends_on: "B3_best"

    model:
      rope_2d: {enabled: true}
      window_attention: {enabled: true}
      spatial_features: {type: "TBD", enabled: true}  # same as B3_best
      adaptive_compression: {enabled: true}

    training:
      stage1: {enabled: true, config: "hyperparameters_6b"}
      stage2: {enabled: true, config: "hyperparameters_6b", use_progressive_unfreezing: true, use_compression_warmup: true}
      stage3: {enabled: true, config: "hyperparameters_6b"}

  B5:
    name: "internvit_next_full"
    description: "Full InternViT-Next (B4 with 3 seeds) - FINAL MODEL"
    phase: 2
    base_model: "6b"
    depends_on: "B4"
    num_seeds: 3

    model:
      rope_2d: {enabled: true}
      window_attention: {enabled: true}
      spatial_features: {type: "TBD", enabled: true}  # same as B4
      adaptive_compression: {enabled: true}

    training:
      stage1: {enabled: true, config: "hyperparameters_6b"}
      stage2: {enabled: true, config: "hyperparameters_6b", use_progressive_unfreezing: true, use_compression_warmup: true}
      stage3: {enabled: true, config: "hyperparameters_6b"}

# ============================================================================
# LOGGING & MONITORING
# ============================================================================
logging:
  output_dir: "/NetDisk/juyeon/research/results/EXP-20260212-001/logs"

  wandb:
    enabled: true
    project: "internvit-next"
    entity: null  # set to wandb username
    name: null  # auto-generated from experiment ID
    tags: ["vision-encoder", "internvit", "2d-rope", "window-attention", "korean", "bilingual"]

  tensorboard:
    enabled: true
    log_dir: "/NetDisk/juyeon/research/results/EXP-20260212-001/tensorboard"

  metrics:
    log_interval: 100
    eval_interval: 2500
    save_interval: 5000
    log_level: "INFO"

  checkpoints:
    save_dir: "/NetDisk/juyeon/research/results/EXP-20260212-001/checkpoints"
    save_total_limit: 5
    save_strategy: "steps"
    save_steps: 5000
    load_best_model_at_end: true
    metric_for_best_model: "eval_loss"
    greater_is_better: false

    # Checkpoint reuse
    reuse_checkpoints:
      enabled: true
      stage1_projector_mini: "checkpoints/shared/stage1_projector_mini.pt"
      stage1_projector_6b: "checkpoints/shared/stage1_projector_6b.pt"
      rope_adapted_mini: "checkpoints/B1_Mini/stage2_rope_adapted.pt"
      rope_adapted_6b: "checkpoints/B1/stage2_rope_adapted.pt"

# ============================================================================
# HARDWARE & INFRASTRUCTURE
# ============================================================================
hardware:
  gpus:
    num_gpus: 2
    gpu_type: "NVIDIA A100-PCIE-80GB"
    memory_per_gpu: "80GB"
    compute_capability: "8.0"

  distributed:
    backend: "nccl"
    init_method: "env://"
    world_size: 2
    rank: 0

  cpu:
    num_workers: 8
    pin_memory: true

  storage:
    cache_dir: "/NetDisk/juyeon/research/.cache"
    data_dir: "/NetDisk/juyeon/research/data"
    output_dir: "/NetDisk/juyeon/research/results"
    checkpoint_dir: "/NetDisk/juyeon/research/results/EXP-20260212-001/checkpoints"

    # Space management
    estimated_total_space: "1.0TB"
    checkpoint_cleanup:
      enabled: true
      keep_best_only: true
      compress_logs: true

# ============================================================================
# REPRODUCIBILITY
# ============================================================================
reproducibility:
  seeds: [42, 123, 456]
  deterministic: true
  cudnn_benchmark: false
  cudnn_deterministic: true
  set_seed_everywhere: true

# ============================================================================
# EXPERIMENT EXECUTION
# ============================================================================
execution:
  # Priority order (critical path)
  priority_order:
    phase1:
      - "A1_Mini"
      - "B1_Mini"
      - "B2_Mini"
      - "B3a_Mini"
      - "B3b_Mini"
      - "B3c_Mini"
      - "B4_Mini"

    phase2:
      - "A0"
      - "A1"
      - "B1"
      - "B2"
      - "B3_best"
      - "B4"
      - "B5"

  # Parallel execution (if resources available)
  parallel:
    enabled: false  # sequential by default
    max_parallel_jobs: 1

  # Early stopping
  early_stopping:
    enabled: true
    patience: 5  # eval steps
    min_delta: 0.001
    monitor: "eval_loss"

# ============================================================================
# SUCCESS CRITERIA
# ============================================================================
success_criteria:
  # Quantitative targets (B5 vs A1)
  performance:
    korean_average: {target: 0.63, baseline: 0.58, min_improvement: 0.05}
    english_average: {target: 0.71, baseline: 0.68, min_improvement: 0.03}
    structured_content_average: {target: 0.68, baseline: 0.62, min_improvement: 0.06}
    overall_average: {target: 0.68, baseline: 0.63, min_improvement: 0.05}
    docvqa: {target: 0.85, baseline: 0.82, min_improvement: 0.03}
    chartqa: {target: 0.76, baseline: 0.74, min_improvement: 0.02}
    ocr_bench: {target: 680, baseline: 650, min_improvement: 30}
    k_dtcbench: {target: 0.62, baseline: 0.55, min_improvement: 0.07}

  # Efficiency targets (B5 vs A1)
  efficiency:
    inference_flops: {target_ratio: 0.70, baseline_ratio: 1.0, max_increase: 0.30}  # 30% reduction
    latency_ttft: {target_ratio: 0.80, baseline_ratio: 1.0, max_increase: 0.20}  # 20% reduction
    visual_tokens: {target_ratio: 0.70, baseline_ratio: 1.0, max_increase: 0.30}  # 30% reduction

  # Component contributions (ablation)
  component_gains:
    rope_2d: {min_gain: 0.010, key_benchmarks: ["docvqa", "chartqa"]}  # 1%
    window_attention: {max_loss: 0.005, flops_reduction: 0.50}  # -0.5% accuracy OK if 50% FLOPs reduction
    spatial_features: {min_gain: 0.020, key_benchmarks: ["chartqa", "docvqa", "wtq", "k_dtcbench"]}  # 2%
    adaptive_compression: {max_loss: 0.002, token_reduction: 0.30}  # -0.2% OK if 30% token reduction

  # Statistical significance
  statistical:
    significance_level: 0.05
    min_seeds: 3
    max_std: 0.01

  # Publishability
  publishability:
    target_venue: ["CVPR", "NeurIPS", "ICLR", "ACL"]
    novelty: "First 2D-RoPE + window attention for InternViT-6B"
    contribution: "Vision encoder architecture > data scaling for low-resource languages"
    empirical_strength: "12+ benchmarks, 3 seeds, statistical tests, comprehensive ablation"
