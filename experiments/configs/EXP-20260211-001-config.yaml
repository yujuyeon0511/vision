# TBI-MLLM Experiment Configuration
# Experiment ID: EXP-20260211-001
# Created: 2026-02-11
# Description: Comprehensive config for Tripartite Architectural Innovation for Bilingual MLLMs

# ============================================================================
# GLOBAL SETTINGS
# ============================================================================
experiment:
  id: "EXP-20260211-001"
  name: "TBI-MLLM-Systematic-Ablation"
  description: "Systematic evaluation of vision encoder, projector, and decoder innovations for bilingual Korean+English MLLM"
  author: "Juyeon"
  created_at: "2026-02-11"
  base_model: "OpenGVLab/InternVL2_5-8B"
  seeds: [42, 123, 456]
  output_dir: "/NetDisk/juyeon/research/results/EXP-20260211-001"
  cache_dir: "/NetDisk/juyeon/research/.cache"

# ============================================================================
# MODEL ARCHITECTURE
# ============================================================================
model:
  # Base model configuration
  base:
    vision_encoder: "OpenGVLab/InternViT-6B-448px-V1-5"
    projector: "mlp"  # default, will be replaced in ablations
    llm_decoder: "internlm/internlm2-chat-8b"
    image_size: 448
    num_tiles: 12  # max tiles for dynamic resolution
    patch_size: 14
    vision_hidden_size: 4096
    llm_hidden_size: 4096
    projector_hidden_size: 4096
    num_query_tokens: 256  # for C-Abstractor variants

  # Stage 1: Multi-Scale Vision Encoder
  vision_encoder:
    # 2D-RoPE configuration (for B1-B3, E1-E2, F1-F4)
    rope_2d:
      enabled: false  # set to true for experiments with 2D-RoPE
      base_freq: 10000.0
      dim_h: 64
      dim_w: 64
      interpolation: "linear"
      scaling_factor: 1.0

    # Multi-scale pyramid features (for B2-B3, E1-E2, F1-F4)
    pyramid:
      enabled: false  # set to true for multi-scale experiments
      scales: [0.25, 0.5, 1.0]  # 1/4, 1/2, 1x resolution
      layer_indices: [8, 16, 24]  # extract from these InternViT layers (total 24 layers)
      output_channels: [1024, 2048, 4096]
      scale_names: ["L1_low", "L2_mid", "L3_high"]
      aggregation: "concatenate"  # or "average", "attention"

    # PyramidDrop for efficiency (for B3, E2, F1-F4)
    pyramid_drop:
      enabled: false
      drop_rate_per_scale: [0.5, 0.3, 0.1]  # aggressive at low res, minimal at high res
      strategy: "attention_based"  # "random", "attention_based", "spatial_uniform"
      attention_threshold: 0.1  # drop tokens with attention < threshold
      min_tokens_per_scale: 64  # safety limit
      apply_at_inference: true  # also apply during inference for speedup
      warmup_steps: 2000  # gradually increase drop rate during warmup

    # LoRA fine-tuning for vision encoder (Phase 2)
    lora:
      enabled: false  # true for Phase 2 fine-tuning
      rank: 32
      alpha: 8
      dropout: 0.05
      target_modules: ["qkv", "proj"]  # ViT attention modules
      layers_to_train: [18, 19, 20, 21, 22, 23]  # top 6 layers only

  # Stage 2: Projector
  projector:
    type: "mlp"  # "mlp", "c_abstractor", "pyramid_c_abstractor"

    # MLP projector (baseline)
    mlp:
      hidden_size: 4096
      num_layers: 2
      activation: "gelu"
      dropout: 0.0

    # C-Abstractor (for C1, E1-E2, F1-F4)
    c_abstractor:
      num_blocks: 6
      input_channels: 4096
      hidden_channels: 2048
      output_tokens: 256
      kernel_size: 3
      stride: 1
      padding: 1
      use_se_attention: true
      se_reduction: 16
      bottleneck_reduction: 4
      activation: "gelu"
      norm: "layer_norm"

    # Pyramid C-Abstractor (for C2, E2, F1-F4)
    pyramid_c_abstractor:
      inherit_from: "c_abstractor"
      multi_scale_input: true
      cross_scale_fusion: "conv_bottleneck"  # "concat", "add", "conv_bottleneck", "attention"
      fusion_channels: [1024, 2048, 4096]
      adaptive_pool_size: [16, 16]  # per-scale spatial pooling before fusion
      fusion_hidden_dim: 2048
      output_tokens: 256
      use_cross_scale_attention: true  # SE attention across scales

  # Stage 3: Decoder Adaptation
  decoder:
    # LoRA fine-tuning (for D1, F1, and Phase 3 baseline)
    lora:
      enabled: false  # set to true for LoRA experiments
      rank: 64
      alpha: 16  # effective LR multiplier = alpha/rank = 0.25
      dropout: 0.05
      target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
      modules_to_save: ["embed_tokens", "lm_head"]  # always train these

    # DARE+TIES model merging (for D2-D4, F2-F4)
    merging:
      enabled: false
      method: "dare_ties"  # "dare_ties", "ties", "dare", "slerp", "linear"

      # DARE (Drop And REscale) parameters
      dare:
        drop_rate: 0.9  # drop 90% of fine-tuned weight deltas
        rescale: true
        rescale_factor: null  # auto-compute as 1/(1-drop_rate)

      # TIES (TrIm, Elect, and mErge) parameters
      ties:
        elect_method: "mass"  # "mass" (majority vote), "sum", "random"
        top_k_percent: 0.2  # keep top 20% of weights by magnitude (TRIM step)

      # Merging weights (lambda values)
      weights:
        lambda_en: 0.6  # weight for original English model
        lambda_ko: 0.4  # weight for Korean fine-tuned model
        normalize: true

      # Model paths for merging
      models:
        base_model: "OpenGVLab/InternVL2_5-8B"  # original English model
        korean_model: null  # path to Korean fine-tuned model (set at runtime)
        output_path: null  # path to save merged model (set at runtime)

    # MoLE (Mixture of Language Experts) for D4, F4
    mole:
      enabled: false
      num_experts: 3  # ko_specialist, en_specialist, shared
      expert_type: "ffn"  # replace FFN layers with MoE FFNs
      layers_to_replace: [16, 20, 24, 28]  # top 4 decoder layers (InternLM2 has 32 layers)

      # Router configuration
      router:
        type: "learned"  # "learned" or "rule_based"
        hidden_dim: 256
        num_layers: 2
        activation: "relu"
        temperature: 1.0
        use_language_hint: true  # use language ID as input feature

      # Expert initialization
      init:
        ko_expert_from: "korean_finetuned_model"
        en_expert_from: "original_internvl_model"
        shared_expert_from: "merged_model"  # from D3/F3

      # Training
      load_balancing_loss_weight: 0.01
      expert_capacity_factor: 1.25
      router_z_loss_weight: 0.001  # regularization for router logits

# ============================================================================
# TRAINING CONFIGURATION
# ============================================================================
training:
  # Multi-phase training protocol
  phases:
    # Phase 1: Projector Pre-training
    phase1:
      enabled: true
      name: "projector_pretraining"
      description: "Train projector on image-caption alignment task"
      max_steps: 10000
      eval_steps: 1000
      save_steps: 2000
      logging_steps: 100

      # What to train
      trainable:
        vision_encoder: false
        projector: true
        llm_decoder: false

      # Data
      data:
        sources: ["cc3m_en", "cc3m_ko", "korean_captions"]
        mixing_weights: [0.4, 0.3, 0.3]
        task_type: "caption"
        max_length: 256

      # Hyperparameters
      batch_size:
        per_device: 2
        gradient_accumulation_steps: 64
        effective_batch_size: 256  # per_device * num_gpus * gradient_accum

      optimizer:
        type: "adamw"
        lr: 1e-3
        betas: [0.9, 0.95]
        eps: 1e-8
        weight_decay: 0.05

      scheduler:
        type: "cosine"
        warmup_steps: 500
        min_lr: 0.0

      precision:
        mixed_precision: "bf16"
        gradient_checkpointing: false

    # Phase 2: Vision+Projector Fine-tuning
    phase2:
      enabled: true
      name: "vision_projector_finetuning"
      description: "Adapt vision encoder and projector to bilingual text-rich images"
      max_steps: 20000
      eval_steps: 2000
      save_steps: 5000
      logging_steps: 100

      # What to train
      trainable:
        vision_encoder: true  # LoRA or last N layers
        projector: true
        llm_decoder: false

      # Data
      data:
        sources: ["textvqa", "docvqa", "chartqa", "korean_ocr", "korean_ui", "plotqa"]
        mixing_weights: [0.2, 0.15, 0.15, 0.2, 0.15, 0.15]
        task_type: "vqa"
        max_length: 512

      # Hyperparameters
      batch_size:
        per_device: 2
        gradient_accumulation_steps: 32
        effective_batch_size: 128

      optimizer:
        type: "adamw"
        lr:
          vision_encoder: 1e-4  # for LoRA; use 2e-5 for full fine-tune
          projector: 5e-5
        betas: [0.9, 0.95]
        eps: 1e-8
        weight_decay: 0.05

      scheduler:
        type: "cosine"
        warmup_steps: 1000
        min_lr: 0.0

      precision:
        mixed_precision: "bf16"
        gradient_checkpointing: true

    # Phase 3: Full or Decoder-Specific Fine-tuning
    phase3:
      enabled: true
      name: "full_finetuning"
      description: "End-to-end task adaptation on bilingual instruction data"
      max_steps: 50000
      eval_steps: 2500
      save_steps: 5000
      logging_steps: 100

      # What to train (varies by experiment)
      trainable:
        vision_encoder: false  # typically frozen in Phase 3
        projector: false       # typically frozen in Phase 3
        llm_decoder: true      # LoRA or full fine-tune

      # Data mixing (varies by decoder strategy)
      data:
        # Standard bilingual fine-tuning (A1, B, C, E, D1, F1)
        standard:
          sources: ["llava_instruct", "sharegpt4v", "textvqa", "docvqa", "chartqa", "table_llava",
                    "plotqa", "infovqa", "mathvista", "k_dtcbench", "varco_vision_ko", "korean_ocr",
                    "korean_ui", "korean_math"]
          mixing_weights: [0.25, 0.15, 0.05, 0.05, 0.05, 0.08, 0.05, 0.02, 0.01,
                           0.10, 0.08, 0.05, 0.03, 0.03]
          korean_ratio: 0.40
          english_ratio: 0.60

        # Korean-only fine-tuning (for D3, D4, F3, F4 - creating KO specialist)
        korean_only:
          sources: ["k_dtcbench", "varco_vision_ko", "korean_ocr", "korean_ui", "korean_math"]
          mixing_weights: [0.35, 0.30, 0.20, 0.10, 0.05]
          korean_ratio: 1.0
          english_ratio: 0.0

        # Balanced for MoLE training (D4, F4 - after merging)
        mole_balanced:
          sources: ["llava_instruct", "sharegpt4v", "k_dtcbench", "varco_vision_ko"]
          mixing_weights: [0.30, 0.20, 0.30, 0.20]
          korean_ratio: 0.50
          english_ratio: 0.50

        task_type: "vqa"
        max_length: 2048

      # Hyperparameters
      batch_size:
        per_device: 2
        gradient_accumulation_steps: 16
        effective_batch_size: 64

      optimizer:
        type: "adamw"
        lr:
          full_model: 1e-5   # for full fine-tuning
          lora: 2e-4         # for LoRA
          mole_experts: 5e-5 # for MoLE expert training
        betas: [0.9, 0.95]
        eps: 1e-8
        weight_decay: 0.05

      scheduler:
        type: "cosine"
        warmup_steps: 2000
        min_lr: 0.0

      precision:
        mixed_precision: "bf16"
        gradient_checkpointing: true

  # Regularization
  regularization:
    gradient_clip_norm: 1.0
    label_smoothing: 0.0
    dropout: 0.0  # disabled, following InternVL best practice

  # DeepSpeed configuration
  deepspeed:
    enabled: true
    stage: 2  # ZeRO-2 (shard optimizer states)
    offload_optimizer: true
    offload_param: false
    cpu_offload: true
    gradient_accumulation_steps: null  # inherit from batch_size config
    gradient_clipping: 1.0
    fp16:
      enabled: false
    bf16:
      enabled: true
    zero_optimization:
      stage: 2
      allgather_partitions: true
      reduce_scatter: true
      overlap_comm: true
      contiguous_gradients: true

  # Flash Attention 2
  flash_attention:
    enabled: true
    version: 2

# ============================================================================
# DATA CONFIGURATION
# ============================================================================
data:
  # Dataset paths
  paths:
    # Korean datasets
    k_dtcbench: "/NetDisk/juyeon/research/data/korean/k_dtcbench"
    varco_vision_ko: "/NetDisk/juyeon/research/data/korean/varco_vision"
    korean_ocr: "/NetDisk/juyeon/research/data/korean/ocr_synthetic"
    korean_ui: "/NetDisk/juyeon/research/data/korean/ui_screenshots"
    korean_math: "/NetDisk/juyeon/research/data/korean/math_translated"
    korean_captions: "/NetDisk/juyeon/research/data/korean/captions"

    # English datasets
    llava_instruct: "/NetDisk/juyeon/research/data/english/llava_v1_5_mix665k"
    sharegpt4v: "/NetDisk/juyeon/research/data/english/sharegpt4v_100k"
    textvqa: "/NetDisk/juyeon/research/data/english/textvqa"
    docvqa: "/NetDisk/juyeon/research/data/english/docvqa"
    chartqa: "/NetDisk/juyeon/research/data/english/chartqa"
    table_llava: "/NetDisk/juyeon/research/data/english/table_llava"
    plotqa: "/NetDisk/juyeon/research/data/english/plotqa"
    infovqa: "/NetDisk/juyeon/research/data/english/infovqa"
    mathvista: "/NetDisk/juyeon/research/data/english/mathvista"
    cc3m_en: "/NetDisk/juyeon/research/data/english/cc3m"
    cc3m_ko: "/NetDisk/juyeon/research/data/korean/cc3m_translated"

  # Preprocessing
  preprocessing:
    image_size: 448
    dynamic_resolution: true
    max_tiles: 12
    pad_to_square: true
    normalize:
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]

  # Tokenization
  tokenization:
    tokenizer: "internlm/internlm2-chat-8b"
    max_length: 2048
    padding: "max_length"
    truncation: true
    add_special_tokens: true

  # Data loading
  dataloader:
    num_workers: 8
    pin_memory: true
    prefetch_factor: 2
    persistent_workers: true

# ============================================================================
# EVALUATION CONFIGURATION
# ============================================================================
evaluation:
  # Benchmarks
  benchmarks:
    # General VQA
    mmbench_en:
      path: "/NetDisk/juyeon/research/data/benchmarks/mmbench/en"
      metric: "accuracy"
      priority: "high"
      language: "en"

    mmbench_ko:
      path: "/NetDisk/juyeon/research/data/benchmarks/mmbench/ko"
      metric: "accuracy"
      priority: "high"
      language: "ko"

    mmstar:
      path: "/NetDisk/juyeon/research/data/benchmarks/mmstar"
      metric: "accuracy"
      priority: "high"
      language: "en"

    seed_bench:
      path: "/NetDisk/juyeon/research/data/benchmarks/seed_bench"
      metric: "accuracy"
      priority: "medium"
      language: "en"

    # OCR & Text
    ocr_bench:
      path: "/NetDisk/juyeon/research/data/benchmarks/ocr_bench"
      metric: "f1"
      priority: "high"
      language: "en"

    textvqa:
      path: "/NetDisk/juyeon/research/data/benchmarks/textvqa"
      metric: "vqa_score"
      priority: "high"
      language: "en"

    # Chart Understanding
    chartqa:
      path: "/NetDisk/juyeon/research/data/benchmarks/chartqa"
      metric: "relaxed_accuracy"
      priority: "high"
      language: "en"

    charxiv:
      path: "/NetDisk/juyeon/research/data/benchmarks/charxiv"
      metric: "accuracy"
      priority: "high"
      language: "en"

    mchartqa:
      path: "/NetDisk/juyeon/research/data/benchmarks/mchartqa"
      metric: "accuracy"
      priority: "medium"
      language: "multi"

    # Table Understanding
    wtq:
      path: "/NetDisk/juyeon/research/data/benchmarks/wtq"
      metric: "accuracy"
      priority: "high"
      language: "en"

    tabfact:
      path: "/NetDisk/juyeon/research/data/benchmarks/tabfact"
      metric: "accuracy"
      priority: "medium"
      language: "en"

    # Document Understanding
    docvqa:
      path: "/NetDisk/juyeon/research/data/benchmarks/docvqa"
      metric: "anls"
      priority: "high"
      language: "en"

    infovqa:
      path: "/NetDisk/juyeon/research/data/benchmarks/infovqa"
      metric: "anls"
      priority: "medium"
      language: "en"

    # Math Reasoning
    mathvista:
      path: "/NetDisk/juyeon/research/data/benchmarks/mathvista"
      metric: "accuracy"
      priority: "high"
      language: "en"

    mathverse:
      path: "/NetDisk/juyeon/research/data/benchmarks/mathverse"
      metric: "accuracy"
      priority: "medium"
      language: "en"

    # Korean-Specific
    k_dtcbench:
      path: "/NetDisk/juyeon/research/data/benchmarks/k_dtcbench"
      metric: "accuracy"
      priority: "critical"
      language: "ko"

    komm_bench:
      path: "/NetDisk/juyeon/research/data/benchmarks/komm_bench"
      metric: "accuracy"
      priority: "high"
      language: "ko"
      optional: true  # skip if not available

  # Aggregated metrics
  aggregated_metrics:
    korean_average:
      benchmarks: ["k_dtcbench", "mmbench_ko", "komm_bench"]
      weight: "equal"

    english_average:
      benchmarks: ["mmbench_en", "textvqa", "ocr_bench", "docvqa", "chartqa", "mathvista"]
      weight: "equal"

    structured_content_average:
      benchmarks: ["chartqa", "docvqa", "wtq"]
      weight: "equal"

    overall_average:
      benchmarks: "all"
      weight: "equal"

  # Inference settings
  inference:
    temperature: 0.0  # greedy decoding
    top_p: null
    top_k: null
    max_new_tokens: 512
    repetition_penalty: 1.0
    do_sample: false
    num_beams: 1
    batch_size: 4

  # Statistical analysis
  statistical:
    num_seeds: 3
    confidence_level: 0.95
    significance_level: 0.05
    test: "paired_t_test"
    multiple_comparison_correction: "bonferroni"
    report_std: true
    report_ci: true
    decimal_places: 4

# ============================================================================
# EXPERIMENT VARIANTS
# ============================================================================
experiments:
  # EXP-A: Baseline
  A0:
    name: "baseline_zero_shot"
    description: "InternVL2.5-8B zero-shot evaluation (no training)"
    model:
      vision_encoder:
        rope_2d: {enabled: false}
        pyramid: {enabled: false}
        pyramid_drop: {enabled: false}
      projector:
        type: "mlp"
      decoder:
        lora: {enabled: false}
    training:
      phases:
        phase1: {enabled: false}
        phase2: {enabled: false}
        phase3: {enabled: false}

  A1:
    name: "baseline_bilingual_finetune"
    description: "InternVL2.5-8B with standard bilingual fine-tuning"
    model:
      vision_encoder:
        rope_2d: {enabled: false}
        pyramid: {enabled: false}
        pyramid_drop: {enabled: false}
      projector:
        type: "mlp"
      decoder:
        lora: {enabled: true, rank: 64, alpha: 16}
    training:
      phases:
        phase1: {enabled: true}
        phase2: {enabled: true}
        phase3: {enabled: true, data: "standard"}

  # EXP-B: Stage 1 (Vision Encoder)
  B1:
    name: "vision_2d_rope_only"
    description: "Replace absolute position embeddings with 2D-RoPE"
    model:
      vision_encoder:
        rope_2d: {enabled: true}
        pyramid: {enabled: false}
        pyramid_drop: {enabled: false}
      projector:
        type: "mlp"
      decoder:
        lora: {enabled: true, rank: 64, alpha: 16}
    training:
      phases:
        phase1: {enabled: true}
        phase2: {enabled: true}
        phase3: {enabled: true, data: "standard"}

  B2:
    name: "vision_multiscale"
    description: "2D-RoPE + multi-scale pyramid features"
    model:
      vision_encoder:
        rope_2d: {enabled: true}
        pyramid: {enabled: true}
        pyramid_drop: {enabled: false}
      projector:
        type: "mlp"
      decoder:
        lora: {enabled: true, rank: 64, alpha: 16}
    training:
      phases:
        phase1: {enabled: true}
        phase2: {enabled: true}
        phase3: {enabled: true, data: "standard"}

  B3:
    name: "vision_multiscale_pyramiddrop"
    description: "B2 + PyramidDrop for efficiency"
    model:
      vision_encoder:
        rope_2d: {enabled: true}
        pyramid: {enabled: true}
        pyramid_drop: {enabled: true}
      projector:
        type: "mlp"
      decoder:
        lora: {enabled: true, rank: 64, alpha: 16}
    training:
      phases:
        phase1: {enabled: true}
        phase2: {enabled: true}
        phase3: {enabled: true, data: "standard"}

  # EXP-C: Stage 2 (Projector)
  C1:
    name: "projector_c_abstractor"
    description: "Replace MLP with C-Abstractor (single-scale)"
    model:
      vision_encoder:
        rope_2d: {enabled: false}
        pyramid: {enabled: false}
        pyramid_drop: {enabled: false}
      projector:
        type: "c_abstractor"
      decoder:
        lora: {enabled: true, rank: 64, alpha: 16}
    training:
      phases:
        phase1: {enabled: true}
        phase2: {enabled: true}
        phase3: {enabled: true, data: "standard"}

  C2:
    name: "projector_pyramid_c_abstractor"
    description: "Pyramid C-Abstractor (multi-scale input)"
    model:
      vision_encoder:
        rope_2d: {enabled: false}
        pyramid: {enabled: false}
        pyramid_drop: {enabled: false}
      projector:
        type: "pyramid_c_abstractor"
      decoder:
        lora: {enabled: true, rank: 64, alpha: 16}
    training:
      phases:
        phase1: {enabled: true}
        phase2: {enabled: true}
        phase3: {enabled: true, data: "standard"}

  # EXP-D: Stage 3 (Decoder Adaptation)
  D1:
    name: "decoder_lora_finetune"
    description: "LoRA fine-tuning on bilingual data"
    model:
      vision_encoder:
        rope_2d: {enabled: false}
        pyramid: {enabled: false}
        pyramid_drop: {enabled: false}
      projector:
        type: "mlp"
      decoder:
        lora: {enabled: true, rank: 64, alpha: 16}
        merging: {enabled: false}
        mole: {enabled: false}
    training:
      phases:
        phase1: {enabled: false}
        phase2: {enabled: false}
        phase3: {enabled: true, data: "standard"}

  D2:
    name: "decoder_dare_ties_merge_only"
    description: "DARE+TIES merge of EN+KO models (no fine-tuning)"
    model:
      vision_encoder:
        rope_2d: {enabled: false}
        pyramid: {enabled: false}
        pyramid_drop: {enabled: false}
      projector:
        type: "mlp"
      decoder:
        lora: {enabled: false}
        merging: {enabled: true, method: "dare_ties"}
        mole: {enabled: false}
    training:
      phases:
        phase1: {enabled: false}
        phase2: {enabled: false}
        phase3: {enabled: false}  # merging only, no training
    merge_config:
      base_model: "OpenGVLab/InternVL2_5-8B"
      korean_model: "path/to/korean_finetuned_model"  # pre-trained separately

  D3:
    name: "decoder_finetune_then_merge"
    description: "Fine-tune on Korean, then DARE+TIES merge with original"
    model:
      vision_encoder:
        rope_2d: {enabled: false}
        pyramid: {enabled: false}
        pyramid_drop: {enabled: false}
      projector:
        type: "mlp"
      decoder:
        lora: {enabled: true, rank: 64, alpha: 16}
        merging: {enabled: true, method: "dare_ties"}
        mole: {enabled: false}
    training:
      phases:
        phase1: {enabled: false}
        phase2: {enabled: false}
        phase3: {enabled: true, data: "korean_only"}  # Korean fine-tune first
    merge_config:
      base_model: "OpenGVLab/InternVL2_5-8B"
      korean_model: "output_from_phase3"  # use phase3 output

  D4:
    name: "decoder_finetune_merge_mole"
    description: "D3 + MoLE language-specific experts"
    model:
      vision_encoder:
        rope_2d: {enabled: false}
        pyramid: {enabled: false}
        pyramid_drop: {enabled: false}
      projector:
        type: "mlp"
      decoder:
        lora: {enabled: true, rank: 64, alpha: 16}
        merging: {enabled: true, method: "dare_ties"}
        mole: {enabled: true}
    training:
      phases:
        phase1: {enabled: false}
        phase2: {enabled: false}
        phase3:
          enabled: true
          stages:
            - name: "korean_finetune"
              data: "korean_only"
              steps: 25000
            - name: "merge_with_english"
              type: "merge"
              steps: 0
            - name: "mole_training"
              data: "mole_balanced"
              steps: 25000
    merge_config:
      base_model: "OpenGVLab/InternVL2_5-8B"
      korean_model: "output_from_korean_finetune"

  # EXP-E: Stage 1+2 Combined
  E1:
    name: "vision_multiscale_plus_c_abstractor"
    description: "B2 + C1 (2D-RoPE + multi-scale + C-Abstractor)"
    model:
      vision_encoder:
        rope_2d: {enabled: true}
        pyramid: {enabled: true}
        pyramid_drop: {enabled: false}
      projector:
        type: "c_abstractor"
      decoder:
        lora: {enabled: true, rank: 64, alpha: 16}
    training:
      phases:
        phase1: {enabled: true}
        phase2: {enabled: true}
        phase3: {enabled: true, data: "standard"}

  E2:
    name: "full_vision_pipeline"
    description: "B3 + C2 (full vision pipeline with PyramidDrop)"
    model:
      vision_encoder:
        rope_2d: {enabled: true}
        pyramid: {enabled: true}
        pyramid_drop: {enabled: true}
      projector:
        type: "pyramid_c_abstractor"
      decoder:
        lora: {enabled: true, rank: 64, alpha: 16}
    training:
      phases:
        phase1: {enabled: true}
        phase2: {enabled: true}
        phase3: {enabled: true, data: "standard"}

  # EXP-F: Full TBI-MLLM (Stage 1+2+3)
  F1:
    name: "tbi_mllm_lora"
    description: "E2 + D1 (full vision + LoRA fine-tune)"
    model:
      vision_encoder:
        rope_2d: {enabled: true}
        pyramid: {enabled: true}
        pyramid_drop: {enabled: true}
      projector:
        type: "pyramid_c_abstractor"
      decoder:
        lora: {enabled: true, rank: 64, alpha: 16}
        merging: {enabled: false}
        mole: {enabled: false}
    training:
      phases:
        phase1: {enabled: true}
        phase2: {enabled: true}
        phase3: {enabled: true, data: "standard"}

  F2:
    name: "tbi_mllm_merge"
    description: "E2 + D2 (full vision + DARE+TIES merge)"
    model:
      vision_encoder:
        rope_2d: {enabled: true}
        pyramid: {enabled: true}
        pyramid_drop: {enabled: true}
      projector:
        type: "pyramid_c_abstractor"
      decoder:
        lora: {enabled: false}
        merging: {enabled: true, method: "dare_ties"}
        mole: {enabled: false}
    training:
      phases:
        phase1: {enabled: true}
        phase2: {enabled: true}
        phase3: {enabled: false}  # merging only
    merge_config:
      base_model: "output_from_E2"  # use E2's Phase 2 checkpoint
      korean_model: "path/to/korean_finetuned_E2"

  F3:
    name: "tbi_mllm_finetune_merge"
    description: "E2 + D3 (full vision + FT→Merge)"
    model:
      vision_encoder:
        rope_2d: {enabled: true}
        pyramid: {enabled: true}
        pyramid_drop: {enabled: true}
      projector:
        type: "pyramid_c_abstractor"
      decoder:
        lora: {enabled: true, rank: 64, alpha: 16}
        merging: {enabled: true, method: "dare_ties"}
        mole: {enabled: false}
    training:
      phases:
        phase1: {enabled: true}
        phase2: {enabled: true}
        phase3: {enabled: true, data: "korean_only"}
    merge_config:
      base_model: "output_from_E2"
      korean_model: "output_from_phase3"

  F4:
    name: "tbi_mllm_full"
    description: "E2 + D4 (FULL TBI-MLLM: vision + projector + FT→Merge→MoLE)"
    model:
      vision_encoder:
        rope_2d: {enabled: true}
        pyramid: {enabled: true}
        pyramid_drop: {enabled: true}
      projector:
        type: "pyramid_c_abstractor"
      decoder:
        lora: {enabled: true, rank: 64, alpha: 16}
        merging: {enabled: true, method: "dare_ties"}
        mole: {enabled: true}
    training:
      phases:
        phase1: {enabled: true}
        phase2: {enabled: true}
        phase3:
          enabled: true
          stages:
            - name: "korean_finetune"
              data: "korean_only"
              steps: 25000
            - name: "merge_with_english"
              type: "merge"
              steps: 0
            - name: "mole_training"
              data: "mole_balanced"
              steps: 25000
    merge_config:
      base_model: "output_from_E2"
      korean_model: "output_from_korean_finetune"

# ============================================================================
# LOGGING & MONITORING
# ============================================================================
logging:
  output_dir: "/NetDisk/juyeon/research/results/EXP-20260211-001/logs"

  wandb:
    enabled: true
    project: "tbi-mllm"
    entity: null  # set to your wandb username
    name: null  # auto-generated from experiment ID
    tags: ["bilingual", "mllm", "internvl", "ablation"]

  tensorboard:
    enabled: true
    log_dir: "/NetDisk/juyeon/research/results/EXP-20260211-001/tensorboard"

  metrics:
    log_interval: 100
    eval_interval: 2500
    save_interval: 5000
    log_level: "INFO"

  checkpoints:
    save_dir: "/NetDisk/juyeon/research/results/EXP-20260211-001/checkpoints"
    save_total_limit: 5
    save_strategy: "steps"
    save_steps: 5000
    load_best_model_at_end: true
    metric_for_best_model: "eval_loss"
    greater_is_better: false

# ============================================================================
# HARDWARE & INFRASTRUCTURE
# ============================================================================
hardware:
  gpus:
    num_gpus: 2
    gpu_type: "NVIDIA A100-PCIE-40GB"
    memory_per_gpu: "40GB"
    compute_capability: "8.0"

  distributed:
    backend: "nccl"
    init_method: "env://"
    world_size: 2
    rank: 0

  cpu:
    num_workers: 8
    pin_memory: true

  storage:
    cache_dir: "/NetDisk/juyeon/research/.cache"
    data_dir: "/NetDisk/juyeon/research/data"
    output_dir: "/NetDisk/juyeon/research/results"
    checkpoint_dir: "/NetDisk/juyeon/research/results/EXP-20260211-001/checkpoints"

# ============================================================================
# REPRODUCIBILITY
# ============================================================================
reproducibility:
  seeds: [42, 123, 456]
  deterministic: true
  cudnn_benchmark: false
  cudnn_deterministic: true
  set_seed_everywhere: true

# ============================================================================
# HYPERPARAMETER SEARCH (OPTIONAL)
# ============================================================================
hyperparameter_search:
  enabled: false
  method: "grid"  # "grid", "random", "bayesian"

  search_space:
    # Merging coefficients (for D2-D4, F2-F4)
    lambda_en: [0.3, 0.4, 0.5, 0.6, 0.7]
    lambda_ko: [0.3, 0.4, 0.5, 0.6, 0.7]

    # LoRA rank (for D1, F1)
    lora_rank: [32, 64, 128]
    lora_alpha: [8, 16, 32]

    # Learning rates
    lr: [1e-5, 2e-5, 5e-5, 1e-4, 2e-4]

    # PyramidDrop rates
    pyramid_drop_l1: [0.3, 0.4, 0.5, 0.6, 0.7]
    pyramid_drop_l2: [0.2, 0.3, 0.4]
    pyramid_drop_l3: [0.0, 0.1, 0.2]

  num_trials: 10
  optimization_metric: "overall_average"
  direction: "maximize"
