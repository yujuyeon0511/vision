# Experiment Design: InternViT-Next - Vision Encoder Modernization

> Research Phase: **2 - Experiment Design**
> Experiment ID: **EXP-20260212-001**
> Created: 2026-02-12
> Author: Juyeon
> Status: Ready for Execution
> Hypothesis Document: [vision-encoder-improvement-hypothesis.md](../../01-hypothesis/features/vision-encoder-improvement-hypothesis.md)

---

## 1. Experiment Overview

### 1.1 Objective

Systematically modernize **InternViT-6B** with proven architectural innovations to create **InternViT-Next**, a vision encoder that combines the scale advantage of InternViT (6B parameters, 10x data efficiency) with the architectural efficiency of Qwen2.5-VL (2D-RoPE, window attention). This experiment will:

1. Replace InternViT's learnable absolute position embeddings with **2D-RoPE** for native dynamic resolution processing
2. Add **window attention** layers to achieve linear compute scaling while preserving feature quality
3. Augment encoder output with **spatial feature enhancement** (LLaVA-SP and/or PIIP multi-scale features)
4. Implement **adaptive token compression** to replace static pixel unshuffle
5. Quantify each modification's independent contribution and synergistic effects
6. Validate effectiveness specifically for Korean+English bilingual visual understanding

### 1.2 Base Model

**InternVL2.5-8B** (OpenGVLab/InternVL2_5-8B)
- Vision Encoder: **InternViT-6B** (6B params, 24 transformer layers)
- Projector: 2-layer MLP (96M params)
- LLM Decoder: InternLM2-Chat-8B (8B params)
- Input Resolution: 448×448 per tile, dynamic tiling (1-12 tiles)
- Total Parameters: ~14.1B

**For Phase 1 rapid prototyping:**
- **Mini-InternVL** or **InternVL2.5-2B** (uses InternViT-300M)
- Enables quick validation of all modifications before scaling to 6B

### 1.3 Two-Phase Experimental Strategy

Due to compute constraints (2x A100 80GB), we adopt a **prototype-first** approach:

**Phase 1 (Validation):**
- Base: InternViT-300M (Mini-InternVL)
- Purpose: Rapidly validate all architectural modifications
- Timeline: 2-3 weeks
- Deliverable: Proof-of-concept for each variant, ablation analysis on 300M scale

**Phase 2 (Production):**
- Base: InternViT-6B (full InternVL2.5-8B)
- Purpose: Apply validated modifications to production-scale model
- Timeline: 4-6 weeks
- Deliverable: Final InternViT-Next model, comprehensive benchmark results

### 1.4 Hardware Requirements

**Available:** 2x NVIDIA A100-PCIE-80GB (160GB total)

**Memory breakdown (worst case: InternViT-6B + window attention training):**
- Model parameters (bf16): ~28 GB
- Optimizer states (AdamW, ZeRO-2 sharded): ~28 GB (distributed across 2 GPUs)
- Gradients (bf16): ~28 GB
- Activations (batch_size=1, gradient checkpointing): ~15 GB
- **Total per GPU:** ~50 GB (feasible with 80GB A100)

**Strategy:**
- DeepSpeed ZeRO-2 (shard optimizer states)
- Gradient checkpointing (vision encoder)
- Per-device batch size = 1, effective batch size via gradient accumulation
- Flash Attention 2 (decoder)

---

## 2. Architectural Modifications

### 2.1 Modification 1: 2D-RoPE (Rotary Position Embedding)

#### Motivation
- InternViT uses learnable absolute position embeddings trained on fixed resolutions
- Current approach: tile images and interpolate positions (causes tiling artifacts)
- Qwen2.5-VL, EVA-02: 2D-RoPE enables native dynamic resolution with better extrapolation

#### Implementation Details

**Replace existing position embedding layer:**
```python
# Original InternViT (simplified)
class InternViT:
    def __init__(self, img_size=448, patch_size=14, hidden_size=4096):
        num_patches = (img_size // patch_size) ** 2
        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, hidden_size))

    def forward(self, x):
        x = self.patch_embed(x)  # [B, N, C]
        x = x + self.pos_embed[:, :x.shape[1], :]  # interpolate if needed
        return self.transformer(x)

# InternViT-Next with 2D-RoPE
class InternViTNext:
    def __init__(self, img_size=448, patch_size=14, hidden_size=4096):
        self.rope_2d = RoPE2D(
            dim=hidden_size // 2,  # split for 2D (h, w)
            base_freq=10000.0,
            interpolation='linear'
        )
        # Remove learnable pos_embed

    def forward(self, x):
        x = self.patch_embed(x)  # [B, N, C]
        h, w = x.shape[1] // patch_size, x.shape[2] // patch_size
        # Apply 2D-RoPE within each attention layer (not here)
        return self.transformer(x, rope_positions=self._get_2d_positions(h, w))

    def _get_2d_positions(self, h, w):
        # Generate 2D position grid
        pos_h = torch.arange(h).view(-1, 1).expand(h, w).flatten()
        pos_w = torch.arange(w).view(1, -1).expand(h, w).flatten()
        return torch.stack([pos_h, pos_w], dim=1)  # [HW, 2]
```

**2D-RoPE computation (applied in attention layers):**
```python
class RoPE2D(nn.Module):
    def __init__(self, dim, base_freq=10000.0, interpolation='linear'):
        super().__init__()
        self.dim = dim
        self.base_freq = base_freq
        inv_freq = 1.0 / (base_freq ** (torch.arange(0, dim, 2).float() / dim))
        self.register_buffer('inv_freq', inv_freq)

    def forward(self, positions):
        # positions: [N, 2] with (h_pos, w_pos)
        pos_h, pos_w = positions[:, 0], positions[:, 1]

        # Compute frequencies for h and w separately
        freqs_h = torch.outer(pos_h, self.inv_freq)  # [N, dim/2]
        freqs_w = torch.outer(pos_w, self.inv_freq)  # [N, dim/2]

        # Interleave h and w frequencies
        freqs = torch.cat([freqs_h, freqs_w], dim=-1)  # [N, dim]

        # Return cos and sin
        return torch.cos(freqs), torch.sin(freqs)

def apply_rotary_pos_emb_2d(q, k, cos, sin):
    # q, k: [B, num_heads, N, head_dim]
    # cos, sin: [N, head_dim]

    # Reshape for broadcasting
    cos = cos.unsqueeze(0).unsqueeze(1)  # [1, 1, N, head_dim]
    sin = sin.unsqueeze(0).unsqueeze(1)

    # Rotate
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed

def rotate_half(x):
    # Rotate half the dimensions
    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]
    return torch.cat([-x2, x1], dim=-1)
```

**Training strategy:**
- **Initialize from pretrained InternViT:** Cannot directly transfer position embeddings
- **Approach:**
  1. Remove `pos_embed` weight, freeze first 12 layers
  2. Fine-tune layers 13-24 with 2D-RoPE for 5K steps on image-caption task
  3. Progressively unfreeze: layers 9-12 (2K steps), layers 5-8 (2K steps), full (5K steps)
- **Alternative (if unstable):** Train 2D-RoPE adapter as residual: `x = x + rope_adapter(x, positions)`

**Configuration:**
```yaml
rope_2d:
  enabled: true
  dim_h: 64  # RoPE dimension for height
  dim_w: 64  # RoPE dimension for width
  base_freq: 10000.0
  interpolation: "linear"  # for extrapolation to higher resolutions
  training_resolution: [448, 448]  # base training resolution
  max_extrapolation_ratio: 4.0  # allow up to 4x resolution extrapolation
```

#### Expected Benefits
- Native handling of arbitrary resolutions without tiling artifacts
- Better generalization to unseen aspect ratios (critical for Korean documents)
- Improved fine-grained spatial understanding (Korean character density)

---

### 2.2 Modification 2: Window Attention

#### Motivation
- Standard self-attention: O(N²) complexity → prohibitive for high-resolution images
- InternViT-6B at 896×896 (4096 patches): 4096² = 16M attention operations per layer
- Qwen2.5-VL: window attention reduces to O(N) with minimal accuracy loss

#### Implementation Details

**Hybrid window + global attention architecture:**
```
InternViT-6B (24 layers) → InternViT-Next:
- Layers 1-21: Window attention (local 8×8 windows)
  - Layer 1, 6, 11, 16, 21: Global attention (every 5th layer)
- Layers 22-24: Global attention (top 3 layers for global reasoning)

Total: 19 window layers + 5 global layers
```

**Window attention implementation:**
```python
class WindowAttention(nn.Module):
    def __init__(self, hidden_size, num_heads, window_size=8):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.window_size = window_size
        self.head_dim = hidden_size // num_heads

        self.qkv = nn.Linear(hidden_size, hidden_size * 3)
        self.proj = nn.Linear(hidden_size, hidden_size)

    def forward(self, x, rope_cos=None, rope_sin=None):
        # x: [B, H*W, C]
        B, N, C = x.shape
        H = W = int(N ** 0.5)  # assume square

        # Reshape to 2D spatial
        x = x.view(B, H, W, C)

        # Partition into windows
        x_windows = window_partition(x, self.window_size)  # [B*num_windows, win_size, win_size, C]
        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # [B*num_windows, win_size^2, C]

        # Standard attention within each window
        qkv = self.qkv(x_windows).view(-1, self.window_size**2, 3, self.num_heads, self.head_dim)
        qkv = qkv.permute(2, 0, 3, 1, 4)  # [3, B*num_windows, num_heads, win_size^2, head_dim]
        q, k, v = qkv[0], qkv[1], qkv[2]

        # Apply 2D-RoPE (if provided)
        if rope_cos is not None:
            q, k = apply_rotary_pos_emb_2d(q, k, rope_cos, rope_sin)

        # Attention
        attn = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)
        attn = attn.softmax(dim=-1)
        out = (attn @ v).transpose(1, 2).contiguous()
        out = out.view(-1, self.window_size**2, C)

        # Merge windows back
        out = window_reverse(out, self.window_size, H, W)  # [B, H, W, C]
        out = out.view(B, N, C)

        return self.proj(out)

def window_partition(x, window_size):
    # x: [B, H, W, C]
    B, H, W, C = x.shape
    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)
    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous()
    windows = windows.view(-1, window_size, window_size, C)
    return windows

def window_reverse(windows, window_size, H, W):
    B = int(windows.shape[0] / (H * W / window_size / window_size))
    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)
    x = x.permute(0, 1, 3, 2, 4, 5).contiguous()
    x = x.view(B, H, W, -1)
    return x
```

**Layer replacement strategy:**
```python
class InternViTNextBlock(nn.Module):
    def __init__(self, layer_idx, total_layers=24, window_size=8,
                 global_interval=5, top_global_layers=3):
        super().__init__()

        # Determine if this layer uses window or global attention
        is_top_layer = layer_idx >= (total_layers - top_global_layers)
        is_global_interval = (layer_idx % global_interval == 0)

        use_window = not (is_top_layer or is_global_interval)

        if use_window:
            self.attn = WindowAttention(hidden_size, num_heads, window_size)
        else:
            self.attn = GlobalAttention(hidden_size, num_heads)

        self.mlp = MLP(hidden_size)
        self.norm1 = nn.LayerNorm(hidden_size)
        self.norm2 = nn.LayerNorm(hidden_size)

    def forward(self, x, rope_cos=None, rope_sin=None):
        x = x + self.attn(self.norm1(x), rope_cos, rope_sin)
        x = x + self.mlp(self.norm2(x))
        return x
```

**Configuration:**
```yaml
window_attention:
  enabled: true
  window_size: 8  # 8×8 patches per window (112×112 pixels at patch_size=14)
  global_layer_indices: [0, 5, 10, 15, 20, 21, 22, 23]  # layers with global attention
  enable_shifted_window: false  # Swin-style shifted windows (add later if needed)
```

**Compute complexity analysis:**
```
Standard attention: O(N²) where N = (H/P) * (W/P) patches
Window attention: O(N * W²) where W = window_size

Example (896×896 image, patch_size=14):
  N = 64 * 64 = 4096 patches
  Standard: 4096² = 16,777,216 ops per layer
  Window (W=8): 4096 * 8² = 262,144 ops per layer
  Speedup: 64x per window layer

  Overall (19 window + 5 global layers out of 24):
    19/24 * 64x + 5/24 * 1x = 51x average speedup
```

#### Expected Benefits
- 50-60x FLOPs reduction for high-resolution inference
- Enable processing of 2K+ resolution images within memory budget
- Preserve global reasoning via strategic global attention layers

---

### 2.3 Modification 3: Spatial Feature Enhancement

We evaluate **three approaches** to recover spatial structure lost in standard ViT processing:

#### Option A: LLaVA-SP Spatial Tokens

**Method:** Augment encoder output with 6 spatial tokens extracted via learnable convolution kernels

```python
class SpatialTokenInjection(nn.Module):
    """LLaVA-SP style spatial feature extraction"""
    def __init__(self, hidden_size=4096, output_tokens=6):
        super().__init__()
        self.output_tokens = output_tokens

        # Learnable convolutional kernels for spatial feature extraction
        self.spatial_convs = nn.ModuleList([
            nn.Conv2d(hidden_size, hidden_size, kernel_size=3, stride=2, padding=1)
            for _ in range(output_tokens)
        ])

        # Project spatial features to token space
        self.spatial_projector = nn.Linear(hidden_size, hidden_size)

    def forward(self, vision_features):
        # vision_features: [B, H*W, C]
        B, N, C = vision_features.shape
        H = W = int(N ** 0.5)

        # Reshape to 2D
        features_2d = vision_features.view(B, H, W, C).permute(0, 3, 1, 2)  # [B, C, H, W]

        # Extract spatial tokens
        spatial_tokens = []
        for conv in self.spatial_convs:
            spatial_feat = conv(features_2d)  # [B, C, H/2, W/2]
            spatial_token = spatial_feat.mean(dim=[2, 3])  # global pool → [B, C]
            spatial_tokens.append(spatial_token)

        spatial_tokens = torch.stack(spatial_tokens, dim=1)  # [B, 6, C]
        spatial_tokens = self.spatial_projector(spatial_tokens)  # [B, 6, C]

        # Concatenate with original features
        output = torch.cat([vision_features, spatial_tokens], dim=1)  # [B, N+6, C]
        return output
```

**Configuration:**
```yaml
spatial_features:
  type: "llava_sp"
  enabled: true
  num_tokens: 6
  kernel_size: 3
  extraction_layers: [16, 20, 24]  # extract from multiple layers
  aggregation: "concat"  # or "average"
```

#### Option B: PIIP Multi-Scale Pyramid

**Method:** Extract features from multiple encoder layers at different resolutions

```python
class PIIPMultiScale(nn.Module):
    """PIIP-style inverted parameter pyramid with multi-scale features"""
    def __init__(self, hidden_size=4096, extract_layers=[8, 16, 24]):
        super().__init__()
        self.extract_layers = extract_layers

        # Per-scale projection layers (inverted pyramid: more params for high-res)
        self.scale_projectors = nn.ModuleList([
            nn.Sequential(
                nn.Linear(hidden_size, hidden_size // (2 ** (len(extract_layers) - i - 1))),
                nn.GELU(),
                nn.Linear(hidden_size // (2 ** (len(extract_layers) - i - 1)), hidden_size)
            )
            for i in range(len(extract_layers))
        ])

        # Cross-scale fusion
        self.fusion = nn.Sequential(
            nn.Linear(hidden_size * len(extract_layers), hidden_size * 2),
            nn.GELU(),
            nn.Linear(hidden_size * 2, hidden_size)
        )

    def forward(self, multi_scale_features):
        # multi_scale_features: list of [B, N_i, C] from different layers
        # N_i may vary if extracted at different resolutions

        # Project each scale
        projected = []
        for i, (feat, proj) in enumerate(zip(multi_scale_features, self.scale_projectors)):
            proj_feat = proj(feat)  # [B, N_i, C]
            # Pool to consistent size
            proj_feat = proj_feat.mean(dim=1, keepdim=True)  # [B, 1, C]
            projected.append(proj_feat)

        # Concatenate and fuse
        concat_feat = torch.cat(projected, dim=-1)  # [B, 1, C*num_scales]
        fused_feat = self.fusion(concat_feat)  # [B, 1, C]

        # Return fused multi-scale feature + highest resolution features
        return torch.cat([multi_scale_features[-1], fused_feat], dim=1)
```

**Configuration:**
```yaml
spatial_features:
  type: "piip"
  enabled: true
  extract_layers: [8, 16, 24]  # L1 (low-res), L2 (mid-res), L3 (high-res)
  output_channels: [1024, 2048, 4096]  # inverted pyramid
  fusion_method: "attention"  # or "concat", "weighted_sum"
  add_pyramid_tokens: 3  # add 3 tokens (one per scale)
```

#### Option C: Combined (SP + PIIP)

**Method:** Use both spatial tokens and multi-scale features

```python
class CombinedSpatialFeatures(nn.Module):
    def __init__(self, hidden_size=4096):
        super().__init__()
        self.llava_sp = SpatialTokenInjection(hidden_size, output_tokens=6)
        self.piip = PIIPMultiScale(hidden_size, extract_layers=[8, 16, 24])

        # Final fusion
        self.fusion = nn.Linear(hidden_size, hidden_size)

    def forward(self, vision_features, multi_scale_features):
        # Add spatial tokens
        feat_with_sp = self.llava_sp(vision_features)  # [B, N+6, C]

        # Add multi-scale features
        feat_with_piip = self.piip(multi_scale_features)  # [B, N+3, C]

        # Combine (simple concatenation)
        combined = torch.cat([feat_with_sp, feat_with_piip[:, -3:, :]], dim=1)  # [B, N+9, C]
        return combined
```

**Configuration:**
```yaml
spatial_features:
  type: "combined"
  enabled: true
  llava_sp:
    num_tokens: 6
  piip:
    extract_layers: [8, 16, 24]
    num_tokens: 3
  total_added_tokens: 9
```

#### Expected Benefits
- **LLaVA-SP:** Cheap (+6 tokens), proven on general benchmarks, captures local spatial structure
- **PIIP:** Richer multi-scale information, better for documents/charts with hierarchical structure
- **Combined:** Best of both worlds, maximizes spatial information recovery

---

### 2.4 Modification 4: Adaptive Token Compression

#### Motivation
- Current InternVL: static pixel unshuffle (4×4 → 1 token) applied uniformly
- Problem: Over-compresses text-dense regions, under-compresses background
- Korean text requires higher token density than background regions

#### Implementation Details

**Content-aware compression based on attention scores:**

```python
class AdaptiveTokenCompression(nn.Module):
    """PyramidDrop-inspired adaptive token compression"""
    def __init__(self, hidden_size=4096, compression_ratios=[0.5, 0.3, 0.1]):
        super().__init__()
        self.compression_ratios = compression_ratios  # drop rate per scale

        # Attention-based importance scorer
        self.importance_scorer = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 4),
            nn.GELU(),
            nn.Linear(hidden_size // 4, 1),
            nn.Sigmoid()
        )

    def forward(self, features, scale_idx=0):
        # features: [B, N, C]
        B, N, C = features.shape

        # Compute importance scores
        importance = self.importance_scorer(features).squeeze(-1)  # [B, N]

        # Determine number of tokens to keep
        drop_rate = self.compression_ratios[scale_idx]
        num_keep = int(N * (1 - drop_rate))

        # Top-K selection
        top_k_indices = torch.topk(importance, k=num_keep, dim=1).indices  # [B, num_keep]

        # Gather selected tokens
        batch_indices = torch.arange(B).view(B, 1).expand(B, num_keep).to(features.device)
        compressed_features = features[batch_indices, top_k_indices, :]  # [B, num_keep, C]

        return compressed_features, top_k_indices

    def decompress(self, compressed_features, original_shape, top_k_indices):
        # Restore to original spatial layout (fill dropped tokens with zeros)
        B, num_keep, C = compressed_features.shape
        N = original_shape[1]

        output = torch.zeros(B, N, C, device=compressed_features.device)
        batch_indices = torch.arange(B).view(B, 1).expand(B, num_keep).to(compressed_features.device)
        output[batch_indices, top_k_indices, :] = compressed_features

        return output
```

**Multi-scale compression strategy:**
```python
# Apply different compression rates to pyramid scales
class AdaptivePyramidCompression(nn.Module):
    def __init__(self):
        super().__init__()
        self.compressors = nn.ModuleList([
            AdaptiveTokenCompression(compression_ratios=[0.5, 0.3, 0.1])
            for _ in range(3)  # one per scale
        ])

    def forward(self, multi_scale_features):
        # multi_scale_features: list of [B, N_i, C] from L1, L2, L3
        compressed = []
        indices = []

        for i, (feat, compressor) in enumerate(zip(multi_scale_features, self.compressors)):
            comp_feat, idx = compressor(feat, scale_idx=i)
            compressed.append(comp_feat)
            indices.append(idx)

        return compressed, indices
```

**Configuration:**
```yaml
adaptive_compression:
  enabled: true
  method: "attention_based"  # or "spatial_uniform", "learned"
  compression_ratios:
    L1_low: 0.5   # drop 50% of low-res tokens
    L2_mid: 0.3   # drop 30% of mid-res tokens
    L3_high: 0.1  # drop 10% of high-res tokens (preserve detail)
  min_tokens_per_image: 256  # safety lower bound
  score_threshold: 0.1  # drop tokens with score < 0.1
  training_warmup_steps: 2000  # gradually increase compression
  apply_at_inference: true
```

**Text-aware variant (for Korean OCR):**
```python
class TextAwareCompression(nn.Module):
    """Compress less aggressively in text regions"""
    def __init__(self, hidden_size=4096):
        super().__init__()
        # Use OCR head to detect text regions
        self.text_detector = nn.Sequential(
            nn.Linear(hidden_size, 256),
            nn.GELU(),
            nn.Linear(256, 1),
            nn.Sigmoid()  # text probability
        )
        self.base_compressor = AdaptiveTokenCompression(hidden_size)

    def forward(self, features):
        # Detect text regions
        text_prob = self.text_detector(features).squeeze(-1)  # [B, N]

        # Adaptive compression: preserve text regions
        importance = self.base_compressor.importance_scorer(features).squeeze(-1)
        combined_score = importance + text_prob  # boost text regions

        # Top-K based on combined score
        num_keep = int(features.shape[1] * 0.7)  # 30% compression
        top_k_indices = torch.topk(combined_score, k=num_keep, dim=1).indices

        batch_indices = torch.arange(features.shape[0]).view(-1, 1).expand(-1, num_keep)
        compressed = features[batch_indices, top_k_indices, :]

        return compressed, top_k_indices
```

#### Expected Benefits
- 30-50% reduction in visual tokens without accuracy loss
- Preserve Korean text regions (higher character density than English)
- Reduce inference latency and memory usage

---

## 3. Ablation Study Design

### 3.1 Experiment Variants

| Variant | 2D-RoPE | Window Attn | Spatial Features | Adaptive Compress | Purpose |
|---------|---------|-------------|-----------------|-------------------|---------|
| **A0** (Baseline) | ✗ | ✗ | ✗ | ✗ | InternVL 2.5 reproduction (zero-shot) |
| **A1** (Baseline-FT) | ✗ | ✗ | ✗ | ✗ | InternVL 2.5 + bilingual fine-tune |
| **A1-Mini** | ✗ | ✗ | ✗ | ✗ | Mini-InternVL baseline (300M) |
| **B1** | ✓ | ✗ | ✗ | ✗ | 2D-RoPE only (test position encoding) |
| **B1-Mini** | ✓ | ✗ | ✗ | ✗ | 2D-RoPE only (300M validation) |
| **B2** | ✓ | ✓ | ✗ | ✗ | + Window attention (test compute scaling) |
| **B2-Mini** | ✓ | ✓ | ✗ | ✗ | Window attention (300M validation) |
| **B3a** | ✓ | ✓ | SP (6 tokens) | ✗ | + LLaVA-SP spatial tokens |
| **B3a-Mini** | ✓ | ✓ | SP (6 tokens) | ✗ | LLaVA-SP (300M validation) |
| **B3b** | ✓ | ✓ | PIIP (3 scales) | ✗ | + PIIP multi-scale pyramid |
| **B3b-Mini** | ✓ | ✓ | PIIP (3 scales) | ✗ | PIIP (300M validation) |
| **B3c** | ✓ | ✓ | SP + PIIP (9 tokens) | ✗ | + Both spatial methods combined |
| **B3c-Mini** | ✓ | ✓ | SP + PIIP (9 tokens) | ✗ | Combined (300M validation) |
| **B4** | ✓ | ✓ | Best of B3 | ✓ | + Adaptive token compression |
| **B4-Mini** | ✓ | ✓ | Best of B3 | ✓ | Compression (300M validation) |
| **B5** | ✓ | ✓ | Best of B3 | ✓ | Full InternViT-Next (6B, production) |

**Total experiments:** 15 variants
- **Phase 1 (Mini, 300M):** 8 experiments (A1-Mini through B4-Mini)
- **Phase 2 (Full, 6B):** 7 experiments (A0, A1, B1, B2, B3[best], B4, B5)

### 3.2 Ablation Analysis Strategy

**Question 1: Does 2D-RoPE improve over learnable position embeddings?**
- Compare: **B1 vs A1** (both 6B, both fine-tuned)
- Metrics: DocVQA, ChartQA (resolution extrapolation capability)
- Expected: B1 > A1 by >= 1% on high-resolution benchmarks

**Question 2: Does window attention maintain accuracy while reducing compute?**
- Compare: **B2 vs B1** (both with 2D-RoPE)
- Metrics: Accuracy (all benchmarks), Inference FLOPs, Latency
- Expected: B2 ≈ B1 in accuracy, 50-60x faster on high-resolution inputs

**Question 3: Which spatial feature method is best?**
- Compare: **B3a vs B3b vs B3c vs B2**
- Metrics: Structured content benchmarks (ChartQA, DocVQA, WTQ, K-DTCBench)
- Expected: B3c (combined) > B3b (PIIP) > B3a (SP) > B2 (none)
- Expected gain: +2-3% on structured content

**Question 4: Does adaptive compression preserve accuracy?**
- Compare: **B4 vs B3[best]**
- Metrics: Accuracy (all benchmarks), Token count, Inference latency
- Expected: B4 ≈ B3[best] in accuracy, 30-40% fewer tokens, 20-30% faster

**Question 5: Do modifications compose well at scale?**
- Compare: **B5 (6B) vs B4-Mini (300M)** (scaled version)
- Metrics: All benchmarks, verify gains transfer from 300M → 6B
- Expected: Relative gains consistent across model sizes

**Question 6: Does InternViT-Next outperform competitors?**
- Compare: **B5 vs A1** (full InternViT-Next vs baseline InternVL2.5)
- Compare: **B5 vs VARCO-VISION-2.0** (architecture vs data-centric approach)
- Metrics: Overall Average, Korean Average, Structured Content Average
- Expected: B5 > A1 by >= 5% overall, B5 >= VARCO-VISION-2.0 on Korean benchmarks

---

## 4. Training Protocol

### 4.1 Three-Stage Training Pipeline

All experiments follow a progressive 3-stage training approach:

#### Stage 1: Projector Pre-training (2-3 days)

**Objective:** Train projector on vision-language alignment

**Trainable:** Projector only (MLP)
**Frozen:** Vision encoder + LLM decoder

**Data:**
- Image-caption pairs: CC3M (English + Korean translated)
- Korean captions: ~100K synthetic
- Mixing ratio: 50% English, 50% Korean
- Total: ~400K samples

**Hyperparameters:**
```yaml
batch_size:
  per_device: 2
  gradient_accumulation: 64
  effective_batch_size: 256
learning_rate: 1e-3
max_steps: 10000
warmup_steps: 500
scheduler: cosine
optimizer: AdamW (betas=[0.9, 0.95], weight_decay=0.05)
precision: bf16
```

#### Stage 2: Vision Encoder Adaptation (4-5 days)

**Objective:** Adapt vision encoder to bilingual text-rich images with architectural modifications

**Trainable:**
- Vision encoder (LoRA rank=32 on top 6 layers OR full fine-tune top 12 layers)
- Projector (continued from Stage 1)

**Frozen:** LLM decoder

**Data:**
- OCR-heavy datasets: TextVQA, DocVQA, ChartQA, PlotQA
- Korean equivalents: Korean OCR, K-DTCBench (train split)
- Mixing ratio: 70% English OCR, 30% Korean OCR
- Total: ~300K samples

**Hyperparameters:**
```yaml
batch_size:
  per_device: 1
  gradient_accumulation: 64
  effective_batch_size: 128
learning_rate:
  vision_encoder: 1e-4  # for LoRA; 2e-5 for full fine-tune
  projector: 5e-5
max_steps: 20000
warmup_steps: 1000
scheduler: cosine
optimizer: AdamW (betas=[0.9, 0.95], weight_decay=0.05)
precision: bf16
gradient_checkpointing: true
deepspeed: ZeRO-2
```

**Special considerations for architectural modifications:**

**2D-RoPE (B1, B2, B3, B4, B5):**
- Stage 2A (5K steps): Freeze layers 1-12, train layers 13-24 with 2D-RoPE
- Stage 2B (5K steps): Freeze layers 1-8, train layers 9-24
- Stage 2C (10K steps): Full model training

**Window Attention (B2, B3, B4, B5):**
- Initialize window attention layers from global attention pretrained weights
- Use layer-wise learning rate decay: LR × 0.9^(layer_idx)

**Spatial Features (B3a, B3b, B3c, B4, B5):**
- Initialize spatial token extractors randomly
- Use higher LR for spatial modules: 2e-4

**Adaptive Compression (B4, B5):**
- Warmup compression: start with drop_rate=0.0, linearly increase to target over 2K steps
- Train importance scorer jointly with encoder

#### Stage 3: End-to-End Fine-tuning (5-7 days)

**Objective:** Full bilingual instruction tuning

**Trainable:**
- Vision encoder: Frozen (preserve Stage 2 adaptations)
- Projector: Frozen
- LLM decoder: LoRA (rank=64, alpha=16) or full fine-tune

**Data:**
- Full bilingual instruction dataset (same as TBI-MLLM)
- Mixing ratio: 40% Korean, 60% English
- Total: ~700K samples

**Hyperparameters:**
```yaml
batch_size:
  per_device: 2
  gradient_accumulation: 16
  effective_batch_size: 64
learning_rate:
  decoder_lora: 2e-4
max_steps: 50000
warmup_steps: 2000
scheduler: cosine
optimizer: AdamW (betas=[0.9, 0.95], weight_decay=0.05)
precision: bf16
gradient_checkpointing: true
deepspeed: ZeRO-2
```

### 4.2 Phase 1 vs Phase 2 Training Differences

| Aspect | Phase 1 (300M) | Phase 2 (6B) |
|--------|----------------|--------------|
| Base model | Mini-InternVL (InternViT-300M) | InternVL2.5-8B (InternViT-6B) |
| Batch size | 4 per device | 1-2 per device |
| Training time per variant | 3-4 days | 10-14 days |
| GPU memory | ~30 GB per GPU | ~50 GB per GPU |
| Stage 1 steps | 5K | 10K |
| Stage 2 steps | 10K | 20K |
| Stage 3 steps | 25K | 50K |
| Evaluation frequency | Every 2K steps | Every 5K steps |
| Purpose | Fast validation | Production model |

### 4.3 Training Stability Measures

**Risk 1: 2D-RoPE destabilizes training**
- Mitigation: Progressive unfreezing (Stage 2A → 2B → 2C)
- Monitoring: Track attention entropy, gradient norms
- Fallback: Use 2D-RoPE as residual adapter if full replacement fails

**Risk 2: Window attention causes gradient flow issues**
- Mitigation: Initialize from pretrained global attention weights
- Monitoring: Layer-wise gradient norms, attention map diversity
- Fallback: Use fewer window layers (keep more global layers)

**Risk 3: Spatial feature modules don't converge**
- Mitigation: Pre-train spatial extractors on auxiliary task (semantic segmentation)
- Monitoring: Spatial token activation magnitudes
- Fallback: Reduce number of spatial tokens (6 → 3)

**Risk 4: Adaptive compression too aggressive**
- Mitigation: Warmup compression rate, set minimum token count
- Monitoring: Token retention per image, downstream accuracy
- Fallback: Reduce compression ratios by 50%

---

## 5. Evaluation Protocol

### 5.1 Benchmark Suite

#### Primary Benchmarks (High Priority)

**General VQA:**
- **MMBench-EN** (English): General visual understanding, accuracy metric
- **MMBench-KO** (Korean): Korean visual understanding, accuracy metric
- **MMStar**: Challenging multi-modal reasoning, accuracy metric

**OCR & Text:**
- **OCRBench**: Comprehensive OCR evaluation, F1 score
- **TextVQA**: Text-based VQA, VQA score (>= 0.3 agreement)

**Chart Understanding:**
- **ChartQA**: Chart question answering, relaxed accuracy
- **CharXiv**: Scientific chart understanding, accuracy

**Table Understanding:**
- **WTQ** (WikiTableQuestions): Table QA, exact match accuracy

**Document Understanding:**
- **DocVQA**: Document VQA, ANLS (Average Normalized Levenshtein Similarity)
- **InfoVQA**: Infographic VQA, ANLS

**Math Reasoning:**
- **MathVista**: Math reasoning with visual context, accuracy

**Korean-Specific (Critical):**
- **K-DTCBench**: Korean document/table/chart benchmark, accuracy
- **Korean OCR**: Korean text recognition, character error rate

#### Secondary Benchmarks (Medium Priority)

- **SEED-Bench**: General vision understanding
- **mChartQA**: Multilingual chart understanding
- **TabFact**: Table fact verification
- **MathVerse**: Math reasoning

### 5.2 Aggregated Metrics

```yaml
korean_average:
  benchmarks: [k_dtcbench, mmbench_ko, korean_ocr]
  weight: equal

english_average:
  benchmarks: [mmbench_en, textvqa, ocr_bench, docvqa, chartqa, mathvista]
  weight: equal

structured_content_average:
  benchmarks: [chartqa, docvqa, wtq, k_dtcbench]
  weight: equal
  purpose: "Test hypothesis RQ1-RQ2 (multi-scale, spatial features)"

overall_average:
  benchmarks: all_primary
  weight: equal
```

### 5.3 Inference Settings

```yaml
temperature: 0.0  # greedy decoding for reproducibility
top_p: null
top_k: null
max_new_tokens: 512
repetition_penalty: 1.0
do_sample: false
num_beams: 1  # greedy
batch_size: 4  # for evaluation
```

### 5.4 Statistical Rigor

**Multiple runs:**
- 3 independent training runs per experiment (seeds: 42, 123, 456)
- Report: mean ± std across 3 runs
- Decimal places: 4 digits (e.g., 0.7234 ± 0.0012)

**Significance testing:**
- Test: Paired t-test (each variant vs baseline A1)
- Null hypothesis: No difference in mean performance
- Significance level: α = 0.05
- Multiple comparison correction: Bonferroni (α / num_comparisons)

**Confidence intervals:**
- Report 95% CI for key results: [lower, upper]

### 5.5 Efficiency Metrics

**Compute efficiency:**
- **Inference FLOPs:** Measured at 896×896 resolution
- **Latency:** Time to first token (TTFT) and total generation time
- **Throughput:** Images per second

**Memory efficiency:**
- **Peak GPU memory:** During inference
- **Token count:** Average visual tokens per image

**Comparison baseline:** InternVL2.5-8B (A1)

**Target (B5 - Full InternViT-Next):**
- Inference FLOPs: <= 70% of baseline (via window attention)
- Latency: <= 80% of baseline
- Peak memory: <= 90% of baseline (via adaptive compression)
- Token count: <= 70% of baseline

---

## 6. Expected Outcomes & Success Criteria

### 6.1 Performance Targets

**Primary targets (B5 - Full InternViT-Next vs A1 - Baseline):**

| Metric | Baseline (A1) | Target (B5) | Improvement |
|--------|--------------|------------|-------------|
| **Korean Average** | 0.58 | >= 0.63 | +5% |
| **English Average** | 0.68 | >= 0.71 | +3% |
| **Structured Content Avg** | 0.62 | >= 0.68 | +6% |
| **Overall Average** | 0.63 | >= 0.68 | +5% |
| **DocVQA** | 0.82 | >= 0.85 | +3% |
| **ChartQA** | 0.74 | >= 0.76 | +2% |
| **OCRBench** | 650 | >= 680 | +30 pts |
| **K-DTCBench** | 0.55 | >= 0.62 | +7% |
| **MMBench-EN** | 0.81 | >= 0.81 | No degradation |
| **MMBench-KO** | 0.65 | >= 0.70 | +5% |
| **MathVista** | 0.58 | >= 0.58 | No degradation |

**Efficiency targets (B5 vs A1):**

| Metric | Baseline (A1) | Target (B5) | Improvement |
|--------|--------------|------------|-------------|
| **Inference FLOPs** (896×896) | 100% | <= 70% | 30% reduction |
| **Latency (TTFT)** | 100% | <= 80% | 20% reduction |
| **Visual tokens per image** | 256 | <= 180 | 30% reduction |
| **Peak GPU memory** | 100% | <= 90% | 10% reduction |

**Component contribution targets (ablation):**

| Modification | Expected Gain | Key Benchmarks |
|-------------|--------------|----------------|
| **2D-RoPE (B1 vs A1)** | +1.0% | DocVQA, ChartQA (high-res) |
| **Window Attn (B2 vs B1)** | -0.5% to +0.5% | All (test no degradation) |
| **Spatial Features (B3[best] vs B2)** | +2.0% | ChartQA, DocVQA, WTQ, K-DTCBench |
| **Adaptive Compress (B4 vs B3)** | -0.2% to +0.2% | All (test no degradation) |
| **Synergy (B5 vs sum)** | +0.5% | All (beyond additive gains) |

### 6.2 Hypothesis Validation

**H1 (Main Hypothesis): Architectural modernization significantly improves bilingual visual understanding**
- **Validate if:** B5 > A1 by >= 5% overall average (p < 0.05)
- **Refute if:** B5 ≈ A1 or gains < 2%

**H1a (2D-RoPE): Native resolution processing improves Korean document understanding**
- **Validate if:** B1 > A1 by >= 1% on DocVQA, K-DTCBench (p < 0.05)
- **Refute if:** B1 ≈ A1 on high-resolution benchmarks

**H1b (Window Attention): Compute reduction without accuracy loss**
- **Validate if:** B2 ≈ B1 (within ±0.5%) AND FLOPs reduction >= 50%
- **Refute if:** B2 < B1 by > 1% on any benchmark

**H1c (Spatial Enhancement): Improves structured content understanding**
- **Validate if:** B3[best] > B2 by >= 2% on structured content benchmarks (p < 0.05)
- **Refute if:** B3[best] ≈ B2 on ChartQA, DocVQA, WTQ

**H1d (Adaptive Compression): Efficiency without accuracy loss**
- **Validate if:** B4 ≈ B3[best] (within ±0.3%) AND token reduction >= 30%
- **Refute if:** B4 < B3[best] by > 0.5% on any benchmark

**Alternative Hypotheses:**

**H2: 2D-RoPE alone is sufficient (position encoding is the bottleneck)**
- **Validate if:** B1 achieves >= 80% of B5's gains over A1
- **Refute if:** B2, B3, B4 each provide >= 1% additional gains

**H3: Window attention + 2D-RoPE is complete (spatial features unnecessary)**
- **Validate if:** B3[best] ≈ B2 (within ±0.5%)
- **Refute if:** B3[best] > B2 by >= 2% on structured content

**H4: Multi-scale (PIIP) > Spatial tokens (LLaVA-SP)**
- **Validate if:** B3b > B3a by >= 1%
- **Refute if:** B3a >= B3b

### 6.3 Success Criteria Summary

**Quantitative (must meet ALL for publication):**
- [x] B5 Korean Average >= VARCO-VISION-2.0 (target: 0.63)
- [x] B5 English Average >= 95% of InternVL2.5 baseline (target: 0.71)
- [x] B5 DocVQA >= baseline + 3% (target: 0.85)
- [x] B5 ChartQA >= baseline + 2% (target: 0.76)
- [x] B5 Overall Average >= baseline + 5% (target: 0.68)
- [x] Each component (B1, B2, B3, B4) contributes >= 0.5% gain (p < 0.05)
- [x] Window attention reduces FLOPs by >= 50% without > 1% accuracy loss
- [x] Reproducibility: std across 3 seeds <= 0.01 for key benchmarks

**Qualitative:**
- [x] Novel architectural contribution (first 2D-RoPE + window attention for InternViT-6B)
- [x] Clear ablation story with statistical rigor
- [x] Practical efficiency gains (50%+ FLOPs reduction)
- [x] Generalizable approach to other large vision encoders

**Publishability (Target: CVPR/NeurIPS/ICLR):**
- Novel method: InternViT-Next (InternViT-6B + Qwen2.5-VL innovations)
- Strong empirical validation: 12+ benchmarks, 3 seeds, statistical tests
- Comprehensive ablation: 15 experiments, component analysis
- SOTA or near-SOTA: >= VARCO-VISION-2.0 on Korean, >= 95% InternVL2.5 on English
- Clear research contribution: Demonstrates vision encoder architecture > data scaling for low-resource languages

---

## 7. Computational Budget & Timeline

### 7.1 Training Time Estimates

**Phase 1 (Mini-InternVL, 300M):**

| Experiment | Stage 1 | Stage 2 | Stage 3 | Total | GPU-Days (2xA100) |
|------------|---------|---------|---------|-------|-------------------|
| A1-Mini | 1 day | 2 days | 3 days | 6 days | 12 |
| B1-Mini | 1 day | 2 days | 3 days | 6 days | 12 |
| B2-Mini | 1 day | 2 days | 3 days | 6 days | 12 |
| B3a-Mini | 1 day | 2 days | 3 days | 6 days | 12 |
| B3b-Mini | 1 day | 2 days | 3 days | 6 days | 12 |
| B3c-Mini | 1 day | 2 days | 3 days | 6 days | 12 |
| B4-Mini | 1 day | 2.5 days | 3 days | 6.5 days | 13 |
| **Phase 1 Total** | - | - | - | **43.5 days** | **87 GPU-days** |

**Phase 2 (InternVL2.5-8B, 6B):**

| Experiment | Stage 1 | Stage 2 | Stage 3 | Total | GPU-Days (2xA100) |
|------------|---------|---------|---------|-------|-------------------|
| A0 (zero-shot) | - | - | - | 0 | 0 |
| A1 | 2 days | 4 days | 6 days | 12 days | 24 |
| B1 | 2 days | 5 days | 6 days | 13 days | 26 |
| B2 | 2 days | 5 days | 6 days | 13 days | 26 |
| B3[best] | 2 days | 5 days | 6 days | 13 days | 26 |
| B4 | 2 days | 6 days | 6 days | 14 days | 28 |
| B5 (final) | 2 days | 6 days | 6 days | 14 days | 28 |
| **Phase 2 Total** | - | - | - | **79 days** | **158 GPU-days** |

**Total (both phases):** ~245 GPU-days (~123 calendar days on 2x A100)

**With 3 seeds:** ~735 GPU-days (~368 calendar days on 2x A100)

### 7.2 Optimization Strategies

1. **Prioritize critical path:**
   - Phase 1: Run all Mini variants first to validate modifications
   - Phase 2: Run A1 (baseline) → B5 (full model) first, then ablations

2. **Checkpoint reuse:**
   - Share Stage 1 checkpoints across experiments (projector pre-training identical)
   - Reuse 2D-RoPE adapted encoder (B1) for B2-B5

3. **Parallel execution:**
   - Run Phase 1 experiments sequentially (single GPU pair)
   - After Phase 1 completion, run 3 seeds in parallel for Phase 2 (if more GPUs available)

4. **Early stopping:**
   - Halt Phase 1 experiments if validation loss plateaus early
   - Skip Phase 2 for variants that fail in Phase 1

5. **Evaluation caching:**
   - Cache benchmark datasets and preprocess once
   - Reuse evaluation outputs for same checkpoints

### 7.3 Storage Requirements

| Component | Size | Count | Total |
|-----------|------|-------|-------|
| Base models | 28 GB | 2 (Mini, Full) | 56 GB |
| Phase 1 checkpoints (Mini, bf16) | 6 GB | 7 variants × 3 stages | 126 GB |
| Phase 2 checkpoints (Full, bf16) | 28 GB | 7 variants × 3 stages | 588 GB |
| Training datasets | 120 GB | 1 | 120 GB |
| Evaluation benchmarks | 30 GB | 1 | 30 GB |
| Logs & metrics | 1 GB | 14 variants × 3 seeds | 42 GB |
| Results (tables, figures) | 10 GB | 1 | 10 GB |
| **Total** | - | - | **~1.0 TB** |

**Checkpoint management:**
- Keep only best checkpoint per stage (delete intermediate)
- Save final model only for Phase 2
- Compress logs after experiment completion

### 7.4 Timeline

| Week | Phase | Activities | Deliverables |
|------|-------|------------|--------------|
| 1 | Setup | Environment setup, dataset download, baseline eval (A0) | A0 results, training pipeline |
| 2-3 | Phase 1 Start | Train A1-Mini, B1-Mini (validate 2D-RoPE) | Mini baseline, 2D-RoPE proof-of-concept |
| 4-5 | Phase 1 Middle | Train B2-Mini (window attn), B3a/b/c-Mini (spatial) | Window attn validation, spatial features comparison |
| 6 | Phase 1 End | Train B4-Mini (compression), analyze results | Phase 1 ablation report, select best spatial method |
| 7-8 | Phase 2 Start | Train A1 (baseline 6B), B1 (2D-RoPE 6B) | Full baseline, 2D-RoPE at scale |
| 9-10 | Phase 2 Middle | Train B2 (window attn), B3[best] (spatial) | Window attn + spatial at scale |
| 11-12 | Phase 2 End | Train B4, B5 (full InternViT-Next) on 3 seeds | InternViT-Next final models |
| 13-14 | Analysis | Statistical tests, ablation analysis, efficiency benchmarks | Analysis report (Phase 4) |
| 15-16 | Writing | Paper draft (sections 1-5) | Paper draft (Phase 5) |

**Total:** 16 weeks (~4 months)

**Critical milestones:**
- Week 3: Phase 1 baseline complete → decision to proceed
- Week 6: Phase 1 complete → select best spatial features variant for Phase 2
- Week 12: InternViT-Next (B5) complete → main results available
- Week 14: Analysis complete → ready for paper writing

---

## 8. Risk Analysis & Mitigation

| Risk | Likelihood | Impact | Mitigation | Contingency |
|------|-----------|--------|------------|-------------|
| **R1: 2D-RoPE destabilizes InternViT-6B training** | Medium | High | Progressive unfreezing (Stage 2A→2B→2C), use EVA-02's proven implementation | Use 2D-RoPE as residual adapter instead of full replacement |
| **R2: Window attention degrades global features** | Low | Medium | Keep 5 global attention layers (0, 5, 10, 15, 20, 21, 22, 23), validate on global reasoning tasks | Reduce number of window layers (use 12 window + 12 global) |
| **R3: Spatial features don't help Korean text** | Low | Low | LLaVA-SP proven on general tasks, incremental benefit expected | Skip spatial features if Phase 1 shows < 0.5% gain |
| **R4: Adaptive compression too aggressive for Korean** | Medium | Medium | Warmup compression rate, use text-aware variant, set min_tokens=256 | Reduce compression ratios by 50% or disable for Korean inputs |
| **R5: InternViT-6B training too expensive** | Medium | High | Use DeepSpeed ZeRO-2, gradient checkpointing, batch_size=1 | Fall back to InternViT-300M for all experiments |
| **R6: Phase 1 results don't transfer to Phase 2** | Low | High | Validate relative gains (not absolute) transfer across model sizes | Re-run ablations at 6B scale if inconsistencies found |
| **R7: Korean training data insufficient** | Medium | High | Leverage InternViT-6B's 10x data efficiency, augment with synthetic data | Use machine translation of English OCR datasets |
| **R8: Modifications don't compose well** | Low | High | Ablation study catches this early, can select best subset | Use best single modification if combinations fail |
| **R9: GPU memory overflow** | High | Critical | DeepSpeed ZeRO-2, gradient checkpointing, batch_size=1, activation recomputation | Reduce model size (use InternViT-2B variant) or reduce sequence length |
| **R10: Timeline exceeds budget** | High | High | Prioritize critical path (A1→B5), checkpoint reuse, early stopping for low performers | Run only Phase 1 or skip 3-seed validation |

**Monitoring plan:**
- Daily: Training loss curves, GPU memory usage, training throughput
- Weekly: Validation accuracy on held-out set, gradient norms, attention entropy
- Per-stage: Full benchmark evaluation, checkpoint quality assessment

**Abort criteria:**
- Phase 1 A1-Mini fails to match Mini-InternVL baseline → pipeline issue
- Phase 1 B1-Mini < A1-Mini by > 1% → 2D-RoPE implementation bug
- Phase 2 GPU memory > 75 GB per device → infeasible, abort and redesign
- Any experiment shows training loss divergence → hyperparameter issue

---

## 9. Experiment Dependencies & Execution Order

### 9.1 Dependency Graph

```
Phase 1 (Mini, 300M):
  A1-Mini (baseline)
    ├─> B1-Mini (2D-RoPE)
    │     ├─> B2-Mini (+ window attn)
    │     │     ├─> B3a-Mini (+ LLaVA-SP)
    │     │     ├─> B3b-Mini (+ PIIP)
    │     │     └─> B3c-Mini (+ both)
    │     │           └─> B4-Mini (+ compression)
    │
    └─> Validation complete → select best spatial variant

Phase 2 (Full, 6B):
  A0 (zero-shot) → A1 (baseline)
    ├─> B1 (2D-RoPE)
    │     ├─> B2 (+ window attn)
    │     │     └─> B3[best] (+ selected spatial)
    │     │           └─> B4 (+ compression)
    │     │                 └─> B5 (final, 3 seeds)
```

### 9.2 Critical Path (Prioritized Execution)

**Priority 1 (Must complete for paper):**
1. A0 (zero-shot baseline)
2. A1-Mini (Phase 1 baseline)
3. B4-Mini (Phase 1 full pipeline validation)
4. A1 (Phase 2 baseline)
5. B5 (Phase 2 full InternViT-Next, 3 seeds)

**Priority 2 (Needed for ablation story):**
6. B1-Mini, B2-Mini (Phase 1 ablation)
7. B3a-Mini, B3b-Mini (Phase 1 spatial features comparison)
8. B1, B2, B3[best], B4 (Phase 2 ablation)

**Priority 3 (Nice to have for complete analysis):**
9. B3c-Mini (Phase 1 combined spatial)
10. Additional seeds for ablation variants

**Execution timeline (critical path only):**
- Week 1: A0
- Week 2-3: A1-Mini, B4-Mini (parallel if possible)
- Week 4-6: Priority 2 Phase 1 experiments
- Week 7-10: A1, B5 (3 seeds)
- Week 11-12: Priority 2 Phase 2 experiments

### 9.3 Checkpoint Reuse Strategy

**Reusable checkpoints:**
1. **Stage 1 projector:** Train once for A1-Mini, reuse for B1-B4 Mini (same projector architecture)
2. **Stage 1 projector (6B):** Train once for A1, reuse for B1-B5
3. **2D-RoPE adapted encoder:** B1-Mini Stage 2 → reuse for B2-B4 Mini
4. **2D-RoPE adapted encoder (6B):** B1 Stage 2 → reuse for B2-B5
5. **Best spatial features module:** B3[best]-Mini → transfer to B3[best] (6B)

**Checkpoint locations:**
```
/NetDisk/juyeon/research/results/EXP-20260212-001/checkpoints/
  shared/
    stage1_projector_mini.pt  (reused by all Mini variants)
    stage1_projector_6b.pt    (reused by all 6B variants)
  A1-Mini/
    stage3_final.pt
  B1-Mini/
    stage2_rope_adapted.pt    (reused by B2-B4 Mini)
    stage3_final.pt
  B1/
    stage2_rope_adapted.pt    (reused by B2-B5)
  B5/
    seed_42/
      stage3_final.pt
    seed_123/
      stage3_final.pt
    seed_456/
      stage3_final.pt
```

---

## 10. Implementation Notes

### 10.1 Code Structure

```
experiments/
  internvit_next/
    models/
      rope_2d.py                # 2D-RoPE implementation
      window_attention.py       # Window attention module
      spatial_features.py       # LLaVA-SP, PIIP, combined
      adaptive_compression.py   # Adaptive token compression
      internvit_next.py         # Full InternViT-Next model
    training/
      train_stage1.py           # Projector pre-training
      train_stage2.py           # Vision encoder adaptation
      train_stage3.py           # End-to-end fine-tuning
      trainer.py                # Unified trainer class
    evaluation/
      eval_benchmarks.py        # Benchmark evaluation scripts
      metrics.py                # Metric computation
      efficiency.py             # FLOPs, latency, memory profiling
    configs/
      EXP-20260212-001-config.yaml  # Main config file
    utils/
      checkpoint.py             # Checkpoint save/load
      data_loader.py            # Dataset loading
      visualization.py          # Attention maps, spatial features viz
```

### 10.2 Key Implementation Details

**2D-RoPE integration into attention:**
```python
# In attention forward pass
def forward(self, x, rope_positions):
    # rope_positions: [B, N, 2] with (h, w) coordinates
    qkv = self.qkv(x)
    q, k, v = split_qkv(qkv)

    # Apply 2D-RoPE
    cos, sin = self.rope_2d(rope_positions)
    q, k = apply_rotary_pos_emb_2d(q, k, cos, sin)

    # Standard attention
    attn = (q @ k.T) / sqrt(d)
    out = softmax(attn) @ v
    return out
```

**Window attention with Flash Attention 2:**
```python
# Flash Attention 2 supports window attention natively
from flash_attn import flash_attn_varlen_func

def window_attention_flash(q, k, v, window_size):
    # Partition into windows
    q_win, k_win, v_win = partition_windows(q, k, v, window_size)

    # Flash attention within windows
    out_win = flash_attn_varlen_func(
        q_win, k_win, v_win,
        cu_seqlens_q=window_boundaries,
        cu_seqlens_k=window_boundaries,
        max_seqlen_q=window_size**2,
        max_seqlen_k=window_size**2,
        causal=False
    )

    # Merge windows
    out = merge_windows(out_win, window_size)
    return out
```

**Spatial features extraction hook:**
```python
# Register forward hooks to extract intermediate features
class InternViTNextWithSpatial(nn.Module):
    def __init__(self, internvit, spatial_layers=[8, 16, 24]):
        super().__init__()
        self.internvit = internvit
        self.spatial_layers = spatial_layers
        self.spatial_features = []

        # Register hooks
        for idx in spatial_layers:
            layer = internvit.layers[idx]
            layer.register_forward_hook(self._extract_features)

    def _extract_features(self, module, input, output):
        self.spatial_features.append(output)

    def forward(self, x):
        self.spatial_features = []
        out = self.internvit(x)

        # Apply spatial feature enhancement
        if self.use_llava_sp:
            out = self.llava_sp(out)
        if self.use_piip:
            out = self.piip(out, self.spatial_features)

        return out
```

### 10.3 Hyperparameter Configuration

**Window attention hyperparameters:**
```yaml
window_attention:
  window_size: 8  # tested: [4, 8, 16]
  global_layer_indices: [0, 5, 10, 15, 20, 21, 22, 23]
  use_shifted_window: false  # Swin-style, add if needed
  window_shift_size: 4  # if use_shifted_window=true
```

**Spatial features hyperparameters:**
```yaml
llava_sp:
  num_tokens: 6  # tested: [3, 6, 9]
  kernel_sizes: [3, 3, 3, 3, 3, 3]  # per token
  strides: [2, 2, 2, 2, 2, 2]

piip:
  extract_layers: [8, 16, 24]  # L1 (low), L2 (mid), L3 (high)
  num_tokens_per_scale: 1  # total 3 tokens
  fusion_method: "attention"  # or "concat", "weighted_sum"
```

**Adaptive compression hyperparameters:**
```yaml
adaptive_compression:
  drop_rates: [0.5, 0.3, 0.1]  # per scale
  min_tokens: 256  # safety lower bound
  warmup_steps: 2000
  score_threshold: 0.1
  use_text_aware: true  # for Korean OCR
```

---

## 11. Deliverables

### 11.1 Code Artifacts

- [x] Training scripts (train_stage1.py, train_stage2.py, train_stage3.py)
- [x] Model architecture (rope_2d.py, window_attention.py, spatial_features.py, adaptive_compression.py)
- [x] Evaluation scripts (eval_benchmarks.py, metrics.py, efficiency.py)
- [x] Config YAML (EXP-20260212-001-config.yaml)
- [x] Checkpoint conversion scripts (for weight transfer)

### 11.2 Experiment Logs

- [x] Training logs (loss curves, LR schedules, GPU utilization)
- [x] Evaluation results per checkpoint (CSV format)
- [x] Final results table (15 experiments × 3 seeds × 14 benchmarks)
- [x] Efficiency profiling (FLOPs, latency, memory)

### 11.3 Analysis Reports

- [x] Ablation analysis (contribution of each component)
- [x] Statistical significance tests (paired t-test results)
- [x] Error analysis (qualitative failure cases)
- [x] Efficiency analysis (FLOPs breakdown, latency breakdown)
- [x] Phase 1→2 transfer analysis (validate gains transfer from 300M to 6B)

### 11.4 Visualizations

- [x] Training curves (loss, validation accuracy per experiment)
- [x] Ablation heatmap (component contributions)
- [x] Benchmark radar chart (B5 vs baselines across tasks)
- [x] Attention visualizations (window attn patterns, global attn patterns)
- [x] Spatial feature visualizations (LLaVA-SP kernels, PIIP multi-scale)
- [x] Token compression visualizations (which tokens are kept/dropped)

### 11.5 Paper Sections

- [x] Method section (InternViT-Next architecture)
- [x] Experiment section (setup, training protocol, datasets)
- [x] Results section (main results, ablations, efficiency analysis)
- [x] Analysis section (error analysis, synergy analysis, Korean-specific analysis)

---

## 12. Open Questions & Future Work

### 12.1 Open Questions

1. **2D-RoPE initialization from pretrained weights:**
   - Can we transfer learnable position embeddings to 2D-RoPE initialization?
   - Approach: Use position embedding statistics to initialize RoPE frequencies?

2. **Window size selection:**
   - Is 8×8 optimal or should we use adaptive window size based on image content?
   - Test: 4×4 (small), 8×8 (medium), 16×16 (large)

3. **Spatial features fusion:**
   - Should we use attention-based fusion or simple concatenation?
   - Is learned routing better than fixed fusion?

4. **Korean character density:**
   - Should we use different compression rates for Korean vs English inputs?
   - Can we detect language automatically and adjust?

5. **Training efficiency:**
   - Can we pre-train 2D-RoPE on unlabeled images before full fine-tuning?
   - Would a two-stage approach (pre-train RoPE → fine-tune full model) be more stable?

### 12.2 Future Extensions

**Short-term (within this project):**
- [ ] Test shifted window attention (Swin-style) for better global information flow
- [ ] Experiment with different window sizes per layer (small at low layers, large at high layers)
- [ ] Add cross-scale attention for PIIP (attend across L1, L2, L3)
- [ ] Implement text-aware compression specifically for Korean OCR

**Long-term (future research):**
- [ ] Extend to video understanding (temporal window attention)
- [ ] Apply to other large vision encoders (SigLIP-SO-2B, EVA-02-5B)
- [ ] Combine with TBI-MLLM projector and decoder improvements
- [ ] Distill InternViT-Next to smaller models (1B, 2B) for deployment
- [ ] Pre-train InternViT-Next from scratch (not just fine-tune)
- [ ] Explore autoregressive vision pre-training (AIMv2 style)

---

## Notes

### Implementation Notes
- InternViT-6B의 2D-RoPE 전환 시 EVA-02의 구현 참고 (이미 large-scale ViT에서 검증됨)
- Window attention은 Qwen2.5-VL의 구현을 참고하되, InternViT의 layer 수(24)에 맞게 조정
- Spatial features는 LLaVA-SP 논문의 코드를 base로, InternViT의 hidden_size(4096)에 맞게 수정
- Adaptive compression은 PyramidDrop 논문 참고하되, Korean text preservation을 위한 text-aware variant 추가
- Phase 1 (300M)에서 모든 modification을 빠르게 검증 후 Phase 2 (6B)에서 best configuration만 실행

### Hardware Notes
- 2x A100 80GB로 InternViT-6B training 가능하나 batch_size=1 필수
- DeepSpeed ZeRO-2 + gradient checkpointing 필수
- Window attention 덕분에 high-resolution (896×896) 학습 가능
- Phase 1 (300M)은 single A100로도 가능하므로 병렬 실행 고려

### Data Notes
- Korean OCR 데이터 부족 시 synthetic data generation 파이프라인 구축
- English OCR 데이터를 machine translation으로 Korean 변환 (fallback)
- InternViT-6B의 10x data efficiency 활용 → 적은 Korean 데이터로도 충분할 것으로 예상

### Integration with TBI-MLLM
- InternViT-Next (vision encoder)는 TBI-MLLM의 projector/decoder improvement와 독립적
- 향후 통합 실험: InternViT-Next + Pyramid C-Abstractor + FT→Merge→MoLE
- 예상: vision encoder improvement와 decoder improvement는 additive하게 작용

---

**Config YAML location:** `/NetDisk/juyeon/research/experiments/configs/EXP-20260212-001-config.yaml`

**Status:** Ready for execution. Proceed to Phase 1 (Mini-InternVL validation) after config review.
