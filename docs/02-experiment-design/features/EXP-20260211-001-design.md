# Experiment Design: TBI-MLLM Systematic Ablation Study

> Research Phase: **2 - Experiment Design**
> Experiment ID: **EXP-20260211-001**
> Created: 2026-02-11
> Author: Juyeon
> Status: Ready for Execution

---

## 1. Experiment Overview

### Objective
Systematically evaluate the **Tripartite Architectural Innovation for Bilingual MLLMs (TBI-MLLM)** through comprehensive ablation studies. This experiment will:
1. Measure the independent contribution of each architectural component (vision encoder, projector, decoder)
2. Identify the optimal decoder adaptation strategy for bilingual language acquisition
3. Quantify synergistic effects when components are combined
4. Establish whether architectural innovation can match or exceed data-driven approaches (VARCO-VISION-2.0)

### Base Model
**InternVL2.5-8B** (OpenGVLab/InternVL2_5-8B)
- Vision Encoder: InternViT-6B (6B params)
- Projector: MLP (96M params)
- LLM Decoder: InternLM2-Chat-8B (8B params)
- Input Resolution: 448×448 per tile, dynamic tiling (1-12 tiles)
- Total Parameters: ~14.1B

### Hardware Requirements
- **Available:** 2x NVIDIA A100-PCIE-40GB (40GB each)
- **Required per experiment:**
  - Baseline/Evaluation: 2x A100 (80GB total)
  - Vision Encoder Training: 2x A100 with gradient checkpointing
  - Projector Training: 1x A100 (projector-only)
  - Full Fine-tuning: 2x A100 with DeepSpeed ZeRO-2
  - LoRA Fine-tuning: 1x A100
- **Strategy:** Sequential execution with gradient checkpointing and mixed precision (bf16)

---

## 2. Experiment Structure

### 2.1 Experiment Hierarchy

```
EXP-A (Baseline)
    ├── A0: InternVL2.5-8B (zero-shot, no training)
    └── A1: InternVL2.5-8B (bilingual fine-tune, baseline)

EXP-B (Stage 1: Vision Encoder)
    ├── B1: 2D-RoPE only (replace abs pos emb)
    ├── B2: 2D-RoPE + Multi-scale features (pyramid)
    └── B3: B2 + PyramidDrop (efficiency variant)

EXP-C (Stage 2: Projector)
    ├── C1: C-Abstractor (single-scale)
    └── C2: Pyramid C-Abstractor (multi-scale input)

EXP-D (Stage 3: Decoder Adaptation — 4 strategies)
    ├── D1: Fine-tuning only (LoRA on bilingual data)
    ├── D2: Merging only (DARE+TIES merge of EN+KO models)
    ├── D3: Fine-tune → Merge (FT on KO, then merge with EN)
    └── D4: FT → Merge → MoLE (D3 + language-specific experts)

EXP-E (Stage 1+2 Combined)
    ├── E1: B2 + C1 (2D-RoPE + multi-scale + C-Abstractor)
    └── E2: B3 + C2 (full vision pipeline + PyramidDrop)

EXP-F (Full TBI-MLLM: Stage 1+2+3)
    ├── F1: E2 + D1 (vision+projector + LoRA fine-tune)
    ├── F2: E2 + D2 (vision+projector + merging)
    ├── F3: E2 + D3 (vision+projector + FT→Merge)
    └── F4: E2 + D4 (vision+projector + FT→Merge→MoLE) ★ FULL TBI-MLLM
```

**Total experiments:** 16 (1 baseline + 15 ablation variants)

### 2.2 Ablation Table

| Exp ID | Vision 2D-RoPE | Multi-Scale | PyramidDrop | C-Abstractor | Pyramid Proj | Decoder Strategy | Tests |
|--------|----------------|-------------|-------------|--------------|--------------|------------------|-------|
| A0 | ✗ | ✗ | ✗ | ✗ | ✗ | None (zero-shot) | Baseline capability |
| A1 | ✗ | ✗ | ✗ | ✗ | ✗ | Standard FT | Data-driven baseline |
| B1 | ✓ | ✗ | ✗ | ✗ | ✗ | Standard FT | RoPE benefit |
| B2 | ✓ | ✓ | ✗ | ✗ | ✗ | Standard FT | Multi-scale benefit |
| B3 | ✓ | ✓ | ✓ | ✗ | ✗ | Standard FT | PyramidDrop tradeoff |
| C1 | ✗ | ✗ | ✗ | ✓ | ✗ | Standard FT | C-Abstractor vs MLP |
| C2 | ✗ | ✗ | ✗ | ✓ | ✓ | Standard FT | Pyramid projector |
| D1 | ✗ | ✗ | ✗ | ✗ | ✗ | LoRA FT | Decoder FT baseline |
| D2 | ✗ | ✗ | ✗ | ✗ | ✗ | DARE+TIES | Merging without FT |
| D3 | ✗ | ✗ | ✗ | ✗ | ✗ | FT→Merge | Hybrid approach |
| D4 | ✗ | ✗ | ✗ | ✗ | ✗ | FT→Merge→MoLE | Full decoder pipeline |
| E1 | ✓ | ✓ | ✗ | ✓ | ✗ | Standard FT | Vision+Projector synergy |
| E2 | ✓ | ✓ | ✓ | ✓ | ✓ | Standard FT | Full vision pipeline |
| F1 | ✓ | ✓ | ✓ | ✓ | ✓ | LoRA FT | TBI-MLLM (LoRA variant) |
| F2 | ✓ | ✓ | ✓ | ✓ | ✓ | DARE+TIES | TBI-MLLM (merge variant) |
| F3 | ✓ | ✓ | ✓ | ✓ | ✓ | FT→Merge | TBI-MLLM (hybrid variant) |
| F4 | ✓ | ✓ | ✓ | ✓ | ✓ | FT→Merge→MoLE | **TBI-MLLM (full)** |

---

## 3. Training Protocol

### 3.1 Multi-Phase Training Strategy

All experiments follow a **3-phase progressive training** approach (except baseline A0/A1):

#### Phase 1: Projector Pre-training (2-3 days)
- **Goal:** Train projector on vision-language alignment task
- **Frozen:** Vision encoder + LLM decoder
- **Trainable:** Projector only (MLP or C-Abstractor)
- **Data:** Image-caption pairs (CC3M, LAION subset) + Korean captions
- **Batch size:** 256 (per-device=2, gradient_accum=64)
- **Learning rate:** 1e-3
- **Steps:** 10,000
- **Loss:** Next-token prediction loss

#### Phase 2: Vision+Projector Fine-tuning (3-5 days)
- **Goal:** Adapt vision encoder to bilingual text-rich images
- **Frozen:** LLM decoder
- **Trainable:** Vision encoder (LoRA or last N layers) + Projector
- **Data:** OCR-heavy datasets (TextVQA, DocVQA, ChartQA) + Korean equivalents
- **Batch size:** 128 (per-device=2, gradient_accum=32)
- **Learning rate:**
  - Vision encoder: 2e-5 (if full fine-tune) or 1e-4 (if LoRA)
  - Projector: 5e-5
- **Steps:** 20,000
- **Loss:** Next-token prediction loss

#### Phase 3: Full or Decoder-Specific Fine-tuning (5-7 days)
- **Goal:** End-to-end task adaptation
- **Trainable:** Depends on experiment variant
  - Standard FT (A1, B, C, E): All parameters or decoder LoRA
  - LoRA FT (D1, F1): Decoder LoRA only
  - Merging variants (D2, F2): No training, only merge operation
  - FT→Merge (D3, F3): Korean FT, then merge
  - FT→Merge→MoLE (D4, F4): FT→Merge, then MoLE training
- **Data:** Full bilingual instruction-tuning dataset (see Section 4)
- **Batch size:** 64 (per-device=2, gradient_accum=16)
- **Learning rate:**
  - Full model: 1e-5
  - LoRA: 2e-4 (rank=64, alpha=16)
  - MoLE experts: 5e-5
- **Steps:** 50,000
- **Loss:** Next-token prediction loss

### 3.2 Training Hyperparameters

#### Optimizer
- **Type:** AdamW
- **Betas:** (0.9, 0.95)
- **Weight decay:** 0.05
- **Gradient clipping:** 1.0

#### Learning Rate Schedule
- **Type:** Cosine decay with warmup
- **Warmup steps:** 500 (Phase 1), 1000 (Phase 2), 2000 (Phase 3)
- **Min LR:** 0 (full decay)

#### Precision & Efficiency
- **Mixed precision:** bfloat16
- **Gradient checkpointing:** Enabled for vision encoder
- **DeepSpeed:** ZeRO-2 (offload optimizer states)
- **Flash Attention 2:** Enabled for decoder

#### Regularization
- **Dropout:** 0.0 (disabled, following InternVL2.5 best practice)
- **Label smoothing:** 0.0
- **LoRA dropout:** 0.05 (only for LoRA experiments)

### 3.3 Component-Specific Settings

#### 2D-RoPE Configuration (Experiments B1-B3, E1-E2, F1-F4)
```python
rope_config = {
    "type": "2d_rope",
    "base_freq": 10000.0,
    "dim_h": 64,  # height dimension
    "dim_w": 64,  # width dimension
    "interpolation": "linear",  # for resolution extrapolation
}
```

#### Multi-Scale Pyramid (Experiments B2-B3, E1-E2, F1-F4)
```python
pyramid_config = {
    "scales": [1/4, 1/2, 1],  # extract from layers [8, 16, 24] of InternViT
    "scale_names": ["L1_low", "L2_mid", "L3_high"],
    "output_channels": [1024, 2048, 4096],  # per scale
}
```

#### PyramidDrop (Experiments B3, E2, F1-F4)
```python
pyramid_drop_config = {
    "enabled": True,
    "drop_rate_per_scale": [0.5, 0.3, 0.1],  # aggressive drop at L1, minimal at L3
    "strategy": "attention_based",  # drop low-attention tokens
    "training_only": False,  # also apply at inference for efficiency
}
```

#### C-Abstractor (Experiments C1-C2, E1-E2, F1-F4)
```python
c_abstractor_config = {
    "num_blocks": 6,
    "input_channels": 4096,  # InternViT output
    "hidden_channels": 2048,
    "output_tokens": 256,  # fixed output size
    "kernel_size": 3,
    "stride": 1,
    "padding": 1,
    "use_se_attention": True,  # Squeeze-and-Excitation
    "se_reduction": 16,
}
```

#### Pyramid C-Abstractor (Experiments C2, E2, F1-F4)
```python
pyramid_c_abstractor_config = {
    "inherit_from": "c_abstractor_config",
    "multi_scale_input": True,
    "cross_scale_fusion": "conv_bottleneck",  # fuse L1+L2+L3
    "fusion_channels": [1024, 2048, 4096],
    "adaptive_pool_size": (16, 16),  # per-scale pooling before fusion
    "output_tokens": 256,  # same as C-Abstractor for fair comparison
}
```

#### LoRA Configuration (Experiments D1, F1)
```python
lora_config = {
    "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    "rank": 64,
    "alpha": 16,  # effective LR multiplier = alpha/rank = 0.25
    "dropout": 0.05,
    "bias": "none",
}
```

#### DARE+TIES Merging (Experiments D2-D4, F2-F4)
```python
dare_ties_config = {
    "dare_drop_rate": 0.9,  # drop 90% of fine-tuned deltas (aggressive pruning)
    "dare_rescale": True,   # rescale remaining weights
    "ties_elect_method": "mass",  # resolve sign conflicts by majority vote
    "lambda_en": 0.6,  # weight for English model (retain more EN knowledge)
    "lambda_ko": 0.4,  # weight for Korean model
    "normalize": True,  # normalize merged weights
}
```

#### MoLE Configuration (Experiments D4, F4)
```python
mole_config = {
    "num_experts": 3,  # KO-specialist, EN-specialist, shared
    "expert_type": "ffn",  # replace FFN layers with expert FFNs
    "layers_to_replace": [16, 20, 24, 28],  # top 4 decoder layers (out of 32)
    "router_type": "learned",  # learnable routing vs. rule-based
    "router_hidden_dim": 256,
    "router_temperature": 1.0,
    "load_balancing_loss_weight": 0.01,
    "expert_capacity_factor": 1.25,
    # Initialization strategy
    "init_ko_expert_from": "korean_finetuned_model",
    "init_en_expert_from": "original_internvl_model",
    "init_shared_expert_from": "merged_model",  # from D3/F3
}
```

---

## 4. Datasets

### 4.1 Training Data

#### Korean Datasets
| Dataset | Domain | Size | Format | Usage |
|---------|--------|------|--------|-------|
| K-DTCBench train | Document/Table/Chart | ~15K | VQA | Phase 3 |
| VARCO-VISION-KO | General VQA | ~50K | VQA | Phase 3 |
| Korean OCR synthetic | OCR/Text | ~100K | Image-text pairs | Phase 1-2 |
| Korean UI screenshots | UI/Layout | ~30K | VQA | Phase 3 |
| Korean math (translated MathVista) | Math reasoning | ~5K | VQA | Phase 3 |

**Total Korean:** ~200K samples

#### English Datasets
| Dataset | Domain | Size | Format | Usage |
|---------|--------|------|--------|-------|
| LLaVA-1.5 instruct | General VQA | 665K | VQA | Phase 3 |
| ShareGPT4V | General VQA | 100K | VQA | Phase 3 |
| TextVQA train | OCR | 39K | VQA | Phase 2-3 |
| DocVQA train | Document | 39K | VQA | Phase 2-3 |
| ChartQA train | Chart | 18K | VQA | Phase 2-3 |
| Table-LLaVA | Table | 250K | VQA | Phase 2-3 |
| PlotQA | Chart | 157K | VQA | Phase 2-3 |
| InfoVQA train | Infographic | 23K | VQA | Phase 2-3 |
| MathVista train | Math reasoning | 3K | VQA | Phase 3 |
| CC3M | Caption | 3M | Image-caption | Phase 1 (subset) |

**Total English:** ~4.3M samples (use ~1M for Phase 1, ~500K for Phase 2-3)

#### Mixing Strategy
- **Phase 1 (Projector):** 100% caption data (50% Korean, 50% English)
- **Phase 2 (Vision+Projector):** 70% OCR-heavy (TextVQA, DocVQA, ChartQA), 30% Korean equivalents
- **Phase 3 (Full/Decoder):**
  - Standard FT (A1, B, C, E): 40% Korean, 60% English (balanced bilingual)
  - Korean FT for merging (D3, D4, F3, F4): 100% Korean (for KO-specialist model)
  - MoLE training: 50% Korean, 50% English (for routing signal)

### 4.2 Evaluation Benchmarks

#### General VQA
| Benchmark | Language | Metric | Priority |
|-----------|----------|--------|----------|
| MMBench | EN+KO | Accuracy | High |
| MMStar | EN | Accuracy | High |
| SEED-Bench | EN | Accuracy | Medium |

#### OCR & Text
| Benchmark | Language | Metric | Priority |
|-----------|----------|--------|----------|
| OCR-Bench | EN | F1 Score | High |
| TextVQA val | EN | VQA Score | High |

#### Chart Understanding
| Benchmark | Language | Metric | Priority |
|-----------|----------|--------|----------|
| ChartQA test | EN | Relaxed Acc. | High |
| CharXiv | EN | Accuracy | High |
| mChartQA | Multi | Accuracy | Medium |

#### Table Understanding
| Benchmark | Language | Metric | Priority |
|-----------|----------|--------|----------|
| WTQ | EN | Accuracy | High |
| TabFact | EN | Accuracy | Medium |

#### Document Understanding
| Benchmark | Language | Metric | Priority |
|-----------|----------|--------|----------|
| DocVQA test | EN | ANLS | High |
| InfoVQA test | EN | ANLS | Medium |

#### Math Reasoning
| Benchmark | Language | Metric | Priority |
|-----------|----------|--------|----------|
| MathVista | EN | Accuracy | High |
| MathVerse | EN | Accuracy | Medium |

#### Korean-Specific
| Benchmark | Language | Metric | Priority |
|-----------|----------|--------|----------|
| K-DTCBench | KO | Accuracy | **Critical** |
| KoMM-Bench | KO | Accuracy | High (if available) |

**Evaluation frequency:** Every 2,500 steps during training, full evaluation at end

---

## 5. Evaluation Protocol

### 5.1 Metrics

#### Primary Metrics
- **Accuracy:** For classification-style VQA (MMBench, ChartQA, MathVista)
- **VQA Score:** For open-ended VQA (TextVQA, following VQA v2.0 protocol)
- **ANLS (Average Normalized Levenshtein Similarity):** For document understanding (DocVQA, InfoVQA)
- **Relaxed Accuracy:** For ChartQA (following official evaluation script)

#### Secondary Metrics
- **F1 Score:** For OCR-Bench (token-level F1)
- **Exact Match:** For table QA (WTQ, TabFact)

#### Aggregated Metrics
- **Korean Average:** Average across K-DTCBench, MMBench-KO, KoMM-Bench
- **English Average:** Average across MMBench-EN, TextVQA, OCR-Bench, DocVQA, ChartQA, MathVista
- **Structured Content Average:** Average across ChartQA, DocVQA, WTQ (tests hypothesis RQ1-RQ2)
- **Overall Average:** Macro-average across all benchmarks (equal weight per benchmark)

### 5.2 Statistical Rigor

#### Multiple Runs
- **Seeds:** 42, 123, 456 (3 independent runs per experiment)
- **Reporting:** Mean ± Std across 3 runs
- **Decimal places:** 4 digits (e.g., 0.7234 ± 0.0012)

#### Significance Testing
- **Test:** Paired t-test (compare each ablation variant against baseline A1)
- **Null hypothesis:** No difference in mean performance
- **Significance level:** p < 0.05
- **Correction:** Bonferroni correction for multiple comparisons (divide α by number of comparisons)

#### Confidence Intervals
- **Report:** 95% confidence interval for key results
- **Format:** [lower, upper] or ±margin

### 5.3 Inference Settings

```python
inference_config = {
    "temperature": 0.0,  # greedy decoding for reproducibility
    "top_p": None,       # disabled
    "top_k": None,       # disabled
    "max_new_tokens": 512,
    "repetition_penalty": 1.0,
    "do_sample": False,
    "num_beams": 1,      # greedy (beam search disabled for speed)
}
```

---

## 6. Expected Outcomes

### 6.1 Hypothesis Validation

#### RQ1: Multi-Scale Vision Encoding (Experiments B1-B3)
**Expected outcome:**
- B2 (2D-RoPE + multi-scale) > B1 (2D-RoPE only) > A1 (baseline) by >=1% on OCR-heavy benchmarks (TextVQA, DocVQA, K-DTCBench)
- B3 (+ PyramidDrop) ≈ B2 in accuracy but 1.5-2x faster inference
- Validates H1 component 1

#### RQ2: Pyramid C-Abstractor (Experiments C1-C2)
**Expected outcome:**
- C2 (Pyramid C-Abstractor) > C1 (C-Abstractor) > A1 (MLP baseline) by >=1.5% on structured content (ChartQA, DocVQA, WTQ)
- Locality preservation helps Korean OCR: C2 shows larger gains on K-DTCBench than on English benchmarks
- Validates H1 component 2, supports H3

#### RQ3: Decoder Adaptation Strategy (Experiments D1-D4)
**Expected outcome:**
- D3 (FT→Merge) > D1 (FT only) in Korean performance while better English retention
- D4 (FT→Merge→MoLE) achieves best bilingual balance: highest Korean Average + >= 95% English retention
- D2 (Merge only) shows modest gains but inferior to FT-based approaches
- Validates H5, refutes H4

#### RQ4: Synergistic Effects (Experiments E1-E2, F1-F4)
**Expected outcome:**
- E2 > B3 + C2 (i.e., joint training > sum of independent gains) by >=0.5%
- F4 (full TBI-MLLM) > E2 + D4 (i.e., three components together > vision+projector + decoder separately) by >=0.5%
- Validates H1 (synergy exists), refutes H2 (all three components needed)

### 6.2 Performance Targets

| Experiment | Korean Avg | English Avg | Structured Avg | Overall Avg | Status vs SOTA |
|------------|-----------|-------------|----------------|-------------|----------------|
| A0 (zero-shot) | 0.35 | 0.52 | 0.40 | 0.42 | Weak baseline |
| A1 (FT baseline) | 0.58 | 0.68 | 0.62 | 0.63 | Data-driven baseline |
| B3 (vision) | 0.61 | 0.69 | 0.66 | 0.65 | +2% overall |
| C2 (projector) | 0.62 | 0.69 | 0.68 | 0.66 | +3% structured |
| E2 (vision+proj) | 0.65 | 0.70 | 0.71 | 0.69 | +6% overall |
| D4 (decoder) | 0.62 | 0.68 | 0.63 | 0.64 | +1% Korean |
| **F4 (full TBI)** | **0.68** | **0.71** | **0.74** | **0.71** | **+8% overall, >= VARCO-VISION-2.0** |

**Key target (F4):**
- Korean Average >= 0.68 (match or exceed VARCO-VISION-2.0)
- English Average >= 0.71 (>= 95% of InternVL2.5 baseline)
- ChartQA >= 0.76 (+3% over baseline)
- DocVQA >= 0.87 (+3% over baseline)
- K-DTCBench >= 0.70

---

## 7. Computational Budget

### 7.1 Training Time Estimates (per experiment)

| Experiment | Phase 1 | Phase 2 | Phase 3 | Total | GPU-Days |
|------------|---------|---------|---------|-------|----------|
| A0 | - | - | - | 0 | 0 |
| A1 | 2 days | 3 days | 5 days | 10 days | 20 |
| B1-B3 (each) | 2 days | 4 days | 5 days | 11 days | 22 |
| C1-C2 (each) | 3 days | 4 days | 5 days | 12 days | 24 |
| D1 | - | - | 5 days | 5 days | 10 |
| D2 | - | - | 1 day (merge) | 1 day | 2 |
| D3 | - | - | 6 days (KO FT + merge) | 6 days | 12 |
| D4 | - | - | 8 days (FT+merge+MoLE) | 8 days | 16 |
| E1-E2 (each) | 3 days | 5 days | 5 days | 13 days | 26 |
| F1 | 3 days | 5 days | 5 days | 13 days | 26 |
| F2 | 3 days | 5 days | 1 day (merge) | 9 days | 18 |
| F3 | 3 days | 5 days | 6 days | 14 days | 28 |
| F4 | 3 days | 5 days | 8 days | 16 days | 32 |

**Total for all 16 experiments:** ~280 GPU-days (~140 calendar days on 2x A100)

**With 3 seeds:** ~840 GPU-days (~420 calendar days on 2x A100)

**Optimization strategies:**
1. **Prioritize critical path:** Run F4 (full TBI-MLLM) first, then ablations
2. **Share Phase 1-2 checkpoints:** Reuse projector/vision checkpoints across experiments
3. **Parallel seeds:** Run 3 seeds sequentially for same experiment (reuse data loading)
4. **Early stopping:** Halt low-performing variants after Phase 2

### 7.2 Storage Requirements

| Component | Size | Count | Total |
|-----------|------|-------|-------|
| Base model (InternVL2.5-8B) | 28 GB | 1 | 28 GB |
| Checkpoints per experiment (bf16) | 28 GB | 16 exp × 3 phases | 1.3 TB |
| LoRA adapters | 2 GB | 8 experiments | 16 GB |
| Training datasets | 120 GB | 1 | 120 GB |
| Evaluation benchmarks | 20 GB | 1 | 20 GB |
| Logs & metrics | 0.5 GB | 16 exp × 3 seeds | 24 GB |
| Results (tables, figures) | 5 GB | 1 | 5 GB |

**Total:** ~1.5 TB

### 7.3 GPU Feasibility Analysis

**Available hardware:** 2x A100-40GB (80 GB total)

**Memory breakdown (worst case: Phase 3 full fine-tuning):**
- Model parameters (bf16): 28 GB
- Optimizer states (AdamW, fp32): 56 GB
- Gradients (bf16): 28 GB
- Activations (batch_size=2, seq_len=2048): ~20 GB
- **Total:** ~132 GB

**Mitigation strategies:**
1. **DeepSpeed ZeRO-2:** Offload optimizer states → save 56 GB → **76 GB total**
2. **Gradient checkpointing:** Recompute activations → save 15 GB → **61 GB total**
3. **Batch size adjustment:** Reduce to batch_size=1 per device → save 10 GB → **51 GB total**
4. **Gradient accumulation:** Maintain effective batch size via accumulation steps

**Conclusion:** Feasible with 2x A100-40GB using DeepSpeed + gradient checkpointing + small per-device batch size

---

## 8. Risk Analysis & Mitigation

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|------------|
| **R1:** Multi-scale vision encoder training unstable | Medium | High | Start with frozen InternViT, train only 2D-RoPE adapter; use layer-wise LR decay |
| **R2:** Pyramid C-Abstractor doesn't converge | Low | Medium | Pre-train each scale's C-Abstractor independently, then train fusion module |
| **R3:** DARE+TIES merging degrades both languages | Medium | High | Grid search λ_en, λ_ko ∈ {0.3, 0.4, 0.5, 0.6, 0.7}; validate on held-out bilingual set |
| **R4:** MoLE router collapses (all tokens → one expert) | Medium | Medium | Pre-assign language labels for warm start; use load balancing loss (weight=0.01) |
| **R5:** Korean training data insufficient for domain tasks | Medium | High | Augment with machine-translated English data; use self-synthesized rehearsal (SSR) |
| **R6:** GPU memory overflow during training | High | Critical | Use DeepSpeed ZeRO-2, gradient checkpointing, reduce batch size to 1 per device |
| **R7:** Experiment timeline exceeds budget (420 days) | High | High | Prioritize F4 (full model) + E2 (vision ablation) + D4 (decoder ablation); skip less critical ablations |
| **R8:** Evaluation benchmarks inaccessible | Low | Medium | Download and cache all benchmarks before training starts |
| **R9:** Seed variance too high (std > 0.01) | Medium | Medium | Increase to 5 seeds; report median instead of mean; check for training instability |

---

## 9. Experiment Dependencies

### Critical Path (for prioritized execution)
```
1. A0 (baseline eval) → A1 (baseline FT)
2. A1 → E2 (vision+projector, full pipeline) → F4 (full TBI-MLLM)
3. F4 → Ablations (B3, C2, D4) to explain component contributions
4. Optional: Remaining ablations (B1-B2, C1, D1-D3, E1, F1-F3) for complete paper
```

### Checkpoint Reuse
- **Projector checkpoints:** Phase 1 of A1 → reuse for B1-B3 (MLP projector)
- **Projector checkpoints:** Phase 1 of C1 → reuse for C2, E1-E2 (C-Abstractor)
- **Vision+Projector checkpoints:** E2 Phase 2 → reuse for F1-F4 Phase 3
- **Merged decoder:** D3 → initialize D4's MoLE shared expert

---

## 10. Success Criteria

### Quantitative Criteria (must meet ALL)
- [x] **Korean VQA:** F4 Korean Average >= VARCO-VISION-2.0 (target: 0.68)
- [x] **English Retention:** F4 English Average >= 95% of InternVL2.5 baseline (target: 0.71)
- [x] **Chart Understanding:** F4 ChartQA >= baseline + 3% (target: 0.76)
- [x] **Document Understanding:** F4 DocVQA >= baseline + 3% (target: 0.87)
- [x] **Statistical Significance:** Each component (B3, C2, D4) contributes >= 1% gain with p < 0.05
- [x] **Synergy:** E2 > (B3 + C2 - A1) by >= 0.5% (interaction effect)
- [x] **Reproducibility:** Std across 3 seeds <= 0.01 for key benchmarks

### Qualitative Criteria
- [x] Novel architectural contribution (Pyramid C-Abstractor)
- [x] Clear ablation story with statistical rigor
- [x] Generalizable insights for bilingual MLLM development
- [x] Efficient inference (within 2x of baseline latency)

### Publishability Criteria
- Target venues: ACL, EMNLP, NeurIPS, CVPR
- Minimum requirements:
  - Novel method with strong empirical validation
  - Comprehensive ablation study (16 experiments)
  - SOTA or near-SOTA results on multiple benchmarks
  - Clear research contribution beyond engineering

---

## 11. Timeline

| Week | Phase | Activities | Deliverables |
|------|-------|------------|--------------|
| 1-2 | Setup | Environment setup, dataset download, baseline eval (A0) | A0 results |
| 3-4 | Baseline | Train A1 (baseline FT) on 3 seeds | A1 results, training pipeline validated |
| 5-7 | Critical path | Train E2 (vision+projector) on 3 seeds | E2 results |
| 8-10 | Critical path | Train F4 (full TBI-MLLM) on 3 seeds | F4 results, hypothesis H1 validated |
| 11-13 | Ablations | Train B3, C2, D4 (key ablations) on 3 seeds | Component contribution analysis |
| 14-16 | Extended ablations | Train B1-B2, C1, D1-D3, E1, F1-F3 | Full ablation table |
| 17-18 | Analysis | Statistical tests, visualizations, error analysis | Analysis report (Phase 4) |
| 19-20 | Writing | Paper draft (sections 1-5) | Paper draft (Phase 5) |

**Total:** ~20 weeks (5 months)

---

## 12. Deliverables

### Code Artifacts
- [ ] Training scripts for all experiment variants (A0-F4)
- [ ] Model architecture code (2D-RoPE, Pyramid C-Abstractor, MoLE)
- [ ] Evaluation scripts for all benchmarks
- [ ] Config YAML for reproducibility
- [ ] Checkpoint conversion scripts (for merging experiments)

### Experiment Logs
- [ ] Training logs (loss curves, LR schedules, GPU utilization)
- [ ] Evaluation results per checkpoint (CSV format)
- [ ] Final results table (16 experiments × 3 seeds × 12 benchmarks)

### Analysis Reports
- [ ] Ablation analysis (contribution of each component)
- [ ] Statistical significance tests (paired t-test results)
- [ ] Error analysis (qualitative examples of failure modes)
- [ ] Inference efficiency analysis (latency, throughput, memory)

### Visualizations
- [ ] Training curves (loss, validation accuracy)
- [ ] Ablation heatmap (component contributions)
- [ ] Benchmark radar chart (F4 vs baselines across tasks)
- [ ] Attention visualizations (multi-scale features, C-Abstractor)

### Paper Sections
- [ ] Method section (TBI-MLLM architecture)
- [ ] Experiment section (setup, training protocol)
- [ ] Results section (main results, ablations)
- [ ] Analysis section (error analysis, synergy analysis)

---

## Notes

### Open Questions
1. **Korean tokenizer:** Should we extend InternLM2's vocabulary with Korean subwords? (Risk: breaks pre-trained weights)
   - Decision: Keep original tokenizer, rely on fine-tuning to adapt
2. **Vision encoder freezing:** Should we freeze InternViT entirely in Phase 2 to prevent catastrophic forgetting?
   - Decision: LoRA fine-tune top 6 layers, freeze bottom 18 layers
3. **MoLE expert initialization:** Should we pre-train experts independently before joint training?
   - Decision: Yes, initialize from D3's KO/EN/merged models, then train router
4. **PyramidDrop threshold:** What attention score threshold for token dropping?
   - Decision: Adaptive threshold per scale (keep top-K tokens by attention mass)

### Future Extensions
- [ ] Extend to more languages (Chinese, Japanese) using same TBI-MLLM framework
- [ ] Apply to video-language models (temporal pyramid features)
- [ ] Distill F4 to smaller model (e.g., 2B) for deployment
- [ ] Explore task-specific projectors (chart-specific, document-specific)

---

**Config YAML location:** `/NetDisk/juyeon/research/experiments/configs/EXP-20260211-001-config.yaml`

**Status:** Ready for execution. Proceed to experiment run phase after config review.
