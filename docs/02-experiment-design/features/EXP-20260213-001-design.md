# Experiment Design: VDC — Visual Drift Compensation for Cross-Lingual MLLM Adaptation

> Research Phase: **2 - Experiment Design**
> Experiment ID: **EXP-20260213-001**
> Created: 2026-02-13 (v2: major revision for top-tier venue)
> Author: Juyeon
> Status: Draft
> Hypothesis: [korean-mllm-visual-blindness-hypothesis.md](../../01-hypothesis/features/korean-mllm-visual-blindness-hypothesis.md)
> Method Spec: [visual-drift-compensation-hypothesis.md](../../01-hypothesis/features/visual-drift-compensation-hypothesis.md)
> Target Venue: **ACL / EMNLP / NeurIPS 2026**

---

## 0. Paper Framing (Top-Tier Positioning)

### Title (Draft)
**"Visual Drift Compensation: Preserving Visual Grounding During Cross-Lingual Adaptation of Multimodal LLMs"**

### One-Sentence Contribution
We propose VDC, a training-time framework that actively tracks and compensates for visual token embedding drift during non-English language adaptation, achieving ≥90% visual grounding preservation at 1.5x training cost across 5 typologically diverse languages.

### Contribution Checklist (Top-Tier Requirements)

| Requirement | Status | How We Address It |
|-------------|--------|-------------------|
| Novel method | **Yes** | VDC: drift-aware visual-linguistic co-adaptation (no prior work) |
| Theoretical motivation | **Yes** | Information-theoretic analysis: I(V; Y \| T_L2) preservation |
| Multiple model scales | **Yes** | 2B (ablation) + 7B-8B (main results) |
| Multiple architectures | **Yes** | InternVL, LLaVA, Qwen2-VL (MLP, cross-attn projectors) |
| Multiple languages | **Yes** | Korean, Japanese, Chinese, Arabic, Thai (5 languages, 4 scripts) |
| Comprehensive benchmarks | **Yes** | 8+ benchmarks per language |
| Strong baselines | **Yes** | 6 baselines including VIRAL, staged training, unfreezing |
| Ablation study | **Yes** | Component-wise ablation + interaction analysis |
| Statistical rigor | **Yes** | 3 seeds, paired t-test, bootstrap CI, effect size |
| Analysis/insight | **Yes** | Drift dynamics visualization, layer-wise attention analysis |

---

## 1. Method: Visual Drift Compensation (VDC)

### 1.1 Core Insight

When fine-tuning an MLLM on language L2, the LLM's embedding space shifts to accommodate L2 patterns. Visual tokens, projected by a fixed or jointly-trained projector, become **out-of-distribution** in this shifted space. Existing methods either prevent the shift (hurting L2 adaptation) or ignore it (causing visual blindness).

**VDC takes a third path**: allow the LLM to adapt freely while actively tracking the drift and compensating the visual projection to stay in-distribution.

### 1.2 Training Objective

```
L_total = L_task + λ₁·L_drift + λ₂·L_contrast + λ₃·L_align
```

| Loss | Purpose | Novelty |
|------|---------|---------|
| `L_task` | Standard next-token prediction | — |
| `L_drift` | Anchor-based drift compensation | **New**: tracks LLM shift dynamically, adapts projector to follow |
| `L_contrast` | Contrastive visual grounding | **New in training**: forces visual dependency during language adaptation |
| `L_align` | Drift-adaptive visual alignment | **Extended**: VIRAL-style but with adaptive λ proportional to drift magnitude |

### 1.3 Component 1: Drift-Aware Anchor Compensation (`L_drift`)

**Motivation**: VIRAL aligns to a fixed vision encoder. But after language adaptation, the LLM's embedding space has moved — we need to align to the *current* LLM space.

**Method**:
1. Maintain a set of **anchor concepts** — 50 bilingual (image, text_en, text_L2) triplets covering universal visual concepts
2. Every N training steps, measure drift:
   ```python
   v_anchor = projector(vision_encoder(anchor_images))   # projected visual tokens
   l_anchor = llm.embed(anchor_texts_L2)                 # LLM's current text embeddings

   L_drift = MMD(v_anchor, l_anchor)  # Maximum Mean Discrepancy
   ```
3. Gradient flows through projector → projector learns to track LLM's embedding shift

**Key difference from VIRAL**: VIRAL: `align(projector_out, frozen_vision_encoder_out)` → target is static. VDC: `align(projector_out, current_llm_embeddings)` → target moves with the LLM.

### 1.4 Component 2: Contrastive Visual Grounding (`L_contrast`)

**Motivation**: Even if visual tokens are in-distribution, the model might learn L2 text shortcuts. We need to explicitly penalize visual neglect.

**Method**: Dual forward pass per training example:
```python
logits_v = model(image + text)      # with visual tokens
logits_t = model(text_only)         # without visual tokens (text-only, stop_grad)

L_contrast = -log(P(target | image, text) / P(target | text))
```

**Key difference**: ICD (ACL 2024) does this at inference. We do it at **training time** during language adaptation, directly shaping the gradient.

**Efficiency**: Text-only forward pass skips vision encoder → overhead is ~30% per step, reduced by computing L_contrast every K steps.

### 1.5 Component 3: Drift-Adaptive Visual Alignment (`L_align`)

**Motivation**: Preserve fine-grained visual detail that L_drift (concept-level) doesn't capture.

**Method**:
```python
for l in critical_layers:  # middle layers: highest modality integration
    v_mllm = extract_visual_hidden(mllm, layer=l)
    v_ref  = extract_features(frozen_vision_encoder, layer=l)
    L_align += w_l * MSE(v_mllm, stop_grad(v_ref))

# Adaptive weighting: increase alignment when drift is high
w_adaptive = w_base * (1 + α * current_drift_magnitude)
```

**Key difference from VIRAL**: Adaptive λ based on measured drift. When drift is low (early training), alignment is relaxed to allow adaptation. When drift spikes, alignment strengthens to prevent collapse.

### 1.6 Component 4: Modality-Aware LoRA Allocation

**Motivation**: Not all layers contribute equally to visual-linguistic integration. Allocate adaptation capacity where it matters most.

**Method**:
1. Pre-compute Modality Integration Rate (MIR) per layer using attention analysis on the base model
2. Allocate LoRA rank proportionally: `rank[l] = base_rank × MIR[l]`
3. Projector gets elevated rank: `rank[proj] = base_rank × β` (β > 1)

---

## 2. Experimental Setup

### 2.1 Models

| Model | Scale | Vision Encoder | Projector | LLM | Usage |
|-------|-------|---------------|-----------|-----|-------|
| **InternVL2.5-2B** | 2.2B | InternViT-300M | MLP | InternLM2-1.8B | Ablation |
| **LLaVA-OneVision-0.5B** | 0.9B | SigLIP-400M | MLP | Qwen2-0.5B | Ablation |
| **Qwen2-VL-2B** | 2.1B | ViT-600M | Cross-Attn Merger | Qwen2-1.5B | Ablation |
| **InternVL2.5-8B** | 8.1B | InternViT-6B | MLP | InternLM2-7B | **Main results** |
| **Qwen2-VL-7B** | 7.6B | ViT-600M | Cross-Attn Merger | Qwen2-7B | **Main results** |

**Scale strategy**:
- **2B models**: All ablation experiments (Phase 1-2). Fits on 1x A100-80GB.
- **7B-8B models**: Main comparison table (Phase 3). Requires 2x A100-80GB with ZeRO-2.

### 2.2 Languages (5, typologically diverse)

| Language | Script | Family | Multimodal Data | Rationale |
|----------|--------|--------|----------------|-----------|
| **Korean** | Hangul | Koreanic | AI Hub VQA (~100K) | Primary motivation; agglutinative |
| **Japanese** | Kanji+Kana | Japonic | JA-VG-VQA (~100K) | Mixed script; CJK |
| **Chinese** | Hanzi | Sinitic | LLaVA-zh (~150K) | Logographic; CJK |
| **Arabic** | Arabic script | Semitic | XM3600-ar + translated (~80K) | RTL; morphologically rich |
| **Thai** | Thai script | Kra-Dai | XM3600-th + translated (~60K) | No word boundaries; tonal |

**Why these 5**: Cover 4 different script systems, 5 language families, key typological properties (agglutinative, logographic, RTL, no spaces). Results generalize broadly.

### 2.3 Benchmarks (8 per language)

**Visual Grounding (core contribution)**:

| Benchmark | Type | What It Measures | Languages |
|-----------|------|-----------------|-----------|
| **xGQA** | Visual QA | Cross-lingual visual reasoning | all 5 |
| **POPE** (multilingual) | Hallucination | Visual hallucination rate | all 5 |
| **CountBenchQA** | Counting | Object counting accuracy | all 5 |
| **Spatial-Bench** | Spatial | Spatial relationship understanding | all 5 |

**General MLLM Capability**:

| Benchmark | Type | What It Measures | Scale |
|-----------|------|-----------------|-------|
| **MMBench** (multilingual) | General | Overall MLLM capability | 7B only |
| **SEED-Bench-2** (multilingual) | General | Multi-dimensional evaluation | 7B only |
| **OCRBench** | OCR | Text recognition in images | all |
| **MMStar** | Reasoning | Multi-modal reasoning | 7B only |

**Language Retention**:

| Benchmark | Type | Purpose |
|-----------|------|---------|
| VQAv2 (English) | VQA | English forgetting detection |
| Language-specific text NLU | NLU | Target language retention |

**Novel Diagnostic Metric — Visual Dependency Score (VDS)**:
```
VDS = Accuracy(with_image) - Accuracy(without_image)
```
Computed on all VQA benchmarks. A model with VDS ≈ 0 has complete visual blindness.

### 2.4 Baselines (6)

| # | Baseline | Description | Training Cost | Source |
|---|----------|-------------|---------------|--------|
| B0 | **Pre-trained (no FT)** | Original model, no language adaptation | 0 | — |
| B1 | **Standard LoRA** | LoRA on LLM, frozen vision encoder + projector | 1.0x | Standard |
| B2 | **Full FT (LLM+Proj)** | Full FT of LLM + projector, frozen vision encoder | 1.2x | Standard |
| B3 | **VIRAL** | B2 + VIRAL alignment loss (static target) | 1.3x | VIRAL (2024) |
| B4 | **Vision Unfreezing** | Unfreeze vision encoder + LLM + projector | 3-5x | Standard |
| B5 | **Staged Training** | 3-stage progressive unfreezing (VARCO-VISION style) | 2.0x | VARCO (2024) |

All baselines use the **same data** per language for fair comparison.

---

## 3. Phase 1: Diagnostic Study (Drift Quantification)

### 3.1 Objective
**Prove that visual token embedding drift exists, is measurable, and correlates with visual grounding degradation.**

This phase provides the **empirical foundation** for VDC — without it, the method lacks motivation.

### 3.2 Measurements

#### M1: Embedding Drift Quantification
For each baseline (B1-B5) × each language (5) × each model (3 × 2B):
- **MMD** (Maximum Mean Discrepancy) between projected visual tokens at t=0 vs t=T
- **Wasserstein distance** between visual and text token distributions per layer
- **Cosine similarity drift**: avg cos_sim(v_token, nearest_text_token) over training
- Plot **drift trajectory** over training steps

#### M2: Attention Distribution Shift
- Per-layer visual token attention ratio: `sum(attn[text→visual]) / sum(attn[text→all])`
- Compare pre-FT vs post-FT for each language × model
- Identify which layers show largest shift

#### M3: Projector Alignment Degradation
- CKA between projector output and vision encoder output (pre vs post FT)
- CKA between projector output and LLM layer-0 representations
- t-SNE visualization of visual vs text tokens in LLM embedding space

#### M4: Visual Dependency Score
- Compute VDS on all VQA benchmarks, pre vs post FT
- Scatter plot: drift magnitude vs VDS drop (expect negative correlation)

### 3.3 Phase 1 Experimental Matrix

| Model | Languages | Baselines | Total Runs | GPU-days |
|-------|-----------|-----------|------------|----------|
| InternVL2.5-2B | 5 | B1, B2 | 10 | 10 |
| LLaVA-OV-0.5B | 5 | B1, B2 | 10 | 5 |
| Qwen2-VL-2B | 5 | B1, B2 | 10 | 10 |

**Diagnostic analysis** (inference-only): ~3 GPU-days
**Total Phase 1**: ~28 GPU-days → ~14 days on 2 GPUs

### 3.4 Expected Result (Paper Figure 1)

A scatter plot showing **strong negative correlation** between drift magnitude and VDS across 30 data points (3 models × 5 languages × 2 baselines), with r² > 0.7. This is the "aha" figure that motivates VDC.

---

## 4. Phase 2: VDC Ablation Study (2B Models)

### 4.1 Objective
Determine the contribution of each VDC component and optimize hyperparameters.

### 4.2 Component Ablation (InternVL2.5-2B, Korean)

| Variant | L_drift | L_contrast | L_align | MA-LoRA | Description |
|---------|---------|------------|---------|---------|-------------|
| **V0** | — | — | — | — | Standard LoRA (B1, reproduce) |
| **V1** | ✓ | — | — | — | Drift compensation only |
| **V2** | — | ✓ | — | — | Contrastive grounding only |
| **V3** | — | — | ✓ | — | Alignment only (≈ VIRAL) |
| **V4** | — | — | — | ✓ | Modality-aware LoRA only |
| **V5** | ✓ | ✓ | — | — | Drift + contrastive |
| **V6** | ✓ | — | ✓ | — | Drift + alignment |
| **V7** | ✓ | ✓ | ✓ | — | All losses, uniform LoRA |
| **V8** | ✓ | ✓ | ✓ | ✓ | **Full VDC** |

**9 variants × 3 seeds × 1 model × 1 language = 27 runs**
GPU-days: ~27 (1 GPU-day per run) → ~14 days on 2 GPUs

### 4.3 Hyperparameter Search (InternVL2.5-2B, Korean)

| Hyperparameter | Search Space | Default |
|---------------|-------------|---------|
| λ₁ (drift) | {0.1, 0.3, 0.5, 1.0} | 0.5 |
| λ₂ (contrast) | {0.05, 0.1, 0.3} | 0.1 |
| λ₃ (align) | {0.1, 0.3, 0.5} | 0.3 |
| α (adaptive weight) | {0.5, 1.0, 2.0} | 1.0 |
| Anchor update freq (N) | {50, 100, 500} | 100 |
| Contrast freq (K) | {1, 2, 5} | 2 |
| MA-LoRA base rank | {8, 16, 32} | 16 |
| MA-LoRA β (projector) | {1.0, 1.5, 2.0} | 1.5 |

**Strategy**: Sequential search (not grid) — fix all others, sweep one at a time.
~20 additional runs → ~20 GPU-days → ~10 days on 2 GPUs

### 4.4 Cross-Architecture Validation (2B, Korean, Full VDC)

| Model | GPU-days |
|-------|----------|
| InternVL2.5-2B (already done) | 0 |
| LLaVA-OV-0.5B | 2 |
| Qwen2-VL-2B | 3 |

**Total Phase 2**: ~52 GPU-days → ~26 days on 2 GPUs

### 4.5 Phase 2 Analysis

- **Component importance**: Bar chart of VDS recovery per component (V1-V4)
- **Interaction effects**: V5-V8 vs sum of individual effects → synergy?
- **ANOVA**: Main effects + 2-way interactions on VDS
- **Hyperparameter sensitivity**: Heatmaps for key λ combinations

---

## 5. Phase 3: Main Results (7B-8B Models, 5 Languages)

### 5.1 Objective
Demonstrate VDC superiority over all baselines at production scale, across 5 languages.

### 5.2 Main Comparison Table (Paper Table 1)

**InternVL2.5-8B** across 5 languages, 8 benchmarks:

| Method | Ko-xGQA | Ko-POPE↓ | Ko-Count | Ko-Spatial | Ko-OCR | Ko-VDS | Avg |
|--------|---------|----------|----------|------------|--------|--------|-----|
| B0: Pre-trained | — | — | — | — | — | — | — |
| B1: Standard LoRA | | | | | | | |
| B2: Full FT | | | | | | | |
| B3: VIRAL | | | | | | | |
| B4: Unfreezing | | | | | | | |
| B5: Staged | | | | | | | |
| **VDC (Ours)** | **bold** | **bold** | **bold** | **bold** | **bold** | **bold** | **bold** |

**Same table repeated for**: Japanese, Chinese, Arabic, Thai.

**Summary table**: Average across all 5 languages (Paper Table 2).

| Method | xGQA | POPE↓ | Count | Spatial | OCR | VDS | Avg | Cost |
|--------|------|-------|-------|---------|-----|-----|-----|------|
| B1: Standard LoRA | | | | | | | | 1.0x |
| B3: VIRAL | | | | | | | | 1.3x |
| B5: Staged | | | | | | | | 2.0x |
| **VDC (Ours)** | | | | | | | | 1.5x |

### 5.3 Second Architecture: Qwen2-VL-7B (Paper Table 3)

Same comparison on Qwen2-VL-7B for Korean, Japanese, Arabic (3 languages) to show architecture-agnosticism.

### 5.4 Phase 3 Compute Budget

| Model | Languages | Methods | Seeds | Runs | GPU-days |
|-------|-----------|---------|-------|------|----------|
| InternVL2.5-8B | 5 | 7 (B0-B5 + VDC) | 3 | 105 | ~210 |
| Qwen2-VL-7B | 3 | 4 (B1,B3,B5,VDC) | 3 | 36 | ~72 |

**Total Phase 3**: ~282 GPU-days → ~141 days on 2 GPUs

> **Note**: This is the most expensive phase. Can be parallelized with more GPUs or reduced by:
> - Running 1 seed first, only running 3 seeds for final numbers
> - Prioritizing Korean + Japanese + Arabic (3 languages instead of 5)
> - Using only InternVL2.5-8B initially, adding Qwen2-VL-7B if results are strong

**Minimum viable Phase 3** (for early submission):
- InternVL2.5-8B × 3 languages × 4 methods (B1, B3, B5, VDC) × 1 seed = 12 runs → ~24 GPU-days

---

## 6. Analysis & Visualization (Paper Figures)

### Figure 1: Drift-VDS Correlation (Phase 1)
Scatter plot: x = drift magnitude, y = VDS. 30+ data points. Shows strong negative correlation.
**Message**: "Drift causes visual blindness."

### Figure 2: Drift Trajectory Over Training
Line plot: drift magnitude over training steps for B1 (LoRA) vs VDC.
**Message**: "VDC controls drift while B1 drift grows unchecked."

### Figure 3: Component Ablation (Phase 2)
Bar chart: VDS recovery for V0-V8.
**Message**: "L_drift is the most important component; all components contribute."

### Figure 4: Attention Heatmaps (Before/After)
Layer-wise attention ratio for visual tokens: Pre-trained → B1 (LoRA) → VDC.
**Message**: "VDC preserves visual attention patterns."

### Figure 5: t-SNE of Embedding Space
Visual vs text token distributions in LLM space: Pre-trained → B1 → VDC.
**Message**: "VDC keeps visual tokens in-distribution."

### Figure 6: Cross-Language Consistency
Radar chart: VDS across 5 languages for B1 vs B3 vs B5 vs VDC.
**Message**: "VDC is language-agnostic — consistent improvement across all scripts."

---

## 7. Hyperparameters

### 7.1 Standard Fine-tuning (All baselines)

```yaml
optimizer: AdamW
weight_decay: 0.01
lr_scheduler: cosine
warmup_ratio: 0.03
max_length: 2048
bf16: true
gradient_checkpointing: true

# 2B models
batch_size_2b: 16  # effective
per_device_batch_2b: 4
grad_accum_2b: 2

# 7B-8B models
batch_size_7b: 16  # effective
per_device_batch_7b: 1
grad_accum_7b: 8  # with 2 GPUs
```

### 7.2 Baseline-Specific

```yaml
B1_standard_lora:
  lr: 2e-5
  epochs: 3
  lora_r: 64
  lora_alpha: 128
  lora_target: [q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj]
  trainable: [lora]
  frozen: [vision_encoder, projector, llm_base]

B2_full_ft:
  lr: 2e-5
  epochs: 3
  trainable: [llm, projector]
  frozen: [vision_encoder]

B3_viral:
  lr: 2e-5
  epochs: 3
  trainable: [llm, projector]
  frozen: [vision_encoder]
  viral_lambda: 0.5

B4_unfreezing:
  lr: 2e-6  # lower LR for vision encoder stability
  epochs: 3
  trainable: [vision_encoder, llm, projector]

B5_staged:
  stage1: {trainable: projector, lr: 1e-3, epochs: 1, data: captioning}
  stage2: {trainable: llm_lora, lr: 2e-5, epochs: 2, data: vqa}
  stage3: {trainable: [llm_lora, projector], lr: 5e-6, epochs: 1, data: vqa_curated}
```

### 7.3 VDC Configuration

```yaml
vdc:
  # Component 1: Drift compensation
  drift:
    lambda: 0.5
    anchor_set_size: 50
    anchor_update_freq: 100  # compute L_drift every 100 steps
    metric: "mmd"  # Maximum Mean Discrepancy
    kernel: "rbf"

  # Component 2: Contrastive visual grounding
  contrast:
    lambda: 0.1
    compute_freq: 2  # every 2nd step
    stop_grad_text_only: true

  # Component 3: Drift-adaptive alignment
  align:
    lambda_base: 0.3
    adaptive_alpha: 1.0  # λ = λ_base × (1 + α × drift_magnitude)
    critical_layers: "auto"  # select layers with highest MIR
    num_layers: 4

  # Component 4: Modality-aware LoRA
  lora:
    base_rank: 16
    projector_beta: 1.5  # projector rank = base × β
    target_modules: [q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj]
    rank_allocation: "mir_proportional"  # rank[l] = base × MIR[l]

  # Training
  base_lr: 2e-5
  epochs: 3
  seed: [42, 123, 456]
```

---

## 8. Implementation Plan

### 8.1 Required Scripts

| Script | Purpose | Phase | Priority |
|--------|---------|-------|----------|
| `vdc/drift_tracker.py` | Anchor management + MMD computation | All | P0 |
| `vdc/contrastive_loss.py` | Dual forward pass + contrastive grounding loss | All | P0 |
| `vdc/adaptive_align.py` | Drift-adaptive visual alignment | All | P0 |
| `vdc/modality_lora.py` | MIR computation + rank allocation | All | P0 |
| `vdc/trainer.py` | Unified VDC trainer (all components) | All | P0 |
| `diagnosis/drift_measure.py` | Offline drift quantification | P1 | P0 |
| `diagnosis/attention_analysis.py` | Layer-wise attention extraction | P1 | P0 |
| `diagnosis/vds_compute.py` | Visual dependency score computation | P1 | P0 |
| `baselines/train_lora.py` | Standard LoRA fine-tuning | P1-3 | P0 |
| `baselines/train_viral.py` | VIRAL baseline | P3 | P1 |
| `baselines/train_staged.py` | Staged training baseline | P3 | P1 |
| `eval/evaluate_all.py` | Unified multi-benchmark evaluation | All | P0 |
| `eval/multilingual_benchmarks.py` | Benchmark data preparation per language | All | P0 |
| `analysis/plot_drift.py` | Drift trajectory visualization | P1 | P1 |
| `analysis/plot_ablation.py` | Ablation bar charts | P2 | P2 |

### 8.2 Execution Timeline

```
Phase 1: Diagnostic Study (4 weeks, ~28 GPU-days)
├── Week 1: Setup, data preparation, anchor concept curation
│   ├── Download/prepare multimodal data for 5 languages
│   ├── Curate 50 anchor concepts with bilingual translations
│   └── Implement drift measurement tools
├── Week 2: Baseline training (2B models × 5 languages × 2 baselines)
│   ├── 30 training runs (parallelized on 2 GPUs)
│   └── Checkpoint saving at regular intervals for drift analysis
├── Week 3: Diagnostic analysis
│   ├── Drift quantification (MMD, Wasserstein, cosine)
│   ├── Attention distribution analysis (per-layer)
│   ├── VDS computation on all benchmarks
│   └── Projector alignment (CKA) analysis
├── Week 4: Analysis & Figure 1 generation
│   ├── Drift-VDS correlation scatter plot
│   ├── Statistical tests (Pearson r, Spearman ρ)
│   └── Decision: proceed to Phase 2 if correlation r² > 0.5
└── Deliverable: Empirical proof that drift causes visual blindness

Phase 2: VDC Ablation (6 weeks, ~52 GPU-days)
├── Week 5-6: Implement VDC components
│   ├── drift_tracker.py, contrastive_loss.py, adaptive_align.py
│   ├── modality_lora.py, unified trainer
│   └── Unit tests for each component
├── Week 7-8: Component ablation (V0-V8, 27 runs)
│   ├── Train all variants (3 seeds each)
│   └── Evaluate on Korean benchmarks
├── Week 9: Hyperparameter search (~20 runs)
│   └── Sequential sweep of key hyperparameters
├── Week 10: Cross-architecture validation
│   ├── Full VDC on LLaVA-OV-0.5B + Qwen2-VL-2B
│   └── Ablation analysis, Figure 3 generation
└── Deliverable: Optimal VDC configuration, component importance

Phase 3: Main Results (8-12 weeks, ~282 GPU-days)
├── Week 11-14: InternVL2.5-8B experiments
│   ├── All baselines × 5 languages × 3 seeds
│   └── VDC × 5 languages × 3 seeds
├── Week 15-17: Qwen2-VL-7B experiments
│   ├── Key baselines × 3 languages × 3 seeds
│   └── VDC × 3 languages × 3 seeds
├── Week 18-19: Comprehensive evaluation
│   ├── All 8 benchmarks per language
│   ├── Statistical tests (paired t-test, bootstrap CI)
│   └── Generate all paper tables and figures
├── Week 20-22: Paper writing
│   ├── Introduction, related work, method
│   ├── Experiments, results, analysis
│   └── Conclusion, appendix
└── Deliverable: Full paper draft

Total: ~22 weeks, ~362 GPU-days
With 2 GPUs: ~181 calendar days (but phases overlap → compressed to ~22 weeks)
```

---

## 9. Success Criteria

### 9.1 Minimum Viable (Conference Submission)

| Criterion | Threshold |
|-----------|-----------|
| VDC > all baselines on VDS | p < 0.05 on ≥3/5 languages |
| VDC > VIRAL on avg xGQA | ≥2% absolute improvement |
| Consistent across ≥3 languages | No language where VDC < best baseline |
| Ablation shows all components contribute | Each component adds ≥1% VDS |
| Works on ≥2 model architectures | InternVL + Qwen2-VL |
| Drift-VDS correlation | r² > 0.5 |

### 9.2 Strong Result (Oral/Spotlight)

| Criterion | Threshold |
|-----------|-----------|
| All minimum criteria + | |
| VDC > all baselines on ALL 5 languages | p < 0.01 |
| VDS recovery ≥90% of pre-FT level | Across all languages |
| English retention | ≤2% drop on VQAv2 |
| Training cost | ≤1.5x standard LoRA |
| Clear drift dynamics visualization | Compelling Figure 2 |

---

## 10. Risk Analysis

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|------------|
| Drift correlation weak (r² < 0.5) | Low | Fatal | Pivot to pure contrastive method; reframe as analysis paper |
| VDC ≈ VIRAL (no significant improvement) | Medium | High | L_drift + L_contrast are novel components absent in VIRAL; if still tied, ablation shows drift compensation is the differentiator |
| 7B training too slow for deadline | High | Medium | Submit with 2B results + 1 language 7B validation; add 7B results in camera-ready |
| Multilingual data quality varies | Medium | Medium | Standardize data size per language; report per-language results separately |
| Anchor concept selection matters | Medium | Low | Ablation on anchor set size (25/50/100); use universal ImageNet concepts |
| L_contrast doubles training time | Low | Medium | Compute every K steps; stop_grad on text-only branch; 30% actual overhead |

---

## 11. Comparison with Previous Design (v1 → v2)

| Aspect | v1 (Previous) | v2 (Current) |
|--------|--------------|--------------|
| **Novelty** | Apply existing methods to Korean | Novel VDC method with 4 components |
| **Scale** | 2B only | 2B (ablation) + **7B-8B (main)** |
| **Languages** | Korean only | **5 languages (Ko/Ja/Zh/Ar/Th)** |
| **Benchmarks** | AI Hub Korean VQA (1 dataset) | **8+ benchmarks per language** |
| **Baselines** | No proper baselines | **6 baselines including VIRAL, staged** |
| **Contribution** | Empirical study | **New method + diagnostic framework** |
| **Target venue** | Workshop / short paper | **Main conference (ACL/EMNLP/NeurIPS)** |

---

## Notes

### 핵심 전략 (Core Strategy)
- **Story**: drift exists (Phase 1) → drift causes blindness (correlation) → VDC compensates drift (Phase 2) → VDC works at scale across languages (Phase 3)
- **Reviewer-proof**: 5언어, 2스케일(2B+7B), 3아키텍처, 8벤치마크, 6베이스라인, 컴포넌트 ablation, 통계 검정
- **Novelty claim**: "First work to model and compensate visual embedding drift during cross-lingual MLLM adaptation"

### 리소스 최적화
- Phase 1 결과가 약하면 (r² < 0.5) 일찍 방향 전환 가능
- Phase 3는 점진적 확장 가능: 1언어 → 3언어 → 5언어
- 7B 실험은 deadline에 따라 유연하게 조정

### 기존 실험과의 관계
- **EXP-20260211-001** (TBI-MLLM): VDC와 아키텍처 개선 결합 가능 (future work)
- **EXP-20260212-001** (InternViT-Next): Better encoder + VDC = synergy (future work)
