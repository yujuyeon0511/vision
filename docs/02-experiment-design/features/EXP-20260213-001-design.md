# Experiment Design: Korean MLLM Visual Blindness — Diagnosis & Mitigation

> Research Phase: **2 - Experiment Design**
> Experiment ID: **EXP-20260213-001**
> Created: 2026-02-13
> Author: Juyeon
> Status: Draft
> Hypothesis Document: [korean-mllm-visual-blindness-hypothesis.md](../../01-hypothesis/features/korean-mllm-visual-blindness-hypothesis.md)

---

## 1. Experiment Overview

### 1.1 Objective

Systematically diagnose why open-source vision encoder + LLM decoder multimodal models exhibit **"visual blindness"** when fine-tuned on Korean datasets, and develop practical mitigation strategies. This experiment will:

1. **Diagnose**: Quantify visual token attention collapse before/after Korean fine-tuning across multiple MLLM architectures
2. **Isolate**: Determine the relative contribution of each root cause (attention bias, catastrophic forgetting, projector misalignment, data quality) through controlled ablation
3. **Mitigate**: Evaluate 6 mitigation strategies and identify the optimal training recipe for Korean MLLMs that preserves visual grounding

### 1.2 Hypotheses Being Tested

| ID | Hypothesis | Test Method |
|----|-----------|-------------|
| H1 | Visual attention collapse: attention key-space becomes text-centric after Korean fine-tuning | Phase 1: Attention distribution analysis |
| H2 | Catastrophic forgetting of visual processing capability | Phase 2: Ablation with selective freezing |
| H3 | Projector misalignment from LLM embedding space drift | Phase 1: CKA/cosine similarity analysis |
| H4 | Data distribution mismatch (quality/quantity) | Phase 2: Data scaling ablation |
| H5 | Compound effect of H1-H4 | Phase 2: Factorial ablation |

### 1.3 Base Models (3 Architectures)

To ensure generalizability, we test across 3 representative MLLM architectures:

| Model | Vision Encoder | Projector | LLM | Total Params | Source |
|-------|---------------|-----------|-----|-------------|--------|
| **InternVL2.5-2B** | InternViT-300M | 2-layer MLP | InternLM2-1.8B | ~2.2B | OpenGVLab |
| **LLaVA-OneVision-0.5B** | SigLIP-400M | 2-layer MLP | Qwen2-0.5B | ~0.9B | LLaVA-VL |
| **Qwen2-VL-2B** | ViT-600M (native) | Cross-attention merger | Qwen2-1.5B | ~2.1B | Alibaba |

**Rationale**: All models fit on a single A100-80GB, enabling rapid iteration. They cover the three major MLLM families and projector types (MLP, cross-attention, spatial merger).

### 1.4 Three-Phase Experimental Strategy

**Phase 1: Diagnosis** (5-7 GPU-days)
- Analyze pre-trained models → Korean fine-tuned models
- Measure attention, representation, and performance changes
- Output: Diagnostic report quantifying visual blindness

**Phase 2: Root Cause Isolation** (20-30 GPU-days)
- Controlled ablation: freeze/unfreeze components, vary data
- Factorial design to decompose variance
- Output: Factor importance ranking

**Phase 3: Mitigation** (15-25 GPU-days)
- Test 6 mitigation strategies
- Compare on unified benchmark
- Output: Best training recipe for Korean MLLM

### 1.5 Hardware Requirements

**Available:** 2x NVIDIA A100-PCIE-80GB (160GB total)

**Memory estimates (worst case per model):**
| Model | Model (bf16) | Optimizer (AdamW) | Gradients | Activations | Total |
|-------|-------------|-------------------|-----------|-------------|-------|
| InternVL2.5-2B | ~4.4 GB | ~8.8 GB | ~4.4 GB | ~6 GB | ~24 GB |
| LLaVA-OV-0.5B | ~1.8 GB | ~3.6 GB | ~1.8 GB | ~3 GB | ~10 GB |
| Qwen2-VL-2B | ~4.2 GB | ~8.4 GB | ~4.2 GB | ~5 GB | ~22 GB |

All models fit on a single A100 with gradient checkpointing. Second GPU enables parallel experiments.

---

## 2. Phase 1: Diagnosis — Quantifying Visual Blindness

### 2.1 Experimental Setup

For each of the 3 base models, we create paired conditions:
- **Condition A (Baseline)**: Original pre-trained model (English-centric)
- **Condition B (Korean FT)**: Model fine-tuned on Korean multimodal data (standard recipe)

Korean fine-tuning recipe (standard, to reproduce the problem):
```yaml
data: Korean VQA + Korean captioning (AI Hub, ~100K samples)
epochs: 3
lr: 2e-5
trainable: LLM + projector (vision encoder frozen)
batch_size: 16 (effective, with gradient accumulation)
```

### 2.2 Diagnostic Measurements

#### D1: Visual Token Attention Distribution
**What**: For each layer `l` and attention head `h`, compute the fraction of total attention mass allocated to visual tokens vs. text tokens.

```
Visual_Attention_Ratio(l, h) = sum(attn[text→visual]) / sum(attn[text→all])
```

**Measurements**:
- Per-layer visual attention ratio (averaged across heads)
- Per-layer visual attention entropy (higher = more distributed)
- Attention sink detection: identify visual tokens with disproportionate attention
- Compare Condition A vs. B at each layer

**Tool**: Custom hook on `model.llm.layers[l].self_attn` to extract attention weights during inference on 500 test images.

#### D2: Projector Alignment Analysis
**What**: Measure how well projector outputs align with the LLM's current embedding space.

**Measurements**:
- Cosine similarity between projector outputs and nearest text embeddings
- CKA (Centered Kernel Alignment) between vision encoder outputs and projector outputs
- CKA between projector outputs and LLM layer-0 residual stream activations
- Distribution of projected visual tokens in LLM embedding space (t-SNE/UMAP visualization)

#### D3: Visual Information Retention Probing
**What**: Linear probes on intermediate LLM representations to test if visual information is present but unused.

**Probing tasks**:
- Object presence (binary: "is there a dog?")
- Spatial relationship ("is X above Y?")
- Color recognition ("what color is the car?")
- Scene classification (indoor/outdoor, etc.)

**Measurements per layer**:
- Probe accuracy (Condition A vs. B)
- Layer at which visual information drops below random chance

#### D4: Image-Dependent vs. Independent Performance
**What**: Separate VQA questions into those requiring the image vs. answerable from text alone.

**Method**: Run each question twice: (1) with image, (2) with blank/noise image. If answer matches, question is image-independent.

**Metric**: `Visual_Dependency_Score = Accuracy(with_image) - Accuracy(without_image)`

### 2.3 Phase 1 Variants

| Variant | Model | Condition | Description | GPU-hours |
|---------|-------|-----------|-------------|-----------|
| D1-A1 | InternVL2.5-2B | Baseline | Pre-trained attention analysis | 2 |
| D1-B1 | InternVL2.5-2B | Korean FT | Post-FT attention analysis | 8 + 2 |
| D1-A2 | LLaVA-OV-0.5B | Baseline | Pre-trained attention analysis | 1 |
| D1-B2 | LLaVA-OV-0.5B | Korean FT | Post-FT attention analysis | 4 + 1 |
| D1-A3 | Qwen2-VL-2B | Baseline | Pre-trained attention analysis | 2 |
| D1-B3 | Qwen2-VL-2B | Korean FT | Post-FT attention analysis | 8 + 2 |
| D2-* | All 3 models | Both | Projector alignment (per model) | 3 x 1 |
| D3-* | All 3 models | Both | Linear probing (per model) | 3 x 4 |
| D4-* | All 3 models | Both | Image dependency test (per model) | 3 x 1 |

**Total Phase 1**: ~54 GPU-hours (~2.3 GPU-days per GPU, ~1.2 days with 2 GPUs)

---

## 3. Phase 2: Root Cause Isolation — Ablation Study

### 3.1 Factor Design

We use a **factorial ablation** design with 4 binary factors:

| Factor | Level 0 (Control) | Level 1 (Treatment) |
|--------|-------------------|---------------------|
| **F1: Trainable Components** | LLM + projector (standard) | Only LoRA on LLM (minimal LLM shift) |
| **F2: Projector Handling** | Train projector jointly | Freeze projector (test LLM-only effect) |
| **F3: Data Quality** | Raw Korean VQA (~100K) | Curated Korean VQA with visual grounding (~30K) |
| **F4: Visual Regularization** | None | VIRAL-style alignment loss on projector output |

### 3.2 Ablation Variants (InternVL2.5-2B primary, validate top-3 on LLaVA & Qwen2-VL)

| Variant | F1 | F2 | F3 | F4 | Description | GPU-days |
|---------|----|----|----|----|-------------|----------|
| **A0** | 0 | 0 | 0 | 0 | Standard Korean FT (reproduce baseline) | 1.5 |
| **A1** | 1 | 0 | 0 | 0 | LoRA-only LLM adaptation | 1.0 |
| **A2** | 0 | 1 | 0 | 0 | Frozen projector | 1.5 |
| **A3** | 0 | 0 | 1 | 0 | Curated data only | 1.0 |
| **A4** | 0 | 0 | 0 | 1 | Visual alignment regularization | 2.0 |
| **A5** | 1 | 0 | 1 | 0 | LoRA + curated data | 1.0 |
| **A6** | 1 | 0 | 0 | 1 | LoRA + visual regularization | 1.5 |
| **A7** | 0 | 0 | 1 | 1 | Curated data + visual regularization | 1.5 |
| **A8** | 1 | 0 | 1 | 1 | LoRA + curated + visual reg (best combo?) | 1.5 |
| **A9** | 1 | 1 | 1 | 1 | All mitigations (upper bound test) | 1.5 |

**Primary model (InternVL2.5-2B)**: 10 variants x ~1.4 GPU-days avg = **~14 GPU-days**

**Cross-architecture validation (top-3 variants on LLaVA & Qwen2-VL)**:
3 variants x 2 models x ~1.2 GPU-days = **~7.2 GPU-days**

**Total Phase 2**: ~21.2 GPU-days → ~11 days with 2 GPUs

### 3.3 Evaluation for Phase 2

All variants evaluated on:

| Metric | Dataset | What it Measures |
|--------|---------|-----------------|
| Korean VQA Accuracy | AI Hub Korean VQA (test split) | Overall Korean visual QA |
| Visual Dependency Score | VQA subset (image vs. no-image) | Whether model actually uses images |
| Visual Attention Ratio | 500 test images (Layer-wise) | Attention health indicator |
| Projector Alignment (CKA) | 500 test images | Embedding space alignment |
| English VQA Accuracy | VQAv2 (val split) | Forgetting detection |
| Korean text-only Accuracy | KoBEST (subset) | Korean language retention |

### 3.4 Statistical Analysis

- **Main effect** of each factor via ANOVA on Visual Dependency Score
- **Interaction effects** between factors (2-way interactions)
- **Variance decomposition**: What fraction of visual blindness is explained by each factor?
- All results over 3 seeds (42, 123, 456), reported as mean +/- std

---

## 4. Phase 3: Mitigation — Solution Comparison

### 4.1 Mitigation Strategies

Based on literature review and Phase 2 results, we test 6 mitigation strategies:

| ID | Strategy | Inspired By | Key Mechanism |
|----|----------|-------------|---------------|
| **M1** | VIRAL Regularization | VIRAL (2024) | L2 loss between projector output and frozen vision encoder representation |
| **M2** | Visual Attention Redistribution (VAR) | Visual Attention Sink (ICLR 2025) | Post-hoc attention weight redistribution in image-centric heads |
| **M3** | Staged Korean Adaptation | VARCO-VISION (2024) | 3-stage: alignment→basic SFT→advanced SFT with progressive unfreezing |
| **M4** | Projector Re-alignment | Novel | Post-Korean-FT projector re-calibration with 5K high-quality visual pairs |
| **M5** | Visual Debias Decoding | Debiasing MLLMs (2024) | Inference-time: contrast output with/without image, amplify visual signal |
| **M6** | Combined Best | Phase 2 results | Best combination from factorial analysis + best mitigation |

### 4.2 Strategy Details

#### M1: VIRAL Regularization
```
L_total = L_next_token + λ_viral * L_viral
L_viral = MSE(projector_output, stop_grad(vision_encoder_output))
λ_viral ∈ {0.1, 0.5, 1.0}  # hyperparameter search
```
- Applied during Korean fine-tuning
- Forces projector outputs to remain aligned with vision encoder
- Additional compute cost: ~10% (one extra forward pass through projector comparison)

#### M2: Visual Attention Redistribution (VAR)
- **Training-free** method applied at inference
- Identify "image-centric" attention heads (heads where visual tokens receive >50% attention in baseline)
- In those heads, redistribute attention from sink tokens to semantic visual tokens
- Redistribution formula: `attn_new[i] = attn[i] * (1 + α * visual_mask[i])`, then renormalize
- `α ∈ {0.5, 1.0, 2.0}`

#### M3: Staged Korean Adaptation (VARCO-VISION inspired)
```
Stage 1 (Alignment):     Train projector only, lr=1e-3, 1 epoch on Korean image-text pairs
Stage 2 (Basic SFT):     Train LLM (LoRA r=64), lr=2e-5, 2 epochs on Korean VQA
Stage 3 (Advanced SFT):  Train LLM + projector, lr=5e-6, 1 epoch on curated Korean multimodal
```
- Key insight: re-align projector BEFORE training LLM on Korean
- Prevents embedding space drift from breaking projector alignment

#### M4: Projector Re-alignment
```
Step 1: Standard Korean fine-tuning (LLM + projector, 3 epochs)
Step 2: Freeze LLM, retrain projector only, lr=1e-4, 0.5 epoch on 5K visual grounding pairs
```
- Hypothesis: projector becomes misaligned → re-align it after LLM adaptation
- Cheapest mitigation strategy (~0.5 GPU-day overhead)

#### M5: Visual Debias Decoding (Inference-time)
```
logits_final = (1 + β) * logits(image, text) - β * logits(noise_image, text)
β ∈ {0.3, 0.5, 1.0}
```
- **No training required** — applied at inference
- Amplifies the contribution of visual information to next-token prediction
- Can be combined with any training strategy

#### M6: Combined Best
- Determined after Phase 2 and M1-M5 results
- Expected: M3 (staged training) + M1 (VIRAL) + M5 (debias decoding)

### 4.3 Phase 3 Variants

| Variant | Strategy | Model | Training | Inference | GPU-days |
|---------|----------|-------|----------|-----------|----------|
| M1a | VIRAL (λ=0.1) | InternVL2.5-2B | Yes | Standard | 2.0 |
| M1b | VIRAL (λ=0.5) | InternVL2.5-2B | Yes | Standard | 2.0 |
| M1c | VIRAL (λ=1.0) | InternVL2.5-2B | Yes | Standard | 2.0 |
| M2a | VAR (α=1.0) | InternVL2.5-2B | No | Modified | 0.2 |
| M2b | VAR (α=2.0) | InternVL2.5-2B | No | Modified | 0.2 |
| M3 | Staged (3-stage) | InternVL2.5-2B | Yes | Standard | 3.5 |
| M4 | Re-align projector | InternVL2.5-2B | Yes | Standard | 2.0 |
| M5a | Debias (β=0.5) | InternVL2.5-2B | No | Modified | 0.2 |
| M5b | Debias (β=1.0) | InternVL2.5-2B | No | Modified | 0.2 |
| M6 | Combined best | InternVL2.5-2B | Yes | TBD | 3.5 |
| M6-LLaVA | Combined best | LLaVA-OV-0.5B | Yes | TBD | 2.0 |
| M6-Qwen | Combined best | Qwen2-VL-2B | Yes | TBD | 3.0 |

**Total Phase 3**: ~20.8 GPU-days → ~10.5 days with 2 GPUs

### 4.4 Evaluation for Phase 3

**Primary Metrics** (all mean +/- std over 3 seeds):

| Metric | Description | Target |
|--------|-------------|--------|
| Korean VQA Accuracy | AI Hub Korean VQA test | ≥ Baseline - 2% |
| Visual Dependency Score (VDS) | Accuracy(image) - Accuracy(no_image) | ≥ 0.15 (baseline-level) |
| Visual Attention Ratio (VAR) | Avg visual token attention in layers 1-8 | ≥ 80% of pre-FT value |
| Projector CKA | CKA(projector_out, vision_encoder_out) | ≥ 0.85 |

**Secondary Metrics**:

| Metric | Description | Purpose |
|--------|-------------|---------|
| English VQA (VQAv2-val) | English VQA accuracy | Forgetting detection |
| Korean MMMU (if available) | Multimodal reasoning | Complex reasoning |
| Korean TextVQA | Text in images (Korean) | OCR + visual grounding |
| Inference Latency | ms per sample | Overhead measurement |
| Training Cost | GPU-hours | Practicality |

**Statistical Tests**:
- Paired t-test: each mitigation vs. baseline (A0)
- Bootstrap confidence intervals (95%) for VDS
- Effect size (Cohen's d) for main comparisons
- Significance threshold: p < 0.05

---

## 5. Dataset

### 5.1 Training Data

| Dataset | Size | Language | Type | Usage |
|---------|------|----------|------|-------|
| AI Hub Korean VQA | ~100K | Korean | VQA | Primary Korean FT data |
| AI Hub Korean Captioning | ~50K | Korean | Caption | Korean alignment data |
| Curated Korean Visual Grounding | ~30K | Korean | VQA (image-required) | High-quality subset for M3/ablation |
| LLaVA-Instruct-150K | 150K | English | VQA/Conv | English baseline |
| ShareGPT4V-100K | 100K | English | Detailed caption | English visual grounding |

### 5.2 Evaluation Data

| Dataset | Size | Language | Type | Usage |
|---------|------|----------|------|-------|
| AI Hub Korean VQA (test) | ~10K | Korean | VQA | Primary Korean evaluation |
| VQAv2 (val) | ~214K | English | VQA | English retention check |
| Korean VDS subset | ~1K | Korean | VQA (curated) | Image dependency test |
| KoBEST (subset) | ~5K | Korean | Text NLU | Korean language retention |

### 5.3 Data Curation: High-Quality Korean Visual Grounding Set

For the curated ~30K subset (used in F3 and M3):
1. Filter AI Hub Korean VQA for questions that **require** the image (VDS > 0.5 on oracle model)
2. Remove questions answerable from common sense alone
3. Ensure diverse visual content: objects, scenes, text, spatial, counting
4. Balance across question types: what/where/how many/yes-no

---

## 6. Hyperparameters

### 6.1 Standard Korean Fine-tuning (Baseline / Condition B)

```yaml
# Used for Phase 1 Korean FT and Phase 2 baseline (A0)
optimizer: AdamW
learning_rate: 2e-5
weight_decay: 0.01
lr_scheduler: cosine
warmup_ratio: 0.03
batch_size: 16  # effective (gradient accumulation)
per_device_batch_size: 2
gradient_accumulation_steps: 4  # per GPU
num_train_epochs: 3
max_length: 2048
bf16: true
gradient_checkpointing: true
trainable_modules:
  - llm (full)
  - projector (full)
frozen_modules:
  - vision_encoder
seed: [42, 123, 456]
```

### 6.2 LoRA Configuration (Factor F1, Strategies M3)

```yaml
lora:
  r: 64
  alpha: 128
  dropout: 0.05
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
  modules_to_save: null  # no full FT modules
```

### 6.3 VIRAL Regularization (M1)

```yaml
viral:
  lambda: 0.5  # default, search {0.1, 0.5, 1.0}
  loss_type: "mse"  # MSE between projector output and vision encoder output
  stop_gradient: true  # stop gradient on vision encoder side
  normalize: true  # L2 normalize before computing loss
```

### 6.4 Staged Training (M3)

```yaml
stage1_alignment:
  trainable: projector_only
  lr: 1e-3
  epochs: 1
  data: korean_captioning  # image-text pairs
  batch_size: 32

stage2_basic_sft:
  trainable: llm_lora
  lr: 2e-5
  epochs: 2
  data: korean_vqa_full  # all Korean VQA
  lora_r: 64
  batch_size: 16

stage3_advanced_sft:
  trainable: llm_lora + projector
  lr: 5e-6
  epochs: 1
  data: korean_vqa_curated  # high-quality subset
  batch_size: 16
```

---

## 7. Implementation Plan

### 7.1 Required Scripts

| Script | Purpose | Priority |
|--------|---------|----------|
| `experiments/scripts/diagnose_attention.py` | Extract and analyze attention distributions (D1) | P0 |
| `experiments/scripts/diagnose_alignment.py` | CKA analysis and projector alignment (D2) | P0 |
| `experiments/scripts/diagnose_probing.py` | Linear probing for visual info retention (D3) | P1 |
| `experiments/scripts/diagnose_vds.py` | Visual dependency score computation (D4) | P0 |
| `experiments/scripts/train_korean_ft.py` | Standard Korean fine-tuning (Phase 2 baseline) | P0 |
| `experiments/scripts/train_viral.py` | Training with VIRAL regularization (M1) | P1 |
| `experiments/scripts/train_staged.py` | Multi-stage Korean adaptation (M3) | P1 |
| `experiments/scripts/realign_projector.py` | Post-FT projector re-alignment (M4) | P1 |
| `experiments/scripts/inference_var.py` | Visual attention redistribution inference (M2) | P2 |
| `experiments/scripts/inference_debias.py` | Visual debias decoding inference (M5) | P2 |
| `experiments/scripts/evaluate_all.py` | Unified evaluation pipeline | P0 |
| `experiments/scripts/curate_korean_vqa.py` | Data curation for high-quality subset | P0 |

### 7.2 Execution Order

```
Week 1-2: Phase 1 (Diagnosis)
├── Day 1-2: Setup, data download, curate Korean VQA subset
├── Day 3-5: Korean FT on 3 models (D1-B1, D1-B2, D1-B3)
├── Day 6-8: Attention analysis (D1), alignment analysis (D2)
├── Day 9-10: Probing (D3), VDS (D4)
└── Deliverable: Diagnostic report with quantified visual blindness

Week 3-4: Phase 2 (Ablation)
├── Day 1-3: Ablation variants A0-A4 (single factors)
├── Day 4-6: Ablation variants A5-A9 (combinations)
├── Day 7-8: Cross-architecture validation (top-3 on LLaVA, Qwen2-VL)
├── Day 9-10: Statistical analysis, variance decomposition
└── Deliverable: Factor importance report, interaction effects

Week 5-7: Phase 3 (Mitigation)
├── Day 1-3: M1 (VIRAL, 3 λ values)
├── Day 4: M2 (VAR, inference-only)
├── Day 5-7: M3 (Staged training)
├── Day 8: M4 (Projector re-alignment)
├── Day 9: M5 (Debias decoding)
├── Day 10-12: M6 (Combined best, 3 architectures)
├── Day 13-14: Final evaluation, statistical tests
└── Deliverable: Mitigation comparison, best recipe

Week 8: Analysis & Paper Writing
├── Generate all tables and figures
├── Write results section
└── Draft full paper
```

---

## 8. Expected Results and Decision Tree

### 8.1 Phase 1 Expected Outcomes

| Measurement | Expected Finding | If Confirmed | If Not Confirmed |
|-------------|-----------------|--------------|------------------|
| D1: Visual attention drops ≥20% | H1 supported | Proceed to attention-based mitigations | Focus on other factors |
| D2: Projector CKA drops ≥0.15 | H3 supported | Projector re-alignment critical | Projector is robust |
| D3: Probing accuracy drops in mid-layers | H2 supported | Information is lost, not just ignored | Information present but unused |
| D4: VDS drops significantly | Visual blindness confirmed | Core problem validated | Problem may be overestimated |

### 8.2 Phase 2 Decision Tree

```
IF F1 (LoRA) alone restores ≥80% VDS:
  → Minimal LLM shift is sufficient → Focus on LoRA-based solutions

IF F3 (curated data) alone restores ≥80% VDS:
  → Data quality is the primary issue → Focus on data curation

IF F4 (VIRAL) alone restores ≥80% VDS:
  → Alignment loss is sufficient → Recommend as default training practice

IF no single factor restores ≥80% VDS:
  → Compound effect (H5) → Combined mitigation required (M6)

IF F1 x F3 interaction is significant:
  → LoRA + good data are synergistic → Staged training with curated data (M3)
```

### 8.3 Success Criteria (Final)

| Criterion | Threshold | Priority |
|-----------|-----------|----------|
| Korean VQA accuracy recovery | ≥ Baseline(English) - 2% | Must-have |
| Visual Dependency Score | ≥ 0.15 (pre-FT level) | Must-have |
| English VQA retention | ≤ 3% drop from pre-FT | Should-have |
| Training overhead | ≤ 1.5x standard FT cost | Should-have |
| Inference overhead | ≤ 1.1x standard inference | Nice-to-have |

---

## 9. Config File

> Auto-generated config saved to: `experiments/configs/EXP-20260213-001-config.yaml`

---

## Notes

### 설계 결정 사항 (Design Decisions)
- **2B급 모델 선택 이유**: A100 1장에서 full fine-tuning 가능, 빠른 반복 실험 가능. 7B+ 모델은 M6에서 validation 단계로 고려.
- **3개 아키텍처 사용 이유**: InternVL (MLP projector), LLaVA (MLP), Qwen2-VL (cross-attention)로 projector 유형별 일반화 가능성 확인.
- **Factorial design**: Full factorial은 2^4=16개 조합이지만, 핵심 10개로 축소하여 실험 비용 절감.
- **Phase 2 → Phase 3 의존성**: Phase 2 결과에 따라 Phase 3의 M6 (Combined) 전략이 결정됨. 적응적 실험 설계.

### 기존 실험과의 관계
- **EXP-20260211-001** (TBI-MLLM): 아키텍처 개선 관점 → 본 실험은 학습 전략 관점으로 상호보완적
- **EXP-20260212-001** (InternViT-Next): Vision encoder 개선 → 본 실험의 mitigation과 결합 가능 (better encoder + better training = synergy)
