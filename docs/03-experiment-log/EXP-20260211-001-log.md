# Experiment Log: EXP-20260211-001

> Research Phase: **3 - Experiment Run**
> Design Ref: `docs/02-experiment-design/features/EXP-20260211-001-design.md`
> Started: 2026-02-11
> Ended: —
> Status: Preparing

---

## 1. Environment Snapshot

### Hardware (Multi-Node: 4 nodes, 8 GPUs)
```
Cluster: 4-node multi-GPU setup
Total GPUs: 8x NVIDIA A100-PCIE-40GB (320 GB VRAM total)

Node 18 (master): ai-server-18 / 192.168.0.28
  GPU: 2x A100-PCIE-40GB | RAM: 444 GiB | Disk: 1.8 TiB free

Node 17 (worker): ai-server-17 / 192.168.0.229
  GPU: 2x A100-PCIE-40GB

Node 5  (worker): ai-server-5  / 192.168.0.180
  GPU: 2x A100-PCIE-40GB

Node 6  (worker): ai-server-6  / 192.168.0.177
  GPU: 2x A100-PCIE-40GB

CUDA Version: 12.8 (PyTorch bundled)
Driver Version: 580.126.09
```

### Software (conda env: tbi-mllm)
```
OS: Ubuntu 22.04.5 LTS (Linux 5.15.0-164-generic)
Python: 3.11.14
PyTorch: 2.10.0+cu128
CUDA (PyTorch): 12.8
CUDA (Driver): 13.0
NCCL: available (multi-node)
transformers: 5.1.0
accelerate: 1.6.0
deepspeed: 0.18.5
peft: 0.18.1
flash-attn: NOT INSTALLED (required)
mergekit: 0.1.4
lmms-eval: 0.5.0
wandb: 0.24.2
tensorboard: 2.20.0
```

### Git State
```
branch: main
commit: a1cd4be [hypothesis][design][experiment] TBI-MLLM research phases 1-3
dirty: yes (modified design doc, log, config; untracked multi-node configs)
```

### Required Setup (pre-training)
```bash
# 1. Create conda environment
conda create -n tbi-mllm python=3.11 -y
conda activate tbi-mllm

# 2. Install PyTorch (CUDA 13.0 compatible)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu130

# 3. Install InternVL dependencies
pip install transformers>=4.40.0 accelerate>=0.30.0 deepspeed>=0.14.0
pip install flash-attn --no-build-isolation
pip install sentencepiece tokenizers pillow

# 4. Install evaluation dependencies
pip install lmms-eval vllm openai

# 5. Install experiment tracking
pip install wandb tensorboard

# 6. Install model merging tools
pip install mergekit

# 7. Clone InternVL repository
git clone https://github.com/OpenGVLab/InternVL.git external/InternVL

# 8. Download base model
python -c "from transformers import AutoModel; AutoModel.from_pretrained('OpenGVLab/InternVL2_5-8B', trust_remote_code=True)"
```

---

## 2. Config Snapshot

> Full config: `experiments/configs/EXP-20260211-001-config.yaml`

```yaml
# Key parameters at runtime
experiment:
  id: "EXP-20260211-001"
  name: "TBI-MLLM-Systematic-Ablation"
  base_model: "OpenGVLab/InternVL2_5-8B"
  seeds: [42, 123, 456]

training:
  phase1:  # Projector pre-training
    lr: 1e-3
    steps: 10000
    batch_size: 256 (per_device=2, accum=64)
    frozen: [vision_encoder, llm_decoder]
  phase2:  # Vision+Projector fine-tuning
    lr: 2e-5 (vision) / 5e-5 (projector)
    steps: 20000
    batch_size: 128 (per_device=2, accum=32)
    frozen: [llm_decoder]
  phase3:  # Full/Decoder fine-tuning
    lr: 1e-5 (full) / 2e-4 (LoRA)
    steps: 50000
    batch_size: 64 (per_device=2, accum=16)

hardware:
  gpus: 2x A100-PCIE-40GB
  precision: bfloat16
  deepspeed: ZeRO-2
  gradient_checkpointing: true
  flash_attention: true
```

---

## 3. Execution Plan

### Critical Path (우선 실행 순서)

| Step | Experiment | Description | Est. Duration | Dependencies |
|------|-----------|-------------|---------------|-------------|
| 0 | Setup | Environment setup, model download, dataset prep | 2-3 days | — |
| 1 | A0 | Baseline zero-shot evaluation | 1 day | Setup |
| 2 | A1 | Baseline bilingual fine-tuning (3 seeds) | 10 days | A0 |
| 3 | E2 | Full vision pipeline (2D-RoPE + Pyramid + C-Abstractor) | 13 days | A1 checkpoint |
| 4 | F4 | Full TBI-MLLM (E2 + FT→Merge→MoLE) | 16 days | E2 checkpoint |
| 5 | B3, C2, D4 | Key ablations (parallel where possible) | 12-15 days | A1 checkpoint |
| 6 | Remaining | B1-B2, C1, D1-D3, E1, F1-F3 | 20-30 days | Various |

### Dataset Preparation Checklist

- [ ] Download InternVL2.5-8B base model (~28GB)
- [ ] Download Korean datasets
  - [ ] K-DTCBench (~15K samples)
  - [ ] VARCO-VISION Korean data (~50K samples)
  - [ ] Korean OCR synthetic data (~100K samples)
- [ ] Download English datasets
  - [ ] LLaVA-1.5 instruct (665K samples)
  - [ ] ShareGPT4V (100K samples)
  - [ ] TextVQA train (39K samples)
  - [ ] DocVQA train (39K samples)
  - [ ] ChartQA train (18K samples)
  - [ ] Table-LLaVA (250K samples)
  - [ ] CC3M subset for Phase 1 (~1M captions)
- [ ] Download evaluation benchmarks
  - [ ] MMBench (EN+KO)
  - [ ] ChartQA test / CharXiv
  - [ ] DocVQA test / InfoVQA test
  - [ ] TextVQA val / OCR-Bench
  - [ ] MathVista / MathVerse
  - [ ] WTQ / TabFact
  - [ ] K-DTCBench test

### Execution Commands

#### Step 0: Environment Setup
```bash
# See "Required Setup" section above
```

#### Step 0.5: Multi-Node NCCL Test
```bash
conda activate tbi-mllm
deepspeed --hostfile experiments/configs/multi-node/hostfile \
  --master_addr 192.168.0.28 --master_port 29500 \
  experiments/scripts/test_multinode.py
```

#### Step 1: A0 — Baseline Zero-shot Evaluation
```bash
conda activate tbi-mllm
python experiments/scripts/evaluate.py \
  --model OpenGVLab/InternVL2_5-8B \
  --benchmarks mmb_en mmb_ko textva docvqa chartqa mathvista k_dtcbench \
  --output results/tables/EXP-20260211-001-A0-results.csv \
  --batch_size 4 \
  --num_gpus 2
```

#### Step 2: A1 — Baseline Bilingual Fine-tuning (8 GPUs, 4 nodes)
```bash
conda activate tbi-mllm
HOSTFILE=experiments/configs/multi-node/hostfile
DS_CONFIG=experiments/configs/multi-node/ds_config_zero2_multinode.json
MASTER=192.168.0.28

for SEED in 42 123 456; do
  deepspeed --hostfile $HOSTFILE \
    --master_addr $MASTER --master_port 29500 \
    experiments/scripts/train.py \
    --config experiments/configs/EXP-20260211-001-config.yaml \
    --variant A1 \
    --seed $SEED \
    --output_dir results/EXP-20260211-001/A1/seed_${SEED} \
    --deepspeed $DS_CONFIG \
    --bf16 \
    --gradient_checkpointing
done
```

#### Step 3: E2 — Full Vision Pipeline (8 GPUs, 4 nodes)
```bash
conda activate tbi-mllm
for SEED in 42 123 456; do
  # Phase 1: Projector pre-training
  deepspeed --hostfile $HOSTFILE \
    --master_addr $MASTER --master_port 29500 \
    experiments/scripts/train.py \
    --config experiments/configs/EXP-20260211-001-config.yaml \
    --variant E2 --phase 1 \
    --seed $SEED \
    --output_dir results/EXP-20260211-001/E2/seed_${SEED}/phase1 \
    --deepspeed $DS_CONFIG

  # Phase 2: Vision+Projector
  deepspeed --hostfile $HOSTFILE \
    --master_addr $MASTER --master_port 29500 \
    experiments/scripts/train.py \
    --config experiments/configs/EXP-20260211-001-config.yaml \
    --variant E2 --phase 2 \
    --seed $SEED \
    --resume_from results/EXP-20260211-001/E2/seed_${SEED}/phase1/last.pt \
    --output_dir results/EXP-20260211-001/E2/seed_${SEED}/phase2 \
    --deepspeed $DS_CONFIG

  # Phase 3: Full fine-tuning
  deepspeed --hostfile $HOSTFILE \
    --master_addr $MASTER --master_port 29500 \
    experiments/scripts/train.py \
    --config experiments/configs/EXP-20260211-001-config.yaml \
    --variant E2 --phase 3 \
    --seed $SEED \
    --resume_from results/EXP-20260211-001/E2/seed_${SEED}/phase2/last.pt \
    --output_dir results/EXP-20260211-001/E2/seed_${SEED}/phase3 \
    --deepspeed $DS_CONFIG
done
```

#### Step 4: F4 — Full TBI-MLLM (8 GPUs, 4 nodes)
```bash
conda activate tbi-mllm
for SEED in 42 123 456; do
  # Reuse E2 Phase 1-2 checkpoints
  # Phase 3a: Korean-only fine-tuning (for merge)
  deepspeed --hostfile $HOSTFILE \
    --master_addr $MASTER --master_port 29500 \
    experiments/scripts/train.py \
    --config experiments/configs/EXP-20260211-001-config.yaml \
    --variant F4 --phase 3a_korean_ft \
    --seed $SEED \
    --resume_from results/EXP-20260211-001/E2/seed_${SEED}/phase2/last.pt \
    --output_dir results/EXP-20260211-001/F4/seed_${SEED}/phase3a \
    --deepspeed $DS_CONFIG

  # Phase 3b: DARE+TIES merging (single node, no distributed)
  conda activate tbi-mllm
  python experiments/scripts/merge_models.py \
    --method dare_ties \
    --model_en results/EXP-20260211-001/E2/seed_${SEED}/phase2/last.pt \
    --model_ko results/EXP-20260211-001/F4/seed_${SEED}/phase3a/last.pt \
    --lambda_en 0.6 --lambda_ko 0.4 \
    --dare_drop_rate 0.9 \
    --output_dir results/EXP-20260211-001/F4/seed_${SEED}/phase3b_merged

  # Phase 3c: MoLE expert training
  deepspeed --hostfile $HOSTFILE \
    --master_addr $MASTER --master_port 29500 \
    experiments/scripts/train_mole.py \
    --config experiments/configs/EXP-20260211-001-config.yaml \
    --variant F4 --phase 3c_mole \
    --seed $SEED \
    --init_merged results/EXP-20260211-001/F4/seed_${SEED}/phase3b_merged \
    --init_ko results/EXP-20260211-001/F4/seed_${SEED}/phase3a/last.pt \
    --init_en results/EXP-20260211-001/E2/seed_${SEED}/phase2/last.pt \
    --output_dir results/EXP-20260211-001/F4/seed_${SEED}/phase3c \
    --deepspeed $DS_CONFIG
done
```

### Monitoring Tips

```bash
# GPU utilization (매 5초)
watch -n 5 nvidia-smi

# Training loss curve (WandB)
wandb login
# → Dashboard: wandb.ai/your-project/TBI-MLLM

# TensorBoard (로컬 대안)
tensorboard --logdir results/EXP-20260211-001/ --port 6006

# Disk usage monitoring
watch -n 60 'df -h /home/juyeon && du -sh results/EXP-20260211-001/'

# Kill training if OOM
# → Reduce per_device_batch_size to 1, increase gradient_accumulation_steps
```

### Checkpoint Strategy
- Save every 2,500 steps
- Keep best 3 checkpoints (by validation loss)
- Keep last checkpoint
- Delete intermediate checkpoints after Phase completion

---

## 4. Training Progress

### Loss Curve
- Initial loss: —
- Final loss: —
- Best validation loss: — (step —)

### Key Checkpoints
| Variant | Phase | Step | Train Loss | Val Loss | Val Metric | Saved |
|---------|-------|------|-----------|----------|------------|-------|
| — | — | — | — | — | — | — |

### GPU Utilization
```
Average GPU util: — %
Average VRAM usage: — GB / 40 GB
Peak VRAM usage: — GB
```

---

## 5. Issues and Resolutions

| # | Issue | Resolution | Impact |
|---|-------|-----------|--------|
| — | — | — | — |

---

## 6. Outputs

### Checkpoints
- Best model: `results/EXP-20260211-001/F4/seed_42/phase3c/best.pt`
- Last model: `results/EXP-20260211-001/F4/seed_42/phase3c/last.pt`

### Logs
- Training log: `results/EXP-20260211-001/*/train.log`
- WandB run: (to be created)

### Generated Files
| File | Path | Description |
|------|------|-------------|
| — | — | — |

---

## 7. Quick Summary

### Did the experiment succeed?
Not yet started. Environment partially ready, datasets not yet downloaded.

### Key observations
1. 4-node cluster ready: 8x A100-40GB (320GB VRAM total), all idle (0% utilization)
2. Conda env `tbi-mllm` created with core packages: PyTorch 2.10.0, transformers 5.1.0, deepspeed 0.18.5
3. **flash-attn NOT installed** — required for efficient attention in InternVL
4. Dataset directories created but **all empty** — need to download actual data
5. Base model (InternVL2.5-8B) **not yet downloaded**
6. Multi-node config files ready: hostfile, DeepSpeed config, setup script, NCCL test script
7. 1.8TB local disk + 9.4TB NFS available — sufficient for all data + checkpoints
8. 444GB RAM — ample for data loading with 8 workers

### Readiness Checklist
- [x] Conda env `tbi-mllm` (Python 3.11.14)
- [x] PyTorch 2.10.0+cu128 (CUDA working, 2 GPUs detected)
- [x] Core packages (transformers, accelerate, deepspeed, peft, mergekit, lmms-eval, wandb)
- [x] Multi-node config (hostfile, DeepSpeed JSON, setup script, NCCL test)
- [ ] **flash-attn** — installing (C++ build in progress)
- [ ] **Worker node setup** — run setup-all-nodes.sh
- [ ] **NCCL multi-node test** — verify 8-GPU communication
- [ ] **Base model download** — InternVL2.5-8B (~28GB)
- [ ] **Training datasets** — all directories empty (~120GB needed)
- [ ] **Evaluation benchmarks** — all directories empty (~20GB needed)
- [x] **Training scripts** — `train.py`, `evaluate.py`, `merge_models.py`, `train_mole.py` written

### Recommended next steps
1. **Install flash-attn:** `conda run -n tbi-mllm pip install flash-attn --no-build-isolation`
2. **Setup worker nodes:** `bash experiments/configs/multi-node/setup-all-nodes.sh`
3. **Test NCCL:** `deepspeed --hostfile ... experiments/scripts/test_multinode.py`
4. **Download base model:** InternVL2.5-8B from HuggingFace
5. **Download datasets:** Korean + English training data + evaluation benchmarks
6. **Write training scripts:** `train.py`, `evaluate.py`, `merge_models.py`, `train_mole.py`
7. **Run A0:** Zero-shot baseline evaluation to verify pipeline
8. **Begin A1:** Baseline bilingual fine-tuning (3 seeds)

---

## Notes
- CUDA driver 13.0, PyTorch bundled CUDA 12.8 — compatible
- Multi-node config: 4 nodes × 2 GPUs = 8 GPUs total. With 8 GPUs, effective batch size maintained via gradient accumulation adjustment
- DeepSpeed ZeRO-2 config: no optimizer offload (multi-node), bf16 enabled
- Original design assumed 2 GPUs → now 8 GPUs. Revise training time estimates: ~4x speedup (280 GPU-days → ~35 calendar days for all experiments)
