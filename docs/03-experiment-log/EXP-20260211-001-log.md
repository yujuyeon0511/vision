# Experiment Log: EXP-20260211-001

> Research Phase: **3 - Experiment Run**
> Design Ref: `docs/02-experiment-design/features/EXP-20260211-001-design.md`
> Started: 2026-02-11
> Ended: —
> Status: Preparing

---

## 1. Environment Snapshot

### Hardware
```
hostname: ai-server-18
GPU: 2x NVIDIA A100-PCIE-40GB
GPU count: 2
VRAM: 40960 MiB per GPU (81920 MiB total)
CUDA Version: 13.0
Driver Version: 580.126.09
RAM: 444 GiB
Disk: 1.9 TiB total, 1.8 TiB available (8% used)
```

### Software
```
OS: Linux 5.15.0-164-generic
Python: 3.13.11
PyTorch: NOT INSTALLED (setup required)
CUDA: 13.0
cuDNN: NOT VERIFIED
transformers: NOT INSTALLED (setup required)
```

### Git State
```
branch: main
commit: dcbdbec [docs] README 한국어 사용 가이드 작성
dirty: no
```

### Required Setup (pre-training)
```bash
# 1. Create conda environment
conda create -n tbi-mllm python=3.11 -y
conda activate tbi-mllm

# 2. Install PyTorch (CUDA 13.0 compatible)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu130

# 3. Install InternVL dependencies
pip install transformers>=4.40.0 accelerate>=0.30.0 deepspeed>=0.14.0
pip install flash-attn --no-build-isolation
pip install sentencepiece tokenizers pillow

# 4. Install evaluation dependencies
pip install lmms-eval vllm openai

# 5. Install experiment tracking
pip install wandb tensorboard

# 6. Install model merging tools
pip install mergekit

# 7. Clone InternVL repository
git clone https://github.com/OpenGVLab/InternVL.git external/InternVL

# 8. Download base model
python -c "from transformers import AutoModel; AutoModel.from_pretrained('OpenGVLab/InternVL2_5-8B', trust_remote_code=True)"
```

---

## 2. Config Snapshot

> Full config: `experiments/configs/EXP-20260211-001-config.yaml`

```yaml
# Key parameters at runtime
experiment:
  id: "EXP-20260211-001"
  name: "TBI-MLLM-Systematic-Ablation"
  base_model: "OpenGVLab/InternVL2_5-8B"
  seeds: [42, 123, 456]

training:
  phase1:  # Projector pre-training
    lr: 1e-3
    steps: 10000
    batch_size: 256 (per_device=2, accum=64)
    frozen: [vision_encoder, llm_decoder]
  phase2:  # Vision+Projector fine-tuning
    lr: 2e-5 (vision) / 5e-5 (projector)
    steps: 20000
    batch_size: 128 (per_device=2, accum=32)
    frozen: [llm_decoder]
  phase3:  # Full/Decoder fine-tuning
    lr: 1e-5 (full) / 2e-4 (LoRA)
    steps: 50000
    batch_size: 64 (per_device=2, accum=16)

hardware:
  gpus: 2x A100-PCIE-40GB
  precision: bfloat16
  deepspeed: ZeRO-2
  gradient_checkpointing: true
  flash_attention: true
```

---

## 3. Execution Plan

### Critical Path (우선 실행 순서)

| Step | Experiment | Description | Est. Duration | Dependencies |
|------|-----------|-------------|---------------|-------------|
| 0 | Setup | Environment setup, model download, dataset prep | 2-3 days | — |
| 1 | A0 | Baseline zero-shot evaluation | 1 day | Setup |
| 2 | A1 | Baseline bilingual fine-tuning (3 seeds) | 10 days | A0 |
| 3 | E2 | Full vision pipeline (2D-RoPE + Pyramid + C-Abstractor) | 13 days | A1 checkpoint |
| 4 | F4 | Full TBI-MLLM (E2 + FT→Merge→MoLE) | 16 days | E2 checkpoint |
| 5 | B3, C2, D4 | Key ablations (parallel where possible) | 12-15 days | A1 checkpoint |
| 6 | Remaining | B1-B2, C1, D1-D3, E1, F1-F3 | 20-30 days | Various |

### Dataset Preparation Checklist

- [ ] Download InternVL2.5-8B base model (~28GB)
- [ ] Download Korean datasets
  - [ ] K-DTCBench (~15K samples)
  - [ ] VARCO-VISION Korean data (~50K samples)
  - [ ] Korean OCR synthetic data (~100K samples)
- [ ] Download English datasets
  - [ ] LLaVA-1.5 instruct (665K samples)
  - [ ] ShareGPT4V (100K samples)
  - [ ] TextVQA train (39K samples)
  - [ ] DocVQA train (39K samples)
  - [ ] ChartQA train (18K samples)
  - [ ] Table-LLaVA (250K samples)
  - [ ] CC3M subset for Phase 1 (~1M captions)
- [ ] Download evaluation benchmarks
  - [ ] MMBench (EN+KO)
  - [ ] ChartQA test / CharXiv
  - [ ] DocVQA test / InfoVQA test
  - [ ] TextVQA val / OCR-Bench
  - [ ] MathVista / MathVerse
  - [ ] WTQ / TabFact
  - [ ] K-DTCBench test

### Execution Commands

#### Step 0: Environment Setup
```bash
# See "Required Setup" section above
```

#### Step 1: A0 — Baseline Zero-shot Evaluation
```bash
python experiments/scripts/evaluate.py \
  --model OpenGVLab/InternVL2_5-8B \
  --benchmarks mmb_en mmb_ko textva docvqa chartqa mathvista k_dtcbench \
  --output results/tables/EXP-20260211-001-A0-results.csv \
  --batch_size 4 \
  --num_gpus 2
```

#### Step 2: A1 — Baseline Bilingual Fine-tuning
```bash
# Phase 3 only (standard fine-tuning, no architecture changes)
for SEED in 42 123 456; do
  deepspeed --num_gpus 2 experiments/scripts/train.py \
    --config experiments/configs/EXP-20260211-001-config.yaml \
    --variant A1 \
    --seed $SEED \
    --output_dir results/EXP-20260211-001/A1/seed_${SEED} \
    --deepspeed ds_config_zero2.json \
    --bf16 \
    --gradient_checkpointing
done
```

#### Step 3: E2 — Full Vision Pipeline
```bash
# Phase 1 → Phase 2 → Phase 3 (sequential)
for SEED in 42 123 456; do
  # Phase 1: Projector pre-training
  deepspeed --num_gpus 2 experiments/scripts/train.py \
    --config experiments/configs/EXP-20260211-001-config.yaml \
    --variant E2 --phase 1 \
    --seed $SEED \
    --output_dir results/EXP-20260211-001/E2/seed_${SEED}/phase1

  # Phase 2: Vision+Projector
  deepspeed --num_gpus 2 experiments/scripts/train.py \
    --config experiments/configs/EXP-20260211-001-config.yaml \
    --variant E2 --phase 2 \
    --seed $SEED \
    --resume_from results/EXP-20260211-001/E2/seed_${SEED}/phase1/last.pt \
    --output_dir results/EXP-20260211-001/E2/seed_${SEED}/phase2

  # Phase 3: Full fine-tuning
  deepspeed --num_gpus 2 experiments/scripts/train.py \
    --config experiments/configs/EXP-20260211-001-config.yaml \
    --variant E2 --phase 3 \
    --seed $SEED \
    --resume_from results/EXP-20260211-001/E2/seed_${SEED}/phase2/last.pt \
    --output_dir results/EXP-20260211-001/E2/seed_${SEED}/phase3
done
```

#### Step 4: F4 — Full TBI-MLLM
```bash
for SEED in 42 123 456; do
  # Reuse E2 Phase 1-2 checkpoints
  # Phase 3a: Korean-only fine-tuning (for merge)
  deepspeed --num_gpus 2 experiments/scripts/train.py \
    --config experiments/configs/EXP-20260211-001-config.yaml \
    --variant F4 --phase 3a_korean_ft \
    --seed $SEED \
    --resume_from results/EXP-20260211-001/E2/seed_${SEED}/phase2/last.pt \
    --output_dir results/EXP-20260211-001/F4/seed_${SEED}/phase3a

  # Phase 3b: DARE+TIES merging
  python experiments/scripts/merge_models.py \
    --method dare_ties \
    --model_en results/EXP-20260211-001/E2/seed_${SEED}/phase2/last.pt \
    --model_ko results/EXP-20260211-001/F4/seed_${SEED}/phase3a/last.pt \
    --lambda_en 0.6 --lambda_ko 0.4 \
    --dare_drop_rate 0.9 \
    --output_dir results/EXP-20260211-001/F4/seed_${SEED}/phase3b_merged

  # Phase 3c: MoLE expert training
  deepspeed --num_gpus 2 experiments/scripts/train_mole.py \
    --config experiments/configs/EXP-20260211-001-config.yaml \
    --variant F4 --phase 3c_mole \
    --seed $SEED \
    --init_merged results/EXP-20260211-001/F4/seed_${SEED}/phase3b_merged \
    --init_ko results/EXP-20260211-001/F4/seed_${SEED}/phase3a/last.pt \
    --init_en results/EXP-20260211-001/E2/seed_${SEED}/phase2/last.pt \
    --output_dir results/EXP-20260211-001/F4/seed_${SEED}/phase3c
done
```

### Monitoring Tips

```bash
# GPU utilization (매 5초)
watch -n 5 nvidia-smi

# Training loss curve (WandB)
wandb login
# → Dashboard: wandb.ai/your-project/TBI-MLLM

# TensorBoard (로컬 대안)
tensorboard --logdir results/EXP-20260211-001/ --port 6006

# Disk usage monitoring
watch -n 60 'df -h /home/juyeon && du -sh results/EXP-20260211-001/'

# Kill training if OOM
# → Reduce per_device_batch_size to 1, increase gradient_accumulation_steps
```

### Checkpoint Strategy
- Save every 2,500 steps
- Keep best 3 checkpoints (by validation loss)
- Keep last checkpoint
- Delete intermediate checkpoints after Phase completion

---

## 4. Training Progress

### Loss Curve
- Initial loss: —
- Final loss: —
- Best validation loss: — (step —)

### Key Checkpoints
| Variant | Phase | Step | Train Loss | Val Loss | Val Metric | Saved |
|---------|-------|------|-----------|----------|------------|-------|
| — | — | — | — | — | — | — |

### GPU Utilization
```
Average GPU util: — %
Average VRAM usage: — GB / 40 GB
Peak VRAM usage: — GB
```

---

## 5. Issues and Resolutions

| # | Issue | Resolution | Impact |
|---|-------|-----------|--------|
| — | — | — | — |

---

## 6. Outputs

### Checkpoints
- Best model: `results/EXP-20260211-001/F4/seed_42/phase3c/best.pt`
- Last model: `results/EXP-20260211-001/F4/seed_42/phase3c/last.pt`

### Logs
- Training log: `results/EXP-20260211-001/*/train.log`
- WandB run: (to be created)

### Generated Files
| File | Path | Description |
|------|------|-------------|
| — | — | — |

---

## 7. Quick Summary

### Did the experiment succeed?
Not yet started. Environment preparation in progress.

### Key observations
1. 2x A100-40GB available, both idle (0% utilization)
2. 1.8TB disk available — sufficient for datasets + checkpoints
3. 444GB RAM — ample for data loading
4. **PyTorch, transformers not yet installed** — environment setup required first

### Recommended next steps
1. Run environment setup commands (conda env, pip installs)
2. Download and verify base model (InternVL2.5-8B)
3. Prepare and validate all training/evaluation datasets
4. Run A0 (zero-shot baseline) to verify evaluation pipeline
5. Begin A1 (baseline fine-tuning) on 3 seeds

---

## Notes
- CUDA 13.0 detected — verify PyTorch wheel compatibility
- Python 3.13.11 installed — may need 3.11 for better library compatibility
- No existing training frameworks installed — clean environment
- Disk usage is low (8%) — plenty of room for experiment artifacts
